# RAGä¼˜åŒ–å»ºè®®ï¼šæ–‡æœ¬åˆ†å‰²ä¸æ£€ç´¢æµç¨‹ä¼˜åŒ–

> ç”Ÿæˆæ—¥æœŸï¼š2025å¹´11æœˆ19æ—¥

## 1. æ–‡æœ¬åˆ†å‰²ä¼˜åŒ–å»ºè®®

### å½“å‰ç³»ç»Ÿçš„é—®é¢˜
é¡¹ç›®ä½¿ç”¨çš„æ˜¯ `RecursiveCharacterTextSplitter`ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™çš„åˆ†å‰²å™¨ï¼š
- **ä¼˜ç‚¹**ï¼šå¿«é€Ÿã€å¯é¢„æµ‹ã€æ— éœ€é¢å¤–è®¡ç®—æˆæœ¬
- **ç¼ºç‚¹**ï¼šå¯èƒ½åœ¨å¥å­ä¸­é—´åˆ†å‰²ã€ç ´åè¯­ä¹‰å®Œæ•´æ€§ã€å›ºå®šchunkå¤§å°ä¸å¤Ÿçµæ´»

### ä¼˜åŒ–æ–¹æ¡ˆ

#### æ–¹æ¡ˆä¸€ï¼šLLMè¯­ä¹‰è¾¹ç•Œæ£€æµ‹ï¼ˆé«˜çº§ï¼‰
ä½¿ç”¨æœ¬åœ°LLMè¯†åˆ«è‡ªç„¶æ®µè½å’Œä¸»é¢˜è¾¹ç•Œï¼š

```python
# Backend/app/services/semantic_splitter.py (æ–°æ–‡ä»¶å»ºè®®)
from typing import List, Dict
import requests

class LLMSemanticSplitter:
    """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½è¯­ä¹‰åˆ†å‰²"""
    
    def __init__(self, ollama_base_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_base_url
    
    async def split_by_semantic_boundary(
        self, 
        text: str, 
        model: str = "qwen2.5:3b",  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹
        max_chunk_size: int = 800
    ) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨LLMè¯†åˆ«è¯­ä¹‰è¾¹ç•Œè¿›è¡Œåˆ†å‰²
        
        å·¥ä½œæµç¨‹:
        1. å°†é•¿æ–‡æœ¬é€å…¥LLM
        2. è¦æ±‚LLMè¯†åˆ«ä¸»é¢˜å˜åŒ–ç‚¹/æ®µè½è¾¹ç•Œ
        3. æ ¹æ®LLMå»ºè®®è¿›è¡Œåˆ†å‰²
        """
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬,è¯†åˆ«è‡ªç„¶çš„è¯­ä¹‰æ®µè½è¾¹ç•Œã€‚
åœ¨æ¯ä¸ªæ®µè½ç»“æŸåæ ‡è®°[SPLIT]ã€‚

è¦æ±‚:
- æ¯ä¸ªæ®µè½åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯­ä¹‰å•å…ƒ
- æ®µè½é•¿åº¦å°½é‡åœ¨{max_chunk_size}å­—ç¬¦å·¦å³
- ä¸è¦åœ¨å¥å­ä¸­é—´åˆ†å‰²
- ä¿æŒä¸»é¢˜è¿è´¯æ€§

æ–‡æœ¬:
{text[:4000]}  # é™åˆ¶è¾“å…¥é•¿åº¦é¿å…tokenè¶…é™

è¯·åœ¨æ®µè½è¾¹ç•Œå¤„æ’å…¥[SPLIT]æ ‡è®°:"""

        response = requests.post(
            f"{self.ollama_url}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,  # ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
                    "num_predict": 2000
                }
            }
        )
        
        result_text = response.json()["response"]
        chunks = result_text.split("[SPLIT]")
        
        return [
            {
                "text": chunk.strip(),
                "metadata": {"split_method": "llm_semantic"}
            }
            for chunk in chunks if chunk.strip()
        ]
```

#### æ–¹æ¡ˆäºŒï¼šæ··åˆåˆ†å‰²ç­–ç•¥ï¼ˆæ¨èï¼‰
ç»“åˆå¤šç§æ–¹æ³•çš„ä¼˜åŠ¿ï¼š

```python
# Backend/app/services/hybrid_splitter.py
from langchain.text_splitter import RecursiveCharacterTextSplitter
import re
from typing import List

class HybridTextSplitter:
    """æ··åˆæ–‡æœ¬åˆ†å‰²ç­–ç•¥"""
    
    def __init__(self):
        # åŸºç¡€åˆ†å‰²å™¨
        self.base_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            separators=["\n\n\n", "\n\n", "\n", "ã€‚", "!", "?", ";", ",", " ", ""]
        )
    
    def split_with_structure(self, text: str) -> List[Dict]:
        """
        ç»“æ„åŒ–åˆ†å‰²:
        1. è¯†åˆ«æ–‡æ¡£ç»“æ„(æ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—)
        2. ä¿æŒç»“æ„å®Œæ•´æ€§
        3. åœ¨ç»“æ„è¾¹ç•Œä¼˜å…ˆåˆ†å‰²
        """
        chunks = []
        
        # 1. è¯†åˆ«Markdownæ ‡é¢˜
        header_pattern = r'^#{1,6}\s.+$'
        sections = re.split(f'({header_pattern})', text, flags=re.MULTILINE)
        
        current_section = ""
        current_header = ""
        
        for section in sections:
            if re.match(header_pattern, section):
                # é‡åˆ°æ–°æ ‡é¢˜,ä¿å­˜ä¸Šä¸€èŠ‚
                if current_section:
                    chunks.extend(self._split_section(
                        current_section, 
                        header=current_header
                    ))
                current_header = section
                current_section = section + "\n"
            else:
                current_section += section
        
        # ä¿å­˜æœ€åä¸€èŠ‚
        if current_section:
            chunks.extend(self._split_section(
                current_section, 
                header=current_header
            ))
        
        return chunks
    
    def _split_section(self, text: str, header: str = "") -> List[Dict]:
        """åˆ†å‰²å•ä¸ªç« èŠ‚"""
        # 2. è¯†åˆ«ç‰¹æ®Šå—(ä»£ç ã€è¡¨æ ¼)
        code_blocks = re.findall(r'```[\s\S]*?```', text)
        
        # å¦‚æœåŒ…å«ä»£ç å—,ä¿æŒä»£ç å®Œæ•´æ€§
        if code_blocks and len(text) < 1500:
            return [{
                "text": text,
                "metadata": {
                    "header": header,
                    "has_code": True,
                    "split_method": "preserve_structure"
                }
            }]
        
        # 3. å¦åˆ™ä½¿ç”¨é€’å½’åˆ†å‰²
        base_chunks = self.base_splitter.split_text(text)
        return [
            {
                "text": chunk,
                "metadata": {
                    "header": header,
                    "split_method": "recursive"
                }
            }
            for chunk in base_chunks
        ]
```

#### æ–¹æ¡ˆä¸‰ï¼šè¯­ä¹‰åµŒå…¥åˆ†å‰²ï¼ˆæœ€å…ˆè¿›ï¼‰
ä½¿ç”¨embeddingç›¸ä¼¼åº¦è¯†åˆ«ä¸»é¢˜å˜åŒ–ï¼š

```python
# Backend/app/services/embedding_based_splitter.py
import numpy as np
from typing import List
from app.services.embedding_service import UnifiedEmbeddingService

class EmbeddingBasedSplitter:
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ™ºèƒ½åˆ†å‰²"""
    
    def __init__(self, embedding_service: UnifiedEmbeddingService):
        self.embedding_service = embedding_service
    
    async def split_by_semantic_similarity(
        self, 
        text: str,
        similarity_threshold: float = 0.75  # ç›¸ä¼¼åº¦é˜ˆå€¼
    ) -> List[str]:
        """
        å·¥ä½œåŸç†:
        1. å°†æ–‡æœ¬æŒ‰å¥å­åˆ†å‰²
        2. è®¡ç®—ç›¸é‚»å¥å­çš„embeddingç›¸ä¼¼åº¦
        3. åœ¨ç›¸ä¼¼åº¦çªé™å¤„åˆ†å‰²(ä¸»é¢˜å˜åŒ–ç‚¹)
        """
        
        # 1. åˆ†å‰²æˆå¥å­
        sentences = self._split_into_sentences(text)
        
        # 2. è·å–æ¯ä¸ªå¥å­çš„embedding
        embeddings = []
        for sentence in sentences:
            emb = await self.embedding_service.get_embedding(sentence)
            embeddings.append(np.array(emb))
        
        # 3. è®¡ç®—ç›¸é‚»å¥å­ç›¸ä¼¼åº¦
        similarities = []
        for i in range(len(embeddings) - 1):
            sim = np.dot(embeddings[i], embeddings[i+1])
            similarities.append(sim)
        
        # 4. è¯†åˆ«ç›¸ä¼¼åº¦çªé™ç‚¹(ä¸»é¢˜å˜åŒ–)
        split_points = [0]
        for i, sim in enumerate(similarities):
            if sim < similarity_threshold:
                split_points.append(i + 1)
        split_points.append(len(sentences))
        
        # 5. æ ¹æ®åˆ†å‰²ç‚¹ç»„åˆchunk
        chunks = []
        for i in range(len(split_points) - 1):
            start = split_points[i]
            end = split_points[i + 1]
            chunk = " ".join(sentences[start:end])
            chunks.append(chunk)
        
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²æˆå¥å­"""
        import re
        # ä¸­è‹±æ–‡å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚!?;ï¼ï¼Ÿï¼›]\s*', text)
        return [s.strip() for s in sentences if s.strip()]
```

---

## 2. RAGæ£€ç´¢æµç¨‹ä¼˜åŒ–å»ºè®®

### å½“å‰ç³»ç»Ÿé—®é¢˜
çŸ¥è¯†åº“æ£€ç´¢æµç¨‹ç›¸å¯¹ç®€å•ï¼š
1. ç”¨æˆ·æŸ¥è¯¢ â†’ embedding â†’ ChromaDBæ£€ç´¢ â†’ è¿”å›topK
2. ç¼ºå°‘æŸ¥è¯¢ä¼˜åŒ–ã€é‡æ’åºã€ä¸Šä¸‹æ–‡æ‰©å±•ç­‰é«˜çº§åŠŸèƒ½

### ä¼˜åŒ–æ–¹æ¡ˆ

#### ä¼˜åŒ–1ï¼šæŸ¥è¯¢æ”¹å†™ï¼ˆQuery Rewritingï¼‰
```python
# Backend/app/services/query_optimizer.py
class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    
    async def expand_query(self, query: str, model: str = "qwen2.5:3b") -> List[str]:
        """
        æŸ¥è¯¢æ‰©å±•:ç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢
        
        ç¤ºä¾‹:
        è¾“å…¥: "Pythonå¼‚å¸¸å¤„ç†"
        è¾“å‡º: [
            "Pythonå¼‚å¸¸å¤„ç†",
            "Python try exceptä½¿ç”¨æ–¹æ³•",
            "Pythoné”™è¯¯æ•è·æœºåˆ¶",
            "å¦‚ä½•å¤„ç†Pythonè¿è¡Œæ—¶é”™è¯¯"
        ]
        """
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆ3ä¸ªè¯­ä¹‰ç›¸å…³çš„å˜ä½“é—®é¢˜ã€‚
æ¯ä¸ªå˜ä½“åº”è¯¥ä»ä¸åŒè§’åº¦è¡¨è¾¾ç›¸åŒçš„ä¿¡æ¯éœ€æ±‚ã€‚

åŸå§‹æŸ¥è¯¢: {query}

å˜ä½“é—®é¢˜(æ¯è¡Œä¸€ä¸ª):
1."""

        response = await self._call_ollama(model, prompt)
        queries = [query] + self._parse_numbered_list(response)
        return queries[:4]  # æœ€å¤š4ä¸ªå˜ä½“
    
    async def decompose_query(self, query: str) -> List[str]:
        """
        å¤æ‚æŸ¥è¯¢åˆ†è§£
        
        ç¤ºä¾‹:
        è¾“å…¥: "Python FastAPIå¦‚ä½•é›†æˆJWTè®¤è¯å’ŒCORSé…ç½®"
        è¾“å‡º: [
            "Python FastAPI JWTè®¤è¯å®ç°",
            "FastAPI CORSé…ç½®æ–¹æ³•"
        ]
        """
        # è¯†åˆ«æ˜¯å¦æ˜¯å¤åˆæŸ¥è¯¢
        if " å’Œ " in query or "ä»¥åŠ" in query or len(query) > 40:
            prompt = f"""å°†ä»¥ä¸‹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸º2-3ä¸ªç®€å•çš„å­æŸ¥è¯¢ã€‚

å¤æ‚æŸ¥è¯¢: {query}

å­æŸ¥è¯¢(æ¯è¡Œä¸€ä¸ª):
1."""
            response = await self._call_ollama("qwen2.5:3b", prompt)
            return self._parse_numbered_list(response)
        
        return [query]
```

#### ä¼˜åŒ–2ï¼šæ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰
```python
# Backend/app/services/hybrid_retriever.py
class HybridRetriever:
    """æ··åˆæ£€ç´¢:å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢"""
    
    async def hybrid_search(
        self,
        query: str,
        kb_id: int,
        top_k: int = 10,
        alpha: float = 0.7  # å‘é‡æ£€ç´¢æƒé‡
    ) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥:
        score = alpha * vector_score + (1-alpha) * bm25_score
        """
        
        # 1. å‘é‡æ£€ç´¢
        vector_results = await self.vector_search(query, kb_id, top_k * 2)
        
        # 2. BM25å…³é”®è¯æ£€ç´¢
        bm25_results = await self.bm25_search(query, kb_id, top_k * 2)
        
        # 3. èåˆæ’åº(Reciprocal Rank Fusion)
        merged = self._reciprocal_rank_fusion(
            vector_results, 
            bm25_results,
            alpha
        )
        
        return merged[:top_k]
    
    def _reciprocal_rank_fusion(
        self, 
        vector_results: List,
        bm25_results: List,
        alpha: float
    ) -> List[Dict]:
        """RRFç®—æ³•èåˆä¸¤ç§æ£€ç´¢ç»“æœ"""
        scores = {}
        k = 60  # RRFå¸¸æ•°
        
        # å‘é‡æ£€ç´¢å¾—åˆ†
        for rank, result in enumerate(vector_results):
            doc_id = result["id"]
            scores[doc_id] = scores.get(doc_id, 0) + alpha / (k + rank + 1)
        
        # BM25å¾—åˆ†
        for rank, result in enumerate(bm25_results):
            doc_id = result["id"]
            scores[doc_id] = scores.get(doc_id, 0) + (1-alpha) / (k + rank + 1)
        
        # æŒ‰å¾—åˆ†æ’åº
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [self._get_doc_by_id(doc_id) for doc_id, _ in ranked]
```

#### ä¼˜åŒ–3ï¼šé‡æ’åºï¼ˆRerankingï¼‰
```python
# Backend/app/services/reranker.py
class CrossEncoderReranker:
    """ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ’åº"""
    
    def __init__(self):
        from sentence_transformers import CrossEncoder
        # åŠ è½½é‡æ’åºæ¨¡å‹
        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def rerank(
        self, 
        query: str, 
        candidates: List[Dict],
        top_k: int = 5
    ) -> List[Dict]:
        """
        é‡æ’åº:
        1. åˆæ£€:å‘é‡æ£€ç´¢å¬å›topK*2å€™é€‰
        2. ç²¾æ’:CrossEncoderè®¡ç®—query-docç²¾ç¡®ç›¸å…³æ€§
        3. è¿”å›top_kæœ€ç›¸å…³ç»“æœ
        """
        
        # æ„é€ query-docå¯¹
        pairs = [(query, doc["text"]) for doc in candidates]
        
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        scores = self.model.predict(pairs)
        
        # æ·»åŠ å¾—åˆ†å¹¶æ’åº
        for doc, score in zip(candidates, scores):
            doc["rerank_score"] = float(score)
        
        reranked = sorted(
            candidates, 
            key=lambda x: x["rerank_score"], 
            reverse=True
        )
        
        return reranked[:top_k]
```

#### ä¼˜åŒ–4ï¼šä¸Šä¸‹æ–‡çª—å£æ‰©å±•
```python
# åœ¨ knowledge_base_service.py ä¸­æ·»åŠ 
async def search_with_context_expansion(
    self,
    query: str,
    kb_id: int,
    top_k: int = 3
) -> List[Dict]:
    """
    æ£€ç´¢ç»“æœä¸Šä¸‹æ–‡æ‰©å±•
    
    é—®é¢˜: å•ä¸ªchunkå¯èƒ½ç¼ºå°‘ä¸Šä¸‹æ–‡ä¿¡æ¯
    è§£å†³: è¿”å›chunkçš„å‰åç›¸é‚»chunk
    """
    
    # 1. åŸºç¡€æ£€ç´¢
    results = await self.search_knowledge_base(query, kb_id, top_k)
    
    # 2. ä¸ºæ¯ä¸ªç»“æœæ‰©å±•ä¸Šä¸‹æ–‡
    expanded_results = []
    for result in results:
        chunk_id = result["chunk_id"]
        
        # è·å–ç›¸é‚»chunk
        prev_chunk = await self._get_adjacent_chunk(chunk_id, offset=-1)
        next_chunk = await self._get_adjacent_chunk(chunk_id, offset=1)
        
        # æ‹¼æ¥ä¸Šä¸‹æ–‡
        full_context = ""
        if prev_chunk:
            full_context += f"[ä¸Šæ–‡] {prev_chunk['text']}\n\n"
        full_context += f"[æ ¸å¿ƒ] {result['text']}\n\n"
        if next_chunk:
            full_context += f"[ä¸‹æ–‡] {next_chunk['text']}"
        
        expanded_results.append({
            **result,
            "expanded_text": full_context
        })
    
    return expanded_results
```

---

## 3. å®æ–½ä¼˜å…ˆçº§å»ºè®®

### ç«‹å³å¯åšï¼ˆä½æˆæœ¬é«˜æ”¶ç›Šï¼‰
1. âœ… ä¼˜åŒ–RecursiveCharacterTextSplitterçš„separatorså‚æ•°ï¼ˆå·²å®Œæˆï¼‰
2. ğŸ”¥ **å®ç°æ··åˆæ£€ç´¢**ï¼ˆå‘é‡+BM25ï¼‰
3. ğŸ”¥ **æ·»åŠ æŸ¥è¯¢æ‰©å±•åŠŸèƒ½**
4. **å®ç°ä¸Šä¸‹æ–‡çª—å£æ‰©å±•**

### ä¸­æœŸä¼˜åŒ–ï¼ˆéœ€è¦é¢å¤–æ¨¡å‹ï¼‰
5. é›†æˆCrossEncoderé‡æ’åºæ¨¡å‹
6. å®ç°åŸºäºembeddingç›¸ä¼¼åº¦çš„è¯­ä¹‰åˆ†å‰²

### é«˜çº§åŠŸèƒ½ï¼ˆéœ€è¦æ›´å¤šè®¡ç®—èµ„æºï¼‰
7. LLMè¯­ä¹‰è¾¹ç•Œæ£€æµ‹
8. å¤šæŸ¥è¯¢èåˆæ£€ç´¢

---

## 4. å…·ä½“å®æ–½æ¨èé¡ºåº

### ä¼˜å…ˆçº§1ï¼šæ··åˆæ£€ç´¢
- **åŸå› **ï¼šå¯¹æ£€ç´¢è´¨é‡æå‡æœ€æ˜æ˜¾
- **å®ç°éš¾åº¦**ï¼šä¸­ç­‰
- **ä¾èµ–**ï¼šéœ€è¦å®ç°BM25ç´¢å¼•

### ä¼˜å…ˆçº§2ï¼šä¸Šä¸‹æ–‡çª—å£æ‰©å±•
- **åŸå› **ï¼šå®ç°ç®€å•ï¼Œæ•ˆæœå¥½
- **å®ç°éš¾åº¦**ï¼šä½
- **ä¾èµ–**ï¼šå½“å‰ç³»ç»Ÿå³å¯æ”¯æŒ

### ä¼˜å…ˆçº§3ï¼šæŸ¥è¯¢æ”¹å†™
- **åŸå› **ï¼šåˆ©ç”¨ç°æœ‰çš„Ollamaæ¨¡å‹
- **å®ç°éš¾åº¦**ï¼šä¸­ç­‰
- **ä¾èµ–**ï¼šéœ€è¦Ollama APIè°ƒç”¨å°è£…

### ä¼˜å…ˆçº§4ï¼šé‡æ’åº
- **åŸå› **ï¼šæ•ˆæœæ˜¾è‘—
- **å®ç°éš¾åº¦**ï¼šä¸­ç­‰
- **ä¾èµ–**ï¼šéœ€è¦ä¸‹è½½CrossEncoderæ¨¡å‹ï¼ˆçº¦90MBï¼‰

---

## 5. æŠ€æœ¯è¦ç‚¹æ€»ç»“

### æ–‡æœ¬åˆ†å‰²æ ¸å¿ƒæ€æƒ³
- **è§„åˆ™åˆ†å‰²**ï¼šå¿«é€Ÿä½†ä¸ç²¾ç¡®ï¼ˆå½“å‰æ–¹æ¡ˆï¼‰
- **è¯­ä¹‰åˆ†å‰²**ï¼šå‡†ç¡®ä½†è®¡ç®—å¯†é›†ï¼ˆLLMã€Embeddingæ–¹æ¡ˆï¼‰
- **æ··åˆåˆ†å‰²**ï¼šç»“åˆç»“æ„è¯†åˆ«å’Œè§„åˆ™åˆ†å‰²ï¼ˆæ¨èï¼‰

### RAGæ£€ç´¢æ ¸å¿ƒæ€æƒ³
- **å•è·¯å¬å›** â†’ **å¤šè·¯å¬å›**ï¼ˆå‘é‡+BM25+ç¨€ç–ï¼‰
- **ç²—æ’** â†’ **ç²¾æ’**ï¼ˆEmbedding â†’ CrossEncoderï¼‰
- **ç‚¹æ£€ç´¢** â†’ **ä¸Šä¸‹æ–‡æ£€ç´¢**ï¼ˆå•chunk â†’ å‰åchunkæ‰©å±•ï¼‰
- **å•æŸ¥è¯¢** â†’ **å¤šæŸ¥è¯¢**ï¼ˆæŸ¥è¯¢æ”¹å†™ã€åˆ†è§£ã€æ‰©å±•ï¼‰

### æ€§èƒ½ä¸æ•ˆæœæƒè¡¡
| æ–¹æ¡ˆ | æ•ˆæœæå‡ | è®¡ç®—æˆæœ¬ | å®ç°éš¾åº¦ |
|------|---------|---------|---------|
| æ··åˆæ£€ç´¢ | â­â­â­â­â­ | â­â­ | â­â­â­ |
| ä¸Šä¸‹æ–‡æ‰©å±• | â­â­â­â­ | â­ | â­â­ |
| æŸ¥è¯¢æ”¹å†™ | â­â­â­â­ | â­â­â­ | â­â­â­ |
| é‡æ’åº | â­â­â­â­â­ | â­â­â­ | â­â­ |
| LLMåˆ†å‰² | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| Embeddingåˆ†å‰² | â­â­â­â­ | â­â­â­â­ | â­â­â­ |

---

## 6. å‚è€ƒèµ„æº

### ç›¸å…³è®ºæ–‡
- **æ··åˆæ£€ç´¢**ï¼š["Precise Zero-Shot Dense Retrieval without Relevance Labels"](https://arxiv.org/abs/2212.10496)
- **é‡æ’åº**ï¼š["RankT5: Fine-Tuning T5 for Text Ranking"](https://arxiv.org/abs/2210.10634)
- **è¯­ä¹‰åˆ†å‰²**ï¼š["Semantic Text Segmentation with LLMs"](https://arxiv.org/abs/2304.09121)

### å¼€æºé¡¹ç›®
- **LlamaIndex**ï¼šé«˜çº§RAGæ¡†æ¶
- **LangChain**ï¼šåŒ…å«å¤šç§æ–‡æœ¬åˆ†å‰²å™¨
- **RAGatouille**ï¼šé›†æˆé‡æ’åºçš„RAGå·¥å…·

### æ¨¡å‹æ¨è
- **Embeddingæ¨¡å‹**ï¼š`bge-large-zh-v1.5`ï¼ˆä¸­æ–‡ï¼‰
- **é‡æ’åºæ¨¡å‹**ï¼š`bge-reranker-large`ï¼ˆä¸­æ–‡ï¼‰
- **CrossEncoder**ï¼š`cross-encoder/ms-marco-MiniLM-L-6-v2`ï¼ˆè‹±æ–‡ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æœ€åæ›´æ–°**ï¼š2025å¹´11æœˆ19æ—¥
