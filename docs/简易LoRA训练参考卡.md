# ğŸ¯ ç®€æ˜“ LoRA è®­ç»ƒ - å¿«é€Ÿå‚è€ƒå¡

## ä¸€é”®å¯åŠ¨

```powershell
.\start-training.bat
```

## è®¿é—®åœ°å€

```
è®­ç»ƒé¡µé¢: http://localhost:8000/static/simple-lora-training.html
æ¨¡å‹ç®¡ç†: http://localhost:8000/static/model-management.html
```

## ä½¿ç”¨æ­¥éª¤

1. ä¸Šä¼ æ•°æ®é›†ï¼ˆæ‹–æ‹½æˆ–ç‚¹å‡»ï¼‰
2. é€‰æ‹©åŸºåº§æ¨¡å‹
3. è¾“å…¥ä»»åŠ¡åç§°
4. ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"

## æ•°æ®é›†æ ¼å¼

### Alpaca æ ¼å¼

```json
[
    {
        "instruction": "é—®é¢˜",
        "input": "",
        "output": "ç­”æ¡ˆ"
    }
]
```

### ShareGPT æ ¼å¼

```json
[
    {
        "conversations": [
            {"from": "human", "value": "ç”¨æˆ·æ¶ˆæ¯"},
            {"from": "gpt", "value": "AIå›å¤"}
        ]
    }
]
```

## è‡ªåŠ¨å‚æ•°

- LoRA Rank: **16**
- è®­ç»ƒè½®æ¬¡: **3 epochs**
- æ‰¹æ¬¡å¤§å°: **4** (æœ‰æ•ˆ batch=16)
- å­¦ä¹ ç‡: **2e-4**
- é‡åŒ–: **4-bit QLoRA**

## è¾“å‡ºä½ç½®

```
Models/LoRA/<ä»»åŠ¡å>_<æ—¶é—´æˆ³>/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â””â”€â”€ tokenizer é…ç½®æ–‡ä»¶
```

## è®­ç»ƒæ—¶é—´ä¼°ç®—

| æ ·æœ¬æ•° | æ—¶é—´ (RTX 3060) |
|--------|----------------|
| 10 | ~5 åˆ†é’Ÿ |
| 100 | ~15 åˆ†é’Ÿ |
| 500 | ~40 åˆ†é’Ÿ |
| 2000 | ~2.5 å°æ—¶ |

## API å¿«é€Ÿå‚è€ƒ

```http
# è·å–æ¨¡å‹åˆ—è¡¨
GET /api/simple-lora/models

# ä¸Šä¼ æ•°æ®é›†
POST /api/simple-lora/upload-dataset

# åˆ›å»ºè®­ç»ƒä»»åŠ¡
POST /api/simple-lora/train

# æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
GET /api/simple-lora/tasks/{id}

# è·å–æ‰€æœ‰ä»»åŠ¡
GET /api/simple-lora/tasks
```

## å¸¸è§é—®é¢˜

### æ˜¾å­˜ä¸è¶³ï¼Ÿ
- å…³é—­å…¶ä»–ç¨‹åº
- é‡å¯ç”µè„‘

### æ•°æ®æ ¼å¼é”™è¯¯ï¼Ÿ
- æ£€æŸ¥ JSON è¯­æ³•
- ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†æµ‹è¯•

### è®­ç»ƒå¤±è´¥ï¼Ÿ
- æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€æ¶ˆæ¯
- æ£€æŸ¥åç«¯æ—¥å¿—

## ç¤ºä¾‹æ•°æ®é›†

ç³»ç»Ÿå·²æä¾›ï¼š
```
TrainingData/example_alpaca_dataset.json
```

ç›´æ¥ä½¿ç”¨ï¼ŒåŒ…å« 10 ä¸ªæ ·æœ¬ï¼

## å®Œæ•´æ–‡æ¡£

- ğŸ“– `docs/ç®€æ˜“LoRAè®­ç»ƒå¿«é€Ÿå¼€å§‹.md`
- ğŸ“š `docs/ç®€æ˜“LoRAè®­ç»ƒåŠŸèƒ½è¯´æ˜.md`
- ğŸ“ `docs/ç®€æ˜“LoRAè®­ç»ƒå®ç°æ€»ç»“.md`
