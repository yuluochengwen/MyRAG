# ç®€æ˜“ LoRA è®­ç»ƒåŠŸèƒ½ä½¿ç”¨è¯´æ˜

## ğŸ“š **åŠŸèƒ½æ¦‚è¿°**

å…¨æ–°çš„ç®€åŒ–ç‰ˆ LoRA å¾®è°ƒç³»ç»Ÿï¼Œä½¿ç”¨ Hugging Face PEFT æ¡†æ¶ï¼Œæ“ä½œç®€å•ï¼Œè‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ã€‚

### **æŠ€æœ¯æ ˆ**
- **è®­ç»ƒæ¡†æ¶**: Hugging Face PEFT (Parameter-Efficient Fine-Tuning)
- **æ¨¡å‹åŠ è½½**: Transformers + BitsAndBytes (4-bit é‡åŒ–)
- **è®­ç»ƒä¼˜åŒ–**: LoRA + Gradient Checkpointing + QLoRA
- **å‰ç«¯**: Tailwind CSS + Vanilla JS

---

## ğŸš€ **å¿«é€Ÿå¼€å§‹**

### **1. å®‰è£…ä¾èµ–**

```powershell
# è¿›å…¥åç«¯ç›®å½•
cd C:\Users\Man\Desktop\MyRAG\Backend

# æ¿€æ´»ç¯å¢ƒ
conda activate MyRAG

# å®‰è£…æ–°çš„è®­ç»ƒä¾èµ–
pip install peft>=0.11.0 datasets>=2.18.0 trl>=0.8.0
```

### **2. åˆå§‹åŒ–æ•°æ®åº“**

æ‰§è¡Œ SQL è„šæœ¬åˆ›å»ºè®­ç»ƒä»»åŠ¡è¡¨ï¼š

```powershell
# ä½¿ç”¨ MySQL å®¢æˆ·ç«¯æ‰§è¡Œ
mysql -u root -p rag_system < scripts/init_simple_lora_tables.sql
```

æˆ–è€…åœ¨ MySQL Workbench ä¸­æ‰§è¡Œï¼š

```sql
-- å¤åˆ¶ Backend/scripts/init_simple_lora_tables.sql çš„å†…å®¹å¹¶æ‰§è¡Œ
```

### **3. å¯åŠ¨åç«¯æœåŠ¡**

```powershell
# ç¡®ä¿åœ¨ MyRAG ç¯å¢ƒä¸­
python Backend/main.py
```

### **4. è®¿é—®è®­ç»ƒé¡µé¢**

æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼š

```
http://localhost:8000/static/simple-lora-training.html
```

---

## ğŸ“– **ä½¿ç”¨æµç¨‹**

### **æ­¥éª¤ 1: å‡†å¤‡æ•°æ®é›†**

æ”¯æŒä¸¤ç§æ ¼å¼ï¼š

#### **Alpaca æ ¼å¼** (æ¨è)

```json
[
    {
        "instruction": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "input": "",
        "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."
    },
    {
        "instruction": "å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±æ–‡",
        "input": "ä»Šå¤©å¤©æ°”çœŸå¥½",
        "output": "The weather is really nice today"
    }
]
```

#### **ShareGPT æ ¼å¼**

```json
[
    {
        "conversations": [
            {"from": "human", "value": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"},
            {"from": "gpt", "value": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹..."}
        ]
    }
]
```

### **æ­¥éª¤ 2: ä¸Šä¼ æ•°æ®é›†**

1. åœ¨è®­ç»ƒé¡µé¢ç‚¹å‡»ä¸Šä¼ åŒºåŸŸæˆ–æ‹–æ‹½ JSON æ–‡ä»¶
2. é€‰æ‹©æ•°æ®é›†æ ¼å¼ï¼ˆAlpaca æˆ– ShareGPTï¼‰
3. ç­‰å¾…ä¸Šä¼ å®Œæˆ

### **æ­¥éª¤ 3: é€‰æ‹©åŸºåº§æ¨¡å‹**

ä»ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©å·²ä¸‹è½½çš„åŸºåº§æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
- `DeepSeek-R1-Distill-Qwen-1.5B`
- å…¶ä»– HuggingFace æ ¼å¼æ¨¡å‹

### **æ­¥éª¤ 4: è¾“å…¥ä»»åŠ¡åç§°**

ä¸ºè®­ç»ƒä»»åŠ¡èµ·ä¸€ä¸ªæ˜“äºè¯†åˆ«çš„åç§°ï¼Œä¾‹å¦‚ï¼š
- `customer_service_lora`
- `medical_qa_v1`

### **æ­¥éª¤ 5: å¼€å§‹è®­ç»ƒ**

ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"æŒ‰é’®ï¼Œç³»ç»Ÿå°†ï¼š
1. è‡ªåŠ¨é…ç½®è®­ç»ƒå‚æ•°ï¼ˆé’ˆå¯¹ RTX 3060 6GB ä¼˜åŒ–ï¼‰
2. åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆ4-bit é‡åŒ–ï¼‰
3. é…ç½® LoRA é€‚é…å™¨
4. å¼€å§‹è®­ç»ƒï¼ˆåå°è¿è¡Œï¼‰
5. è‡ªåŠ¨ä¿å­˜åˆ° `Models/LoRA/<ä»»åŠ¡å>_<æ—¶é—´æˆ³>/`

### **æ­¥éª¤ 6: ç›‘æ§è¿›åº¦**

è®­ç»ƒä»»åŠ¡ä¼šæ˜¾ç¤ºåœ¨å³ä¾§åˆ—è¡¨ä¸­ï¼Œå®æ—¶æ›´æ–°ï¼š
- è¿›åº¦ç™¾åˆ†æ¯”
- å½“å‰çŠ¶æ€ï¼ˆç­‰å¾…ä¸­/è®­ç»ƒä¸­/å·²å®Œæˆ/å¤±è´¥ï¼‰
- å½“å‰è½®æ¬¡
- åˆ›å»ºæ—¶é—´

---

## âš™ï¸ **è‡ªåŠ¨é…ç½®å‚æ•°**

ç³»ç»Ÿå·²é’ˆå¯¹ **RTX 3060 6GB** æ˜¾å¡ä¼˜åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´ï¼š

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|------|------|
| LoRA Rank | 16 | é€‚é…å™¨ç§© |
| LoRA Alpha | 32 | ç¼©æ”¾ç³»æ•° |
| LoRA Dropout | 0.05 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| è®­ç»ƒè½®æ¬¡ | 3 | è¶³å¤Ÿæ”¶æ•› |
| æ‰¹æ¬¡å¤§å° | 4 | æ˜¾å­˜å‹å¥½ |
| æ¢¯åº¦ç´¯ç§¯ | 4 | æœ‰æ•ˆ batch=16 |
| å­¦ä¹ ç‡ | 2e-4 | é€‚ä¸­å­¦ä¹ ç‡ |
| é‡åŒ– | 4-bit | QLoRA æŠ€æœ¯ |
| æœ€å¤§åºåˆ—é•¿åº¦ | 512 | token é™åˆ¶ |

---

## ğŸ“ **è¾“å‡ºç»“æ„**

è®­ç»ƒå®Œæˆåï¼ŒLoRA æ¨¡å‹ä¿å­˜åœ¨ï¼š

```
Models/LoRA/
â””â”€â”€ <ä»»åŠ¡å>_<æ—¶é—´æˆ³>/
    â”œâ”€â”€ adapter_config.json          # LoRA é…ç½®
    â”œâ”€â”€ adapter_model.safetensors    # LoRA æƒé‡
    â”œâ”€â”€ tokenizer_config.json        # åˆ†è¯å™¨é…ç½®
    â””â”€â”€ special_tokens_map.json      # ç‰¹æ®Š token æ˜ å°„
```

---

## ğŸ”§ **API æ¥å£**

### **1. è·å–åŸºåº§æ¨¡å‹åˆ—è¡¨**

```http
GET /api/simple-lora/models
```

**å“åº”ç¤ºä¾‹:**

```json
{
    "models": [
        {
            "name": "DeepSeek-R1-Distill-Qwen-1.5B",
            "path": "C:\\...\\Models\\LLM\\DeepSeek-R1-Distill-Qwen-1.5B",
            "size_mb": 3024.5
        }
    ],
    "count": 1
}
```

### **2. ä¸Šä¼ æ•°æ®é›†**

```http
POST /api/simple-lora/upload-dataset
Content-Type: multipart/form-data

file: <JSONæ–‡ä»¶>
dataset_type: alpaca
```

**å“åº”ç¤ºä¾‹:**

```json
{
    "filename": "my_dataset.json",
    "path": "C:\\...\\TrainingData\\my_dataset.json",
    "size_mb": 2.5,
    "message": "ä¸Šä¼ æˆåŠŸ"
}
```

### **3. åˆ›å»ºè®­ç»ƒä»»åŠ¡**

```http
POST /api/simple-lora/train
Content-Type: multipart/form-data

task_name: my_lora
base_model: DeepSeek-R1-Distill-Qwen-1.5B
dataset_filename: my_dataset.json
dataset_type: alpaca
```

**å“åº”ç¤ºä¾‹:**

```json
{
    "task_id": 1,
    "task_name": "my_lora",
    "status": "running",
    "message": "è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨"
}
```

### **4. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€**

```http
GET /api/simple-lora/tasks/{task_id}
```

**å“åº”ç¤ºä¾‹:**

```json
{
    "task_id": 1,
    "task_name": "my_lora",
    "status": "running",
    "progress": 65.5,
    "current_epoch": 2,
    "message": "å¼€å§‹è®­ç»ƒ...",
    "created_at": "2025-11-20T12:30:00",
    "completed_at": null
}
```

### **5. è·å–æ‰€æœ‰ä»»åŠ¡**

```http
GET /api/simple-lora/tasks
```

---

## ğŸ¯ **è®­ç»ƒå»ºè®®**

### **æ•°æ®é›†å¤§å°**

| æ ·æœ¬æ•° | è®­ç»ƒæ—¶é—´ï¼ˆä¼°ç®—ï¼‰ | å»ºè®®ç”¨é€” |
|--------|-----------------|---------|
| 100-500 | 10-30åˆ†é’Ÿ | æµ‹è¯•ã€å°è§„æ¨¡å¾®è°ƒ |
| 500-2000 | 30-90åˆ†é’Ÿ | ä¸­ç­‰è§„æ¨¡ä»»åŠ¡ |
| 2000-10000 | 1.5-6å°æ—¶ | å¤§è§„æ¨¡ä»»åŠ¡ |

### **æ˜¾å¡å†…å­˜**

- **6GB (RTX 3060)**: å½“å‰å‚æ•°å·²ä¼˜åŒ–ï¼Œæ¨è batch_size=4
- **8GB (RTX 3070)**: å¯å¢åŠ åˆ° batch_size=8
- **12GB+**: å¯è°ƒå¤§ max_seq_length åˆ° 1024

### **æ•°æ®è´¨é‡**

1. **æŒ‡ä»¤æ¸…æ™°**: instruction è¦æ˜ç¡®å…·ä½“
2. **å›ç­”å‡†ç¡®**: output è¦æ­£ç¡®å®Œæ•´
3. **å¤šæ ·æ€§**: è¦†ç›–ä¸åŒåœºæ™¯å’Œé—®é¢˜ç±»å‹
4. **æ ¼å¼ç»Ÿä¸€**: ä¿æŒä¸€è‡´çš„ JSON æ ¼å¼

---

## â— **å¸¸è§é—®é¢˜**

### **Q: è®­ç»ƒå¤±è´¥æ€ä¹ˆåŠï¼Ÿ**

A: æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€ä¸­çš„é”™è¯¯æ¶ˆæ¯ï¼Œå¸¸è§åŸå› ï¼š
- æ˜¾å­˜ä¸è¶³ï¼šå‡å°‘ batch_size æˆ– max_seq_length
- æ•°æ®æ ¼å¼é”™è¯¯ï¼šæ£€æŸ¥ JSON æ ¼å¼æ˜¯å¦æ­£ç¡®
- åŸºåº§æ¨¡å‹ä¸å­˜åœ¨ï¼šç¡®è®¤æ¨¡å‹å·²ä¸‹è½½åˆ° Models/LLM/

### **Q: è®­ç»ƒå®Œæˆåå¦‚ä½•ä½¿ç”¨ï¼Ÿ**

A: æœ‰ä¸¤ç§æ–¹å¼ï¼š
1. **éƒ¨ç½²åˆ° Ollama**ï¼ˆæ¨èï¼‰:
   - è¿›å…¥"æ¨¡å‹ç®¡ç†"é¡µé¢
   - åˆ‡æ¢åˆ°"LoRA ç®¡ç†"æ ‡ç­¾
   - ç‚¹å‡»"æ‰«ææ–°æ¨¡å‹"
   - ç‚¹å‡»"éƒ¨ç½²"æŒ‰é’®

2. **ç›´æ¥åŠ è½½**:
   ```python
   from peft import PeftModel
   model = PeftModel.from_pretrained(base_model, "Models/LoRA/<ä»»åŠ¡å>")
   ```

### **Q: å¯ä»¥ä¸­é€”åœæ­¢è®­ç»ƒå—ï¼Ÿ**

A: ç›®å‰ä¸æ”¯æŒæ‰‹åŠ¨åœæ­¢ï¼Œè®­ç»ƒä¼šåœ¨åå°å®Œæˆã€‚å»ºè®®ï¼š
- å…ˆç”¨å°æ•°æ®é›†æµ‹è¯•
- ç¡®è®¤æ— è¯¯åå†è®­ç»ƒå®Œæ•´æ•°æ®é›†

### **Q: æ”¯æŒå¤š GPU å—ï¼Ÿ**

A: å½“å‰ç‰ˆæœ¬ä½¿ç”¨ `device_map="auto"`ï¼Œä¼šè‡ªåŠ¨åˆ©ç”¨å¯ç”¨çš„ GPUã€‚å¤š GPU è®­ç»ƒéœ€è¦ä¿®æ”¹ä»£ç å¯ç”¨ DeepSpeed æˆ– FSDPã€‚

---

## ğŸ”„ **ä¸æ¨¡å‹ç®¡ç†é¡µé¢é›†æˆ**

è®­ç»ƒå®Œæˆåï¼š

1. è®¿é—® `http://localhost:8000/static/model-management.html`
2. åˆ‡æ¢åˆ°"LoRA ç®¡ç†"æ ‡ç­¾
3. ç‚¹å‡»"æ‰«ææ–°æ¨¡å‹"æŒ‰é’®
4. æ–°è®­ç»ƒçš„ LoRA ä¼šè‡ªåŠ¨å‡ºç°
5. ç‚¹å‡»"éƒ¨ç½²"å³å¯éƒ¨ç½²åˆ° Ollama
6. åœ¨æ™ºèƒ½åŠ©æ‰‹ä¸­ç»‘å®š LoRA æ¨¡å‹

---

## ğŸ“Š **æ•°æ®åº“è¡¨ç»“æ„**

```sql
CREATE TABLE simple_lora_tasks (
    id INT AUTO_INCREMENT PRIMARY KEY,
    task_name VARCHAR(255) NOT NULL,
    base_model VARCHAR(255) NOT NULL,
    dataset_file VARCHAR(512) NOT NULL,
    dataset_type VARCHAR(50) DEFAULT 'alpaca',
    output_path VARCHAR(512) NOT NULL,
    training_params JSON,
    status ENUM('pending', 'running', 'completed', 'failed'),
    progress DECIMAL(5,2) DEFAULT 0.00,
    current_epoch INT DEFAULT 0,
    message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP NULL,
    completed_at TIMESTAMP NULL
);
```

---

## ğŸ‰ **å®Œæˆï¼**

ç°åœ¨æ‚¨å¯ä»¥ï¼š
1. ä¸Šä¼ æ•°æ®é›†
2. ä¸€é”®å¯åŠ¨è®­ç»ƒ
3. å®æ—¶ç›‘æ§è¿›åº¦
4. è‡ªåŠ¨ä¿å­˜æ¨¡å‹
5. è½»æ¾éƒ¨ç½²ä½¿ç”¨

æ— éœ€å¤æ‚é…ç½®ï¼Œä¸“æ³¨äºæ•°æ®å’Œåº”ç”¨åœºæ™¯ï¼
