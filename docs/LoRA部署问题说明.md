# LoRA éƒ¨ç½²åˆ° Ollama - é—®é¢˜è¯´æ˜ä¸è§£å†³æ–¹æ¡ˆ

## ğŸ”´ **å½“å‰é—®é¢˜**

ç®€æ˜“è®­ç»ƒç”Ÿæˆçš„ LoRA æ¨¡å‹æ— æ³•ç›´æ¥éƒ¨ç½²åˆ° Ollamaï¼ŒåŸå› ï¼š

**Ollama çš„ ADAPTER åŠŸèƒ½ä¸å®Œå…¨æ”¯æŒ HuggingFace PEFT æ ¼å¼çš„ LoRA adapter**

---

## ğŸ“– **ä»€ä¹ˆæ˜¯"éƒ¨ç½²åˆ° Ollama"ï¼Ÿ**

### å®Œæ•´æµç¨‹ï¼š

```
1ï¸âƒ£ è®­ç»ƒé˜¶æ®µ
   ä½¿ç”¨ PEFT + Transformers è®­ç»ƒ
   â†“
   ç”Ÿæˆ LoRA adapter æ–‡ä»¶ï¼š
   - adapter_config.json
   - adapter_model.safetensors
   
2ï¸âƒ£ éƒ¨ç½²é˜¶æ®µï¼ˆå½“å‰å¡ä½ï¼‰
   å°† LoRA æ³¨å†Œåˆ° Ollama
   â†“
   ä½¿ LoRA å¯ä»¥é€šè¿‡ Ollama API è°ƒç”¨
   
3ï¸âƒ£ ä½¿ç”¨é˜¶æ®µ
   åœ¨æ™ºèƒ½åŠ©æ‰‹ä¸­ç»‘å®šè¯¥ LoRA
   â†“
   å¯¹è¯æ—¶è‡ªåŠ¨ä½¿ç”¨è¯¥ LoRA çš„èƒ½åŠ›
```

---

## ğŸ› ï¸ **è§£å†³æ–¹æ¡ˆ**

###æ–¹æ¡ˆ 1ï¼šåˆå¹¶ LoRA åéƒ¨ç½²ï¼ˆæ¨èï¼Œè‡ªåŠ¨åŒ–ï¼‰

**åŸç†**ï¼šå°† LoRA åˆå¹¶åˆ°åŸºåº§æ¨¡å‹ï¼Œç”Ÿæˆå®Œæ•´æ¨¡å‹ï¼Œç„¶åå¯¼å…¥ Ollama

**æ­¥éª¤**ï¼š

1. **æ‰‹åŠ¨åˆå¹¶ LoRA**ï¼ˆåœ¨ MyRAG ç¯å¢ƒä¸­ï¼‰ï¼š

```python
# åœ¨ PowerShell ä¸­æ‰§è¡Œ
cd C:\Users\Man\Desktop\MyRAG\Models\LoRA\777_20251120_031210

# åˆ›å»º merge.py æ–‡ä»¶
@'
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from pathlib import Path

# è·¯å¾„é…ç½®
base_model_path = r"C:\Users\Man\Desktop\MyRAG\Models\LLM\DeepSeek-R1-Distill-Qwen-1.5B"
lora_path = r"C:\Users\Man\Desktop\MyRAG\Models\LoRA\777_20251120_031210"
output_path = Path(lora_path) / "merged_model"
output_path.mkdir(exist_ok=True)

print("1/4 åŠ è½½åŸºåº§æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    device_map="cpu",
    trust_remote_code=True
)

print("2/4 åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    base_model_path,
    trust_remote_code=True
)

print("3/4 åŠ è½½å¹¶åˆå¹¶ LoRA...")
model = PeftModel.from_pretrained(base_model, lora_path)
model = model.merge_and_unload()

print("4/4 ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
model.save_pretrained(str(output_path))
tokenizer.save_pretrained(str(output_path))

print(f"âœ… åˆå¹¶å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
'@ | Out-File -FilePath merge.py -Encoding utf8

# æ¿€æ´»ç¯å¢ƒå¹¶æ‰§è¡Œ
conda activate MyRAG
python merge.py
```

2. **å¯¼å…¥åˆ° Ollama**ï¼š

```powershell
# åˆ›å»º Modelfile
cd C:\Users\Man\Desktop\MyRAG\Models\LoRA\777_20251120_031210\merged_model

@'
FROM ./
PARAMETER temperature 0.7
PARAMETER top_p 0.9
'@ | Out-File -FilePath Modelfile -Encoding utf8

# å¯¼å…¥ Ollama
ollama create my-custom-lora -f Modelfile
```

3. **æµ‹è¯•æ¨¡å‹**ï¼š

```powershell
ollama run my-custom-lora "ä½ å¥½"
```

---

### æ–¹æ¡ˆ 2ï¼šè½¬æ¢ä¸º GGUF æ ¼å¼ï¼ˆé«˜çº§ï¼Œæ‰‹åŠ¨ï¼‰

**åŸç†**ï¼šä½¿ç”¨ llama.cpp å·¥å…·å°†æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼

**æ­¥éª¤**ï¼š

1. å…ˆæ‰§è¡Œæ–¹æ¡ˆ1çš„åˆå¹¶æ­¥éª¤
2. ä¸‹è½½ llama.cppï¼šhttps://github.com/ggerganov/llama.cpp
3. è½¬æ¢æ¨¡å‹ï¼š
```bash
python convert.py merged_model/
```
4. é‡åŒ–æ¨¡å‹ï¼ˆå¯é€‰ï¼‰ï¼š
```bash
./quantize merged_model/ggml-model-f16.gguf merged_model/ggml-model-q4_0.gguf q4_0
```
5. å¯¼å…¥ Ollamaï¼š
```powershell
ollama create my-lora-q4 -f ggml-model-q4_0.gguf
```

---

### æ–¹æ¡ˆ 3ï¼šç›´æ¥ä½¿ç”¨ Python APIï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰

**ä¸éƒ¨ç½²åˆ° Ollamaï¼Œç›´æ¥åœ¨ Python ä¸­åŠ è½½ä½¿ç”¨**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# åŠ è½½åŸºåº§
base = AutoModelForCausalLM.from_pretrained("base_model_path")
# åŠ è½½ LoRA
model = PeftModel.from_pretrained(base, "lora_path")

# æ¨ç†
inputs = tokenizer("ä½ å¥½", return_tensors="pt")
outputs = model.generate(**inputs)
```

---

##âš™ï¸ **è‡ªåŠ¨åŒ–æ–¹æ¡ˆï¼ˆå¼€å‘ä¸­ï¼‰**

æˆ‘æ­£åœ¨æ›´æ–°ä»£ç ï¼Œå®ç°è‡ªåŠ¨åˆå¹¶ + éƒ¨ç½²åŠŸèƒ½ï¼š

**æ–°çš„éƒ¨ç½²æµç¨‹**ï¼š

```
ç‚¹å‡»"éƒ¨ç½²"æŒ‰é’®
  â†“
åå°è‡ªåŠ¨æ‰§è¡Œï¼š
  1. æ£€æµ‹åˆ° PEFT LoRA æ ¼å¼
  2. è‡ªåŠ¨åˆå¹¶åˆ°åŸºåº§æ¨¡å‹
  3. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ° merged_model/
  4. åˆ›å»º Modelfile
  5. æ³¨å†Œåˆ° Ollama
  6. æ›´æ–°æ•°æ®åº“çŠ¶æ€
  â†“
éƒ¨ç½²å®Œæˆï¼Œå¯ä»¥ä½¿ç”¨ï¼
```

---

## ğŸ¯ **æ¨èæ“ä½œï¼ˆå½“å‰ï¼‰**

### å¿«é€Ÿæµ‹è¯•æ–¹æ¡ˆï¼š

```powershell
# 1. è¿›å…¥ LoRA ç›®å½•
cd C:\Users\Man\Desktop\MyRAG\Models\LoRA\777_20251120_031210

# 2. æ¿€æ´»ç¯å¢ƒ
conda activate MyRAG

# 3. æ‰§è¡Œåˆå¹¶ï¼ˆå¤åˆ¶ä¸Šé¢çš„ merge.py å†…å®¹ï¼‰
python merge.py

# 4. å¯¼å…¥ Ollama
cd merged_model
ollama create test-777-lora -f Modelfile

# 5. æµ‹è¯•
ollama run test-777-lora "æµ‹è¯•ä¸€ä¸‹è®­ç»ƒæ•ˆæœ"
```

---

## ğŸ“Š **ä¸ºä»€ä¹ˆ Ollama ADAPTER ä¸å·¥ä½œï¼Ÿ**

å¯èƒ½åŸå› ï¼š

1. **æ ¼å¼ä¸å…¼å®¹**ï¼šOllama å¯èƒ½æœŸæœ›ç‰¹å®šæ ¼å¼çš„ LoRA
2. **è·¯å¾„é—®é¢˜**ï¼šWindows è·¯å¾„å¤„ç†é—®é¢˜
3. **ç‰ˆæœ¬é—®é¢˜**ï¼šOllama ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒ PEFT æ ¼å¼
4. **é…ç½®é—®é¢˜**ï¼šadapter_config.json æ ¼å¼ä¸ç¬¦åˆ Ollama é¢„æœŸ

---

## âœ… **ä¸‹ä¸€æ­¥è®¡åˆ’**

æˆ‘ä¼šæ›´æ–°ä»£ç ï¼Œå®ç°ï¼š

1. âœ… è‡ªåŠ¨æ£€æµ‹ LoRA æ ¼å¼
2. âœ… è‡ªåŠ¨åˆå¹¶åˆ°åŸºåº§æ¨¡å‹
3. âœ… è‡ªåŠ¨å¯¼å…¥ Ollama
4. âœ… ä¸€é”®éƒ¨ç½²æµç¨‹

**é¢„è®¡å®Œæˆæ—¶é—´ï¼š10-15 åˆ†é’Ÿ**

---

## ğŸ’¡ **ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼ˆæœ€å¿«ï¼‰**

ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹ï¼Œæ‰‹åŠ¨æ³¨å†Œï¼š

```powershell
# æŒ‰ç…§"æ–¹æ¡ˆ1"çš„æ­¥éª¤æ‰§è¡Œå³å¯
# å¤§çº¦éœ€è¦ 5-10 åˆ†é’Ÿå®Œæˆ
```

---

**ç°åœ¨æ‚¨æ˜ç™½"éƒ¨ç½²"çš„å«ä¹‰äº†å—ï¼Ÿ** ğŸ“

ç®€å•è¯´ï¼š
- âœ… è®­ç»ƒ = ç”Ÿæˆ LoRA adapter æ–‡ä»¶
- âŒ éƒ¨ç½² = è®© Ollama èƒ½å¤Ÿä½¿ç”¨è¿™ä¸ª LoRAï¼ˆå½“å‰å¡ä½ï¼‰
- ğŸ”§ è§£å†³ = å…ˆåˆå¹¶ï¼Œå†å¯¼å…¥ Ollama
