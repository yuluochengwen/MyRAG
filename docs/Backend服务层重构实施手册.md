# BackendæœåŠ¡å±‚é‡æ„å®æ–½æ‰‹å†Œ

> **ç‰ˆæœ¬**: v3.0 æ¶æ„ä¼˜åŒ–ç‰ˆ  
> **åˆ›å»ºæ—¶é—´**: 2025-01-26  
> **é€‚ç”¨é¡¹ç›®**: MyRAGçŸ¥è¯†åº“ç³»ç»Ÿ  
> **é‡æ„å‘¨æœŸ**: 12å‘¨  
> âš ï¸ **é‡è¦è¯´æ˜**: æ‰€æœ‰"ç®€åŒ–"å‡ä¸ºæ¶æ„ä¼˜åŒ–ï¼Œ**ä¸åˆ é™¤ä»»ä½•ä¸šåŠ¡åŠŸèƒ½**ï¼Œä»…é€šè¿‡æ¶ˆé™¤é‡å¤ä»£ç ã€åˆ†å±‚è§£è€¦å®ç°ä»£ç å‡å°‘  

---

## ğŸ“‹ å¿«é€Ÿå¯¼èˆª

| ç« èŠ‚ | å†…å®¹ | æ—¶é—´ |
|------|------|------|
| [ä¸€ã€ç°çŠ¶è¯„ä¼°](#ä¸€ç°çŠ¶è¯„ä¼°) | ä»£ç ç»Ÿè®¡ã€é—®é¢˜è¯†åˆ«ã€ä¼˜å…ˆçº§ | - |
| [äºŒã€é˜¶æ®µ0ï¼šå‡†å¤‡](#äºŒé˜¶æ®µ0å‡†å¤‡week-0) | æµ‹è¯•åŸºå‡†ã€ç¯å¢ƒå‡†å¤‡ | Week 0 |
| [ä¸‰ã€é˜¶æ®µ1ï¼šåŸºç¡€å±‚](#ä¸‰é˜¶æ®µ1åŸºç¡€å±‚week-1-2) | DeviceManager, ModelLoader | Week 1-2 |
| [å››ã€é˜¶æ®µ2ï¼šæ¨¡å‹å±‚](#å››é˜¶æ®µ2æ¨¡å‹å±‚week-3-5) | LLMæŠ½è±¡ã€transformersæ‹†åˆ† | Week 3-5 |
| [äº”ã€é˜¶æ®µ3ï¼šä¸šåŠ¡å±‚](#äº”é˜¶æ®µ3ä¸šåŠ¡å±‚week-6-8) | æ£€ç´¢ç­–ç•¥ã€çŸ¥è¯†åº“é‡æ„ | Week 6-8 |
| [å…­ã€é˜¶æ®µ4ï¼šåº”ç”¨å±‚](#å…­é˜¶æ®µ4åº”ç”¨å±‚week-9-10) | RAG Pipelineã€Agent | Week 9-10 |
| [ä¸ƒã€é˜¶æ®µ5ï¼šæ¸…ç†](#ä¸ƒé˜¶æ®µ5æ¸…ç†week-11-12) | åˆ é™¤æ—§ä»£ç ã€ä¼˜åŒ– | Week 11-12 |

---

## ä¸€ã€ç°çŠ¶è¯„ä¼°

### 1.1 ä»£ç è§„æ¨¡

**18ä¸ªæœåŠ¡æ–‡ä»¶ï¼Œæ€»è®¡6118è¡Œ**ï¼ˆå®é™…ç»Ÿè®¡ï¼‰

| é£é™©çº§åˆ« | æ–‡ä»¶ | è¡Œæ•° | æ ¸å¿ƒé—®é¢˜ |
|---------|------|------|---------|
| ğŸ”´ æé«˜ | transformers_service.py | 776 | 7ä¸ªèŒè´£æ··åˆï¼ˆè®¾å¤‡ç®¡ç†/æ¨¡å‹åŠ è½½/LoRA/æç¤ºè¯/ç”Ÿæˆï¼‰ |
| ğŸ”´ é«˜ | chat_service.py | 561 | 5ä¸ªèŒè´£æ··åˆï¼ˆä¼šè¯ç®¡ç†/RAG/æµå¼è¾“å‡º/å†å²ï¼‰ |
| ğŸŸ¡ ä¸­ | knowledge_base_service.py | 528 | CRUD+æ£€ç´¢+å‘é‡åŒ– |
| ğŸŸ¡ ä¸­ | neo4j_graph_service.py | 513 | å›¾è°±æ„å»º+æŸ¥è¯¢ |
| ğŸŸ¡ ä¸­ | simple_lora_trainer.py | 500 | æ•°æ®é›†å¤„ç†+è®­ç»ƒç®¡ç† |
| ğŸŸ¢ ä½ | å…¶ä»–13ä¸ªæ–‡ä»¶ | 3240 | ç›¸å¯¹å¯æ¥å— |

### 1.2 æ ¸å¿ƒé—®é¢˜ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

| ä¼˜å…ˆçº§ | é—®é¢˜ | å½±å“æ–‡ä»¶ | è§£å†³æ–¹æ¡ˆ | é˜¶æ®µ |
|--------|------|---------|---------|------|
| **P0** | è®¾å¤‡ç®¡ç†é‡å¤4æ¬¡ | transformers, embedding, lora_trainer, ollama | åˆ›å»ºDeviceManager | 1 |
| **P0** | æ¨¡å‹åŠ è½½é‡å¤3æ¬¡ | transformers, embedding, lora_trainer | åˆ›å»ºModelLoader | 1 |
| **P1** | transformers_serviceè¿‡å¤§ | transformers_service (835è¡Œ) | æ‹†åˆ†ä¸º6ä¸ªæ¨¡å— | 2 |
| **P1** | ç¼ºå°‘LLMæŠ½è±¡å±‚ | transformers, ollama | å®šä¹‰BaseLLMæ¥å£ | 2 |
| **P2** | chat_serviceèŒè´£ä¸æ¸… | chat_service (624è¡Œ) | æå–RAG Pipeline | 3 |
| **P2** | æ£€ç´¢ç­–ç•¥åˆ†æ•£ | 3ä¸ªæ–‡ä»¶ | ç­–ç•¥æ¨¡å¼é‡æ„ | 3 |
| **P3** | å·¥å…·ç±»ç¼ºå¤± | entity_extractionç­‰ | åˆ›å»ºJSONParserç­‰ | 4 |

### 1.3 é‡æ„ç›®æ ‡

**æ¶æ„ç›®æ ‡**ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰ï¼š
- â¬‡ï¸ ä»£ç é‡å¤ç‡: >25% â†’ <10% (-60%) - é€šè¿‡æå–å…¬å…±æ¨¡å—
- â¬‡ï¸ æœ€å¤§æ–‡ä»¶: 835è¡Œ â†’ 280è¡Œ (-66%) - é€šè¿‡èŒè´£åˆ†ç¦»
- â¬†ï¸ æ¨¡å—åŒ–ç¨‹åº¦: 18ä¸ªå¹³é“ºæ–‡ä»¶ â†’ 7å¤§æ¨¡å—åˆ†ç±»
- â¬†ï¸ æµ‹è¯•è¦†ç›–ç‡: 30% â†’ 80% (+167%)
- â¬†ï¸ å¯æ‰©å±•æ€§: æ’ä»¶åŒ–æ¶æ„ï¼Œæ˜“äºæ·»åŠ æ–°æ¨¡å‹/æ£€ç´¢ç­–ç•¥

**ä»£ç é‡å˜åŒ–**ï¼ˆè‡ªç„¶ç»“æœï¼‰ï¼š
- serviceså±‚: 6118è¡Œ â†’ 3881è¡Œ (-37%ï¼Œå› ä¸ºé‡å¤ä»£ç ç§»è‡³coreå±‚ï¼‰
- coreå±‚: 200è¡Œ â†’ 2500è¡Œ (+2300è¡Œï¼Œæ–°å¢åŸºç¡€è®¾æ–½ï¼‰
- å‡€æ•ˆæœ: ä»£ç æ€»é‡ç•¥å¢ï¼Œä½†**è´¨é‡å¤§å¹…æå‡**

---

## äºŒã€é˜¶æ®µ0ï¼šå‡†å¤‡ (Week 0)

### ç›®æ ‡
âœ… å»ºç«‹æµ‹è¯•åŸºå‡†  
âœ… å‡†å¤‡é‡æ„ç¯å¢ƒ  
âœ… è¯†åˆ«å›æ»šç‚¹

### ä»»åŠ¡æ¸…å•

**T0.1 æµ‹è¯•åŸºå‡†å»ºç«‹**
```bash
# 1. è¿è¡Œç°æœ‰æµ‹è¯•
cd Backend
pytest app/tests/ --cov=app/services --cov-report=html

# 2. è®°å½•æ€§èƒ½åŸºå‡†
python benchmark/llm_latency.py  # è®°å½•æ¨ç†é€Ÿåº¦
python benchmark/memory_usage.py  # è®°å½•å†…å­˜å ç”¨

# 3. ä¿å­˜ç»“æœ
cp htmlcov docs/test_baseline_before.html
```

**T0.2 ä»£ç åº¦é‡**
```bash
# è®¡ç®—åœˆå¤æ‚åº¦
radon cc app/services -a > docs/complexity_before.txt

# ä»£ç é‡å¤åˆ†æ
pylint app/services --disable=all --enable=duplicate-code > docs/duplication_before.txt
```

**T0.3 ç¯å¢ƒå‡†å¤‡**
```bash
# åˆ›å»ºé‡æ„åˆ†æ”¯
git checkout -b refactor/service-layer-v2

# é…ç½®è‡ªåŠ¨åŒ–æµ‹è¯•
# æ¯æ¬¡æäº¤è‡ªåŠ¨è¿è¡Œæµ‹è¯•
```

### äº¤ä»˜ç‰©
- âœ… `docs/test_baseline.md` - æµ‹è¯•åŸºå‡†æŠ¥å‘Š
- âœ… `docs/code_metrics.md` - ä»£ç åº¦é‡æŠ¥å‘Š
- âœ… Featureåˆ†æ”¯åˆ›å»ºå®Œæˆ

---

## ä¸‰ã€é˜¶æ®µ1ï¼šåŸºç¡€å±‚ (Week 1-2)

### ç›®æ ‡
ğŸ¯ å»ºç«‹åŸºç¡€è®¾æ–½  
ğŸ¯ æ¶ˆé™¤P0çº§åˆ«çš„é‡å¤ä»£ç   
ğŸ¯ ä¸ºåç»­é‡æ„é“ºè·¯

### Week 1: è®¾å¤‡ç®¡ç†ä¸å·¥å…·ç±»

#### T1.1 åˆ›å»ºDeviceManagerï¼ˆ2å¤©ï¼‰

**æ ¸å¿ƒèŒè´£**ï¼šç»Ÿä¸€ç®¡ç†CUDA/MPS/CPUè®¾å¤‡ã€æ˜¾å­˜ç›‘æ§ã€æ˜¾å­˜æ¸…ç†

**æ”¹è¿›ç‚¹**ï¼ˆåŸºäºå¯è¡Œæ€§éªŒè¯ï¼‰ï¼š

- âœ… è¡¥å……MPSæ”¯æŒï¼ˆApple Siliconï¼‰
- âœ… æ·»åŠ GPUåç§°æŸ¥è¯¢æ–¹æ³•
- âœ… å®Œå–„è®¾å¤‡åˆå§‹åŒ–é…ç½®
- âœ… æ·»åŠ è®¾å¤‡ç±»å‹åˆ¤æ–­å±æ€§ï¼ˆis_cuda/is_mps/is_cpuï¼‰

**ä»£ç ç¤ºä¾‹**ï¼š

```python
# Backend/app/core/device/gpu_manager.py
import torch
from typing import Dict
from app.utils.logger import get_logger

logger = get_logger(__name__)

class DeviceManager:
    """è®¾å¤‡ç®¡ç†å™¨ï¼ˆæ”¯æŒCUDA/MPS/CPUï¼‰"""
    
    def __init__(self):
        self.device = self._detect_device()
        self.device_name = self._get_device_name()
        self._init_device_settings()
        logger.info(f"è®¾å¤‡åˆå§‹åŒ–: {self.device} ({self.device_name})")
        
        if self.device == "cuda":
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"GPUæ˜¾å­˜: {total_memory:.2f}GB")
    
    def _detect_device(self) -> str:
        """æ£€æµ‹å¯ç”¨è®¾å¤‡ï¼ˆä¼˜å…ˆçº§ï¼šCUDA > MPS > CPUï¼‰"""
        if torch.cuda.is_available():
            return "cuda"
        # Apple Silicon æ”¯æŒ
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    
    def _get_device_name(self) -> str:
        """è·å–è®¾å¤‡åç§°"""
        if self.device == "cuda":
            return torch.cuda.get_device_name(0)
        elif self.device == "mps":
            return "Apple Silicon (MPS)"
        return "CPU"
    
    def _init_device_settings(self):
        """åˆå§‹åŒ–è®¾å¤‡é…ç½®"""
        if self.device == "cuda":
            torch.backends.cudnn.benchmark = True
    
    def get_memory_info(self) -> Dict:
        """è·å–æ˜¾å­˜/å†…å­˜ä¿¡æ¯ï¼ˆGBï¼‰"""
        if self.device == "cuda":
            return {
                "allocated_gb": torch.cuda.memory_allocated(0) / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved(0) / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                "device_name": self.device_name
            }
        elif self.device == "mps":
            return {"device_name": self.device_name}
        return {"device_name": "CPU"}
    
    def cleanup(self):
        """æ¸…ç†æ˜¾å­˜ç¼“å­˜"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
    
    def get_quantization_config(self):
        """è·å–INT4é‡åŒ–é…ç½®ï¼ˆä»…CUDAæ”¯æŒï¼‰"""
        if self.device != "cuda":
            return None
        
        from transformers import BitsAndBytesConfig
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )
    
    @property
    def is_cuda(self) -> bool:
        """æ˜¯å¦ä¸ºCUDAè®¾å¤‡"""
        return self.device == "cuda"
    
    @property
    def is_mps(self) -> bool:
        """æ˜¯å¦ä¸ºMPSè®¾å¤‡ï¼ˆApple Siliconï¼‰"""
        return self.device == "mps"
    
    @property
    def is_cpu(self) -> bool:
        """æ˜¯å¦ä¸ºCPUè®¾å¤‡"""
        return self.device == "cpu"
```

**æµ‹è¯•ä»£ç **ï¼š
```python
# Backend/app/tests/test_device_manager.py
def test_device_manager_init():
    dm = DeviceManager()
    assert dm.device in ["cuda", "cpu"]

def test_get_device_info():
    dm = DeviceManager()
    info = dm.get_device_info()
    assert info.device_type in ["cuda", "cpu"]
    assert info.total_memory_gb >= 0
```

**æ›¿æ¢æ—§ä»£ç **ï¼š
```python
# åœ¨transformers_service.py, embedding_service.pyç­‰4ä¸ªæ–‡ä»¶ä¸­æ›¿æ¢ï¼š
# æ—§ä»£ç ï¼š
# self.device = "cuda" if torch.cuda.is_available() else "cpu"

# æ–°ä»£ç ï¼š
from app.core.device.gpu_manager import DeviceManager
self.device_manager = DeviceManager()
self.device = self.device_manager.device
```

---

#### T1.2 åˆ›å»ºåŸºç¡€å·¥å…·ç±»ï¼ˆ3å¤©ï¼‰

**JSONParser - ç»Ÿä¸€JSONè§£æå®¹é”™**ï¼š
```python
# Backend/app/core/utils/json_parser.py
import json
import re
from typing import Optional, Dict, Any

class JSONParser:
    """ç»Ÿä¸€çš„JSONè§£æå®¹é”™å·¥å…·"""
    
    @staticmethod
    def extract_json(text: str, fallback: Optional[Dict] = None) -> Dict[str, Any]:
        """
        3å±‚é™çº§ç­–ç•¥æå–JSON
        
        Args:
            text: å¯èƒ½åŒ…å«JSONçš„æ–‡æœ¬
            fallback: è§£æå¤±è´¥æ—¶è¿”å›çš„é»˜è®¤å€¼
            
        Returns:
            è§£æåçš„å­—å…¸
        """
        # ç¬¬1å±‚ï¼šç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass
        
        # ç¬¬2å±‚ï¼šæå–ä»£ç å—
        try:
            if '```json' in text:
                start = text.find('```json') + 7
                end = text.find('```', start)
                json_str = text[start:end].strip()
            elif '```' in text:
                start = text.find('```') + 3
                end = text.find('```', start)
                json_str = text[start:end].strip()
            else:
                raise ValueError("No code block found")
            
            return json.loads(json_str)
        except:
            pass
        
        # ç¬¬3å±‚ï¼šæŸ¥æ‰¾{}
        try:
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > 0:
                json_str = text[start:end]
                return json.loads(json_str)
        except:
            pass
        
        # å…¨éƒ¨å¤±è´¥ï¼Œè¿”å›fallback
        return fallback or {}
```

**PathResolver - ç»Ÿä¸€è·¯å¾„ç®¡ç†**ï¼š
```python
# Backend/app/core/utils/path_resolver.py
from pathlib import Path
from app.core.config import settings

class PathResolver:
    """ç»Ÿä¸€çš„è·¯å¾„è§£ææœåŠ¡"""
    
    def __init__(self):
        self.base_dir = Path(settings.file.upload_dir).parent
    
    def get_model_path(self, model_type: str, model_name: str) -> Path:
        """
        è·å–æ¨¡å‹è·¯å¾„
        
        Args:
            model_type: llm / embedding / lora
            model_name: æ¨¡å‹åç§°
            
        Returns:
            å®Œæ•´è·¯å¾„
        """
        type_map = {
            "llm": "Models/LLM",
            "embedding": "Models/Embedding",
            "lora": "Models/LoRA"
        }
        return self.base_dir / type_map[model_type] / model_name
    
    def get_kb_path(self, kb_id: int) -> Path:
        """è·å–çŸ¥è¯†åº“è·¯å¾„"""
        return self.base_dir / "KnowledgeBase" / f"kb_{kb_id}"
    
    def get_training_data_path(self, filename: str) -> Path:
        """è·å–è®­ç»ƒæ•°æ®è·¯å¾„"""
        return self.base_dir / "TrainingData" / filename
```

**ProcessManager - è¿›ç¨‹ç®¡ç†**ï¼š
```python
# Backend/app/core/utils/process_manager.py
import subprocess
import psutil
import time
from typing import List, Optional, Dict, Any

class ProcessManager:
    """ç»Ÿä¸€çš„è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    
    def start_process(
        self,
        cmd: List[str],
        wait_time: int = 5,
        log_file: Optional[str] = None
    ) -> int:
        """
        å¯åŠ¨è¿›ç¨‹å¹¶ç­‰å¾…å°±ç»ª
        
        Args:
            cmd: å‘½ä»¤åˆ—è¡¨
            wait_time: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            
        Returns:
            è¿›ç¨‹PID
        """
        # Windowsç‰¹æ®Šå¤„ç†
        creation_flags = (
            subprocess.CREATE_NEW_PROCESS_GROUP 
            if hasattr(subprocess, 'CREATE_NEW_PROCESS_GROUP') 
            else 0
        )
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
        log_handle = open(log_file, 'w') if log_file else subprocess.PIPE
        
        process = subprocess.Popen(
            cmd,
            stdout=log_handle,
            stderr=subprocess.STDOUT,
            creationflags=creation_flags
        )
        
        # ç­‰å¾…è¿›ç¨‹å¯åŠ¨
        time.sleep(wait_time)
        
        # éªŒè¯è¿›ç¨‹å­˜åœ¨
        if not psutil.pid_exists(process.pid):
            raise RuntimeError(f"è¿›ç¨‹å¯åŠ¨å¤±è´¥: PID {process.pid}")
        
        return process.pid
    
    def stop_process(self, pid: int, timeout: int = 10) -> bool:
        """
        ä¼˜é›…åœæ­¢è¿›ç¨‹ï¼ˆterminate â†’ wait â†’ killï¼‰
        
        Args:
            pid: è¿›ç¨‹ID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸåœæ­¢
        """
        try:
            process = psutil.Process(pid)
            
            # 1. å°è¯•ä¼˜é›…ç»ˆæ­¢
            process.terminate()
            
            # 2. ç­‰å¾…è¿›ç¨‹ç»“æŸ
            try:
                process.wait(timeout=timeout)
                return True
            except psutil.TimeoutExpired:
                # 3. è¶…æ—¶åå¼ºåˆ¶æ€æ­»
                process.kill()
                return True
                
        except psutil.NoSuchProcess:
            return True  # è¿›ç¨‹å·²ä¸å­˜åœ¨
        except Exception as e:
            return False
    
    def get_process_status(self, pid: int) -> Dict[str, Any]:
        """è·å–è¿›ç¨‹çŠ¶æ€"""
        try:
            process = psutil.Process(pid)
            return {
                "pid": pid,
                "status": process.status(),
                "running": process.is_running(),
                "cpu_percent": process.cpu_percent(),
                "memory_mb": process.memory_info().rss / 1024 / 1024
            }
        except psutil.NoSuchProcess:
            return {"pid": pid, "running": False}
```

**TaskStateManager - ä»»åŠ¡çŠ¶æ€ç®¡ç†**ï¼š
```python
# Backend/app/core/utils/task_state_manager.py
from enum import Enum
from typing import Dict, List, Optional

class TaskState(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class TaskStateManager:
    """ç»Ÿä¸€çš„ä»»åŠ¡çŠ¶æ€ç®¡ç†ï¼ˆå¸¦çŠ¶æ€æœºéªŒè¯ï¼‰"""
    
    # åˆæ³•çš„çŠ¶æ€è½¬æ¢
    TRANSITIONS = {
        TaskState.PENDING: [TaskState.RUNNING, TaskState.FAILED],
        TaskState.RUNNING: [TaskState.COMPLETED, TaskState.FAILED],
        TaskState.COMPLETED: [],  # ç»ˆæ€
        TaskState.FAILED: []       # ç»ˆæ€
    }
    
    def can_transition(
        self,
        from_state: TaskState,
        to_state: TaskState
    ) -> bool:
        """éªŒè¯çŠ¶æ€è½¬æ¢æ˜¯å¦åˆæ³•"""
        return to_state in self.TRANSITIONS.get(from_state, [])
    
    def update_task_status(
        self,
        db_connection,
        table_name: str,
        task_id: int,
        new_state: TaskState,
        **extra_fields
    ) -> bool:
        """
        æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆè‡ªåŠ¨éªŒè¯åˆæ³•æ€§ï¼‰
        
        Args:
            db_connection: æ•°æ®åº“è¿æ¥
            table_name: è¡¨å
            task_id: ä»»åŠ¡ID
            new_state: æ–°çŠ¶æ€
            **extra_fields: é¢å¤–å­—æ®µï¼ˆprogress, messageç­‰ï¼‰
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        # 1. è·å–å½“å‰çŠ¶æ€
        with db_connection.cursor() as cursor:
            cursor.execute(
                f"SELECT status FROM {table_name} WHERE id = %s",
                (task_id,)
            )
            result = cursor.fetchone()
            if not result:
                return False
            
            current_state = TaskState(result['status'])
        
        # 2. éªŒè¯è½¬æ¢åˆæ³•æ€§
        if not self.can_transition(current_state, new_state):
            raise ValueError(
                f"éæ³•çŠ¶æ€è½¬æ¢: {current_state.value} â†’ {new_state.value}"
            )
        
        # 3. æ‰§è¡Œæ›´æ–°
        update_fields = ["status = %s"]
        params = [new_state.value]
        
        for field, value in extra_fields.items():
            update_fields.append(f"{field} = %s")
            params.append(value)
        
        # æ·»åŠ æ—¶é—´æˆ³
        if new_state == TaskState.COMPLETED:
            update_fields.append("completed_at = NOW()")
        
        params.append(task_id)
        
        with db_connection.cursor() as cursor:
            sql = f"UPDATE {table_name} SET {', '.join(update_fields)} WHERE id = %s"
            cursor.execute(sql, params)
            db_connection.commit()
        
        return True
```

---

### Week 2: æ¨¡å‹åŠ è½½å™¨

#### T1.3 åˆ›å»ºModelLoaderï¼ˆ3å¤©ï¼‰

**æ ¸å¿ƒèŒè´£**ï¼šç»Ÿä¸€æ¨¡å‹åŠ è½½ã€é‡åŒ–é…ç½®ã€LoRAåˆå¹¶ã€æ˜¾å­˜ç›‘æ§

**æ”¹è¿›ç‚¹**ï¼ˆåŸºäºéªŒè¯ç»“æœï¼‰ï¼š
- âœ… è¡¥å……æ¨¡å‹å¤§å°ä¼°ç®—ï¼ˆå†³å®šåŠ è½½ç­–ç•¥ï¼‰
- âœ… æ·»åŠ å°æ¨¡å‹ä¼˜åŒ–ï¼ˆ<2GBä¸ä½¿ç”¨device_mapï¼‰
- âœ… æ”¯æŒFlash Attentionæ£€æµ‹
- âœ… å®Œå–„Tokenizerå®¹é”™ï¼ˆfast/slowé™çº§ï¼‰
- âœ… æ·»åŠ æ˜¾å­˜ç›‘æ§ï¼ˆåŠ è½½å‰åå¯¹æ¯”ï¼‰
- âœ… å®ç°æ¨¡å‹ç¼“å­˜ç®¡ç†ï¼ˆå¸è½½æ—§æ¨¡å‹ï¼‰
- âœ… å®Œå–„LoRAåŠ è½½é€»è¾‘

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# Backend/app/core/model/model_loader.py
import json
from pathlib import Path
from typing import Optional, Tuple, Any
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from app.core.device.gpu_manager import DeviceManager
from app.utils.logger import get_logger

logger = get_logger(__name__)

class ModelLoader:
    """ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½å™¨ï¼ˆæ”¯æŒæ™®é€š/é‡åŒ–/LoRAï¼‰"""
    
    def __init__(self, device_manager: DeviceManager):
        self.device_manager = device_manager
        self.current_model = None
        self.current_tokenizer = None
        self.current_model_name = None
    
    def estimate_model_size(self, model_path: Path) -> float:
        """
        ä¼°ç®—INT4é‡åŒ–åçš„æ¨¡å‹å¤§å°ï¼ˆGBï¼‰
        ç”¨äºå†³å®šåŠ è½½ç­–ç•¥ï¼ˆå°æ¨¡å‹ä¸ä½¿ç”¨device_mapï¼‰
        """
        try:
            # æ–¹æ³•1: ä»config.jsonä¼°ç®—å‚æ•°é‡
            config_file = model_path / "config.json"
            if config_file.exists():
                with open(config_file) as f:
                    config = json.load(f)
                
                vocab_size = config.get("vocab_size", 32000)
                hidden_size = config.get("hidden_size", 2048)
                num_layers = config.get("num_hidden_layers", 24)
                
                # ç²—ç•¥ä¼°ç®—å‚æ•°é‡ï¼ˆbillionï¼‰
                params_b = (vocab_size * hidden_size + 
                           num_layers * hidden_size * hidden_size * 4) / 1e9
                
                # INT4: 0.5 bytes per parameter
                return params_b * 0.5
        except Exception as e:
            logger.warning(f"æ— æ³•ä»config.jsonä¼°ç®—æ¨¡å‹å¤§å°: {e}")
        
        try:
            # æ–¹æ³•2: è®¡ç®—safetensorsæ–‡ä»¶å¤§å°
            total_size = sum(
                f.stat().st_size 
                for f in model_path.rglob('*.safetensors')
            ) / 1024**3
            # INT4 çº¦ä¸ºåŸå§‹å¤§å°çš„ 1/4
            return total_size * 0.25
        except:
            return 0.0
    
    async def load(
        self,
        model_path: Path,
        quantize: bool = True,
        lora_path: Optional[Path] = None,
        enable_flash_attention: bool = True
    ) -> Tuple[Any, Any]:
        """
        ç»Ÿä¸€çš„åŠ è½½å…¥å£
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            quantize: æ˜¯å¦é‡åŒ–
            lora_path: LoRAè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            enable_flash_attention: æ˜¯å¦å°è¯•å¯ç”¨Flash Attention
            
        Returns:
            (model, tokenizer)
        """
        # 1. å¸è½½æ—§æ¨¡å‹
        self._unload_current_model()
        
        # 2. åŠ è½½tokenizer
        tokenizer = self._load_tokenizer(model_path)
        
        # 3. ä¼°ç®—æ¨¡å‹å¤§å°ï¼Œå†³å®šåŠ è½½ç­–ç•¥
        model_size_gb = self.estimate_model_size(model_path)
        logger.info(f"ä¼°ç®—æ¨¡å‹å¤§å°: {model_size_gb:.2f} GB (INT4é‡åŒ–å)")
        
        # 4. åŠ è½½åŸºåº§æ¨¡å‹
        model = self._load_base_model(
            model_path, 
            quantize, 
            model_size_gb,
            enable_flash_attention
        )
        
        # 5. åº”ç”¨LoRAï¼ˆå¦‚æœæœ‰ï¼‰
        if lora_path:
            model = self._apply_lora(model, lora_path)
        
        # 6. ç¼“å­˜å½“å‰æ¨¡å‹
        self.current_model = model
        self.current_tokenizer = tokenizer
        self.current_model_name = model_path.name
        
        return model, tokenizer
    
    def _load_tokenizer(self, model_path: Path):
        """åŠ è½½tokenizerï¼ˆä¼˜å…ˆfastï¼Œå¤±è´¥é™çº§åˆ°slowï¼‰"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                use_fast=True
            )
            logger.info("âœ“ Fast tokenizer åŠ è½½æˆåŠŸ")
            return tokenizer
        except Exception as e:
            logger.warning(f"Fast tokenizer å¤±è´¥ï¼Œå›é€€åˆ° slow tokenizer: {e}")
            return AutoTokenizer.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                use_fast=False
            )
    
    def _load_base_model(
        self, 
        model_path: Path, 
        quantize: bool,
        model_size_gb: float,
        enable_flash_attention: bool
    ):
        """åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆæ™ºèƒ½ä¼˜åŒ–ï¼‰"""
        load_kwargs = {
            "pretrained_model_name_or_path": str(model_path),
            "trust_remote_code": True,
            "torch_dtype": torch.float16,
            "low_cpu_mem_usage": True,
        }
        
        # Flash Attention æ£€æµ‹
        if enable_flash_attention:
            try:
                import flash_attn
                load_kwargs["attn_implementation"] = "flash_attention_2"
                logger.info("âœ“ Flash Attention 2 å·²å¯ç”¨")
            except ImportError:
                logger.info("Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å®ç°")
        
        # é‡åŒ–é…ç½®
        if quantize and self.device_manager.is_cuda:
            load_kwargs["quantization_config"] = (
                self.device_manager.get_quantization_config()
            )
            
            # å°æ¨¡å‹ä¼˜åŒ–ï¼š<2GBä¸ä½¿ç”¨device_mapï¼ˆé¿å…é¢å¤–å¼€é”€ï¼‰
            if model_size_gb < 2.0:
                logger.info("å°æ¨¡å‹æ£€æµ‹ï¼Œç›´æ¥åŠ è½½åˆ°GPUï¼ˆé¿å…device_mapå¼€é”€ï¼‰")
                load_kwargs["device_map"] = None
            else:
                logger.info("å¤§æ¨¡å‹æ£€æµ‹ï¼Œä½¿ç”¨device_map=auto")
                load_kwargs["device_map"] = "auto"
                load_kwargs["max_memory"] = {0: "5.5GiB", "cpu": "0GiB"}
        elif self.device_manager.is_cuda:
            load_kwargs["device_map"] = "auto"
            load_kwargs["max_memory"] = {0: "5.5GiB", "cpu": "0GiB"}
        
        # æ˜¾å­˜ç›‘æ§ï¼šåŠ è½½å‰
        memory_before = self.device_manager.get_memory_info()
        if "allocated_gb" in memory_before:
            logger.info(f"åŠ è½½å‰æ˜¾å­˜: {memory_before['allocated_gb']:.2f}GB å·²åˆ†é…")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(**load_kwargs)
        model.eval()
        
        # æ˜¾å­˜ç›‘æ§ï¼šåŠ è½½å
        memory_after = self.device_manager.get_memory_info()
        if "allocated_gb" in memory_after:
            delta = memory_after["allocated_gb"] - memory_before.get("allocated_gb", 0)
            logger.info(f"åŠ è½½åæ˜¾å­˜: {memory_after['allocated_gb']:.2f}GB (+{delta:.2f}GB)")
            
            total_gb = memory_after.get("total_gb", 0)
            if total_gb > 0:
                utilization = (memory_after["allocated_gb"] / total_gb) * 100
                logger.info(f"æ˜¾å­˜åˆ©ç”¨ç‡: {utilization:.1f}%")
        
        return model
    
    def _apply_lora(self, base_model, lora_path: Path):
        """åº”ç”¨LoRAé€‚é…å™¨"""
        logger.info(f"åº”ç”¨LoRAé€‚é…å™¨: {lora_path}")
        model = PeftModel.from_pretrained(
            base_model,
            str(lora_path),
            torch_dtype=torch.float16
        )
        # å¯é€‰ï¼šåˆå¹¶æƒé‡ï¼ˆæé«˜æ¨ç†é€Ÿåº¦ï¼‰
        # model = model.merge_and_unload()
        return model
    
    def _unload_current_model(self):
        """å¸è½½å½“å‰æ¨¡å‹ï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰"""
        if self.current_model is not None:
            logger.info(f"å¸è½½æ—§æ¨¡å‹: {self.current_model_name}")
            del self.current_model
            del self.current_tokenizer
            self.device_manager.cleanup()
            self.current_model = None
            self.current_tokenizer = None
            self.current_model_name = None
```

---

### é˜¶æ®µ1æ€»ç»“

**å®Œæˆæ ‡å‡†**ï¼š
- âœ… æ‰€æœ‰æ–°æ¨¡å—å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆè¦†ç›–ç‡>80%ï¼‰
- âœ… åœ¨4ä¸ªæœåŠ¡ä¸­æˆåŠŸæ›¿æ¢è®¾å¤‡ç®¡ç†ä»£ç 
- âœ… åœ¨3ä¸ªæœåŠ¡ä¸­æˆåŠŸæ›¿æ¢æ¨¡å‹åŠ è½½ä»£ç 
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼ˆæ— ä¸‹é™ï¼‰

**é¢„æœŸæ•ˆæœ**ï¼š
- â¬‡ï¸ ä»£ç é‡å¤ç‡: 25% â†’ 18% (-28%)
- â¬‡ï¸ è®¾å¤‡ç®¡ç†é‡å¤: 4å¤„ â†’ 1å¤„ (-75%)
- â¬‡ï¸ æ¨¡å‹åŠ è½½é‡å¤: 3å¤„ â†’ 1å¤„ (-66%)

**ä¸‹ä¸€æ­¥**ï¼šè¿›å…¥é˜¶æ®µ2 - æ¨¡å‹æœåŠ¡å±‚é‡æ„

---

## å››ã€é˜¶æ®µ2ï¼šæ¨¡å‹å±‚ (Week 3-5)

### ç›®æ ‡
ğŸ¯ å»ºç«‹ç»Ÿä¸€çš„LLMæŠ½è±¡å±‚  
ğŸ¯ æ‹†åˆ†transformers_serviceï¼ˆ835è¡Œ â†’ 280è¡Œï¼‰  
ğŸ¯ å®ç°æ¨¡å‹æœåŠ¡çš„å¯æ’æ‹”æ¶æ„

### Week 3: LLMæŠ½è±¡æ¥å£

#### T2.1 å®šä¹‰BaseLLMæ¥å£ï¼ˆ2å¤©ï¼‰

**æ ¸å¿ƒæ¥å£è®¾è®¡**ï¼š
```python
# Backend/app/core/llm/base_llm.py
from abc import ABC, abstractmethod
from typing import AsyncGenerator, Dict, Any, Optional, List
from dataclasses import dataclass

@dataclass
class LLMConfig:
    """LLMé€šç”¨é…ç½®"""
    model_name: str
    temperature: float = 0.7
    max_new_tokens: int = 512
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.1

@dataclass
class Message:
    """æ¶ˆæ¯ç»“æ„"""
    role: str  # system/user/assistant
    content: str

class BaseLLM(ABC):
    """æ‰€æœ‰LLMæœåŠ¡çš„åŸºç±»"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.model_name = config.model_name
    
    @abstractmethod
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¼‚æ­¥åŠ è½½ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        pass
    
    @abstractmethod
    async def generate(
        self,
        messages: List[Message],
        stream: bool = False,
        **kwargs
    ) -> str | AsyncGenerator[str, None]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒæµå¼/éæµå¼ï¼‰
        
        Args:
            messages: å¯¹è¯å†å²
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: é¢å¤–å‚æ•°ï¼ˆè¦†ç›–configï¼‰
            
        Returns:
            å®Œæ•´å›å¤ æˆ– æµå¼ç”Ÿæˆå™¨
        """
        pass
    
    @abstractmethod
    async def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            {
                "name": "æ¨¡å‹åç§°",
                "type": "transformers/ollama/openai",
                "status": "loaded/loading/error",
                "device": "cuda/cpu",
                "memory_usage_mb": 1234,
                "loaded_adapters": ["lora1", "lora2"]  # ä»…LoRAæ¨¡å‹
            }
        """
        pass
    
    @abstractmethod
    async def cleanup(self):
        """æ¸…ç†èµ„æºï¼ˆå¸è½½æ¨¡å‹ï¼‰"""
        pass
    
    def _merge_config(self, **kwargs) -> Dict[str, Any]:
        """åˆå¹¶é…ç½®å‚æ•°ï¼ˆkwargsä¼˜å…ˆï¼‰"""
        base = {
            "temperature": self.config.temperature,
            "max_new_tokens": self.config.max_new_tokens,
            "top_p": self.config.top_p,
            "top_k": self.config.top_k,
            "repetition_penalty": self.config.repetition_penalty
        }
        base.update(kwargs)
        return base
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š
```python
# Backend/app/tests/test_base_llm.py
import pytest
from app.core.llm.base_llm import BaseLLM, LLMConfig, Message

class MockLLM(BaseLLM):
    """æµ‹è¯•ç”¨çš„Mockå®ç°"""
    
    async def initialize(self) -> bool:
        return True
    
    async def generate(self, messages, stream=False, **kwargs):
        if stream:
            async def gen():
                yield "Hello"
                yield " World"
            return gen()
        return "Hello World"
    
    async def get_model_info(self):
        return {
            "name": self.model_name,
            "type": "mock",
            "status": "loaded"
        }
    
    async def cleanup(self):
        pass

@pytest.mark.asyncio
async def test_llm_config_merge():
    config = LLMConfig(model_name="test", temperature=0.5)
    llm = MockLLM(config)
    
    merged = llm._merge_config(temperature=0.9, max_new_tokens=1024)
    assert merged["temperature"] == 0.9  # kwargsä¼˜å…ˆ
    assert merged["max_new_tokens"] == 1024
    assert merged["top_p"] == 0.9  # ä½¿ç”¨é»˜è®¤å€¼

@pytest.mark.asyncio
async def test_llm_generate():
    config = LLMConfig(model_name="test")
    llm = MockLLM(config)
    
    messages = [Message(role="user", content="Hi")]
    response = await llm.generate(messages)
    assert response == "Hello World"
```

---

#### T2.2 å®ç°OllamaLLMï¼ˆ1å¤©ï¼‰

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# Backend/app/core/llm/ollama_llm.py
import httpx
from typing import AsyncGenerator, List
from app.core.llm.base_llm import BaseLLM, Message

class OllamaLLM(BaseLLM):
    """Ollama LLMå®ç°"""
    
    def __init__(self, config, base_url: str = "http://localhost:11434"):
        super().__init__(config)
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=300.0)
    
    async def initialize(self) -> bool:
        """æ£€æŸ¥Ollamaæ˜¯å¦åœ¨çº¿"""
        try:
            response = await self.client.get(f"{self.base_url}/api/tags")
            models = response.json().get("models", [])
            return any(m["name"] == self.model_name for m in models)
        except:
            return False
    
    async def generate(
        self,
        messages: List[Message],
        stream: bool = False,
        **kwargs
    ):
        """è°ƒç”¨Ollama API"""
        gen_config = self._merge_config(**kwargs)
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": m.role, "content": m.content} for m in messages],
            "stream": stream,
            "options": {
                "temperature": gen_config["temperature"],
                "num_predict": gen_config["max_new_tokens"],
                "top_p": gen_config["top_p"],
                "top_k": gen_config["top_k"],
                "repeat_penalty": gen_config["repetition_penalty"]
            }
        }
        
        if stream:
            return self._stream_generate(payload)
        else:
            return await self._sync_generate(payload)
    
    async def _sync_generate(self, payload: dict) -> str:
        """éæµå¼ç”Ÿæˆ"""
        response = await self.client.post(
            f"{self.base_url}/api/chat",
            json=payload
        )
        return response.json()["message"]["content"]
    
    async def _stream_generate(self, payload: dict) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆ"""
        async with self.client.stream(
            "POST",
            f"{self.base_url}/api/chat",
            json=payload
        ) as response:
            async for line in response.aiter_lines():
                if line:
                    data = json.loads(line)
                    if "message" in data:
                        yield data["message"]["content"]
    
    async def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        try:
            response = await self.client.post(
                f"{self.base_url}/api/show",
                json={"name": self.model_name}
            )
            info = response.json()
            return {
                "name": self.model_name,
                "type": "ollama",
                "status": "loaded",
                "parameters": info.get("parameters", "unknown"),
                "size_gb": info.get("size", 0) / 1024**3
            }
        except:
            return {"name": self.model_name, "type": "ollama", "status": "error"}
    
    async def cleanup(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        await self.client.aclose()
```

---

#### T2.3 ç»Ÿä¸€EmbeddingæœåŠ¡ï¼ˆ2å¤©ï¼‰

**å½“å‰é—®é¢˜**ï¼š
- `embedding_service.py` (334è¡Œ)ï¼šTransformers embeddingå®ç°
- `ollama_embedding_service.py` (204è¡Œ)ï¼šOllama embeddingå®ç°
- **é‡å¤ä»£ç **ï¼šä¸¤ä¸ªæœåŠ¡æœ‰ç›¸ä¼¼çš„æ¥å£å’Œé”™è¯¯å¤„ç†

**é‡æ„æ–¹æ¡ˆ**ï¼šç»Ÿä¸€ä¸ºä¸€ä¸ªEmbeddingServiceï¼Œæ”¯æŒå¤šç§åç«¯

```python
# Backend/app/services/llm/embedding_service.pyï¼ˆé‡æ„åï¼‰
from typing import List, Optional
from pathlib import Path
import numpy as np

class EmbeddingService:
    """ç»Ÿä¸€çš„EmbeddingæœåŠ¡ï¼ˆæ”¯æŒTransformerså’ŒOllamaï¼‰"""
    
    def __init__(
        self,
        backend: str = "transformers",  # transformers | ollama
        model_name: str = "BAAI/bge-small-zh-v1.5"
    ):
        self.backend = backend
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        if self.backend == "transformers":
            return await self._init_transformers()
        elif self.backend == "ollama":
            return await self._init_ollama()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„backend: {self.backend}")
    
    async def _init_transformers(self) -> bool:
        """åˆå§‹åŒ–Transformersæ¨¡å‹"""
        from transformers import AutoModel, AutoTokenizer
        from app.core.device.gpu_manager import DeviceManager
        
        device_manager = DeviceManager()
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(
                self.model_name,
                trust_remote_code=True
            ).to(device_manager.device)
            self.model.eval()
            return True
        except Exception as e:
            print(f"Transformersåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def _init_ollama(self) -> bool:
        """åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯"""
        import httpx
        self.client = httpx.AsyncClient(timeout=60.0)
        return True
    
    async def embed_text(self, text: str) -> List[float]:
        """æ–‡æœ¬å‘é‡åŒ–ï¼ˆç»Ÿä¸€æ¥å£ï¼‰"""
        if self.backend == "transformers":
            return await self._embed_transformers(text)
        elif self.backend == "ollama":
            return await self._embed_ollama(text)
    
    async def _embed_transformers(self, text: str) -> List[float]:
        """Transformerså‘é‡åŒ–"""
        import torch
        
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ä½¿ç”¨[CLS] tokençš„embedding
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
        
        return embedding.tolist()
    
    async def _embed_ollama(self, text: str) -> List[float]:
        """Ollamaå‘é‡åŒ–"""
        response = await self.client.post(
            "http://localhost:11434/api/embeddings",
            json={"model": self.model_name, "prompt": text}
        )
        return response.json()["embedding"]
    
    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å‘é‡åŒ–"""
        return [await self.embed_text(text) for text in texts]
```

**é‡æ„æ•ˆæœ**ï¼š
- 2ä¸ªæ–‡ä»¶ï¼ˆ538è¡Œï¼‰â†’ 1ä¸ªæ–‡ä»¶ï¼ˆ250è¡Œï¼‰
- å‡å°‘288è¡Œï¼ˆ-54%ï¼‰
- ç»Ÿä¸€æ¥å£ï¼Œä¾¿äºåˆ‡æ¢backend

---

### Week 4-5: æ¨¡å‹æœåŠ¡é‡æ„

#### T2.4 æ‹†åˆ†TransformersServiceï¼ˆ6å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š`transformers_service.py`ï¼ˆ835è¡Œï¼‰åŒ…å«7ä¸ªèŒè´£æ··åˆ

**æ‹†åˆ†æ–¹æ¡ˆ**ï¼š7ä¸ªç‹¬ç«‹æ¨¡å— + 1ä¸ªåè°ƒå™¨ï¼ˆåŸºäºå¯è¡Œæ€§éªŒè¯è°ƒæ•´ï¼‰

**æ”¹è¿›ç‚¹**ï¼š
- âœ… æ–°å¢ConfigManagerç»Ÿä¸€é…ç½®ç®¡ç†
- âœ… GenerationEngineæ”¹åä¸ºInferenceEngineï¼ŒèŒè´£æ›´æ˜ç¡®
- âœ… å¢åŠ 1å¤©æ—¶é—´ç”¨äºé…ç½®ç®¡ç†æ¨¡å—å¼€å‘

##### æ¨¡å—0: ConfigManager - é…ç½®ç®¡ç†ï¼ˆ100è¡Œï¼Œæ–°å¢ï¼‰

```python
# Backend/app/core/llm/transformers/config_manager.py
import torch
from transformers import BitsAndBytesConfig
from typing import Dict, Any
from app.core.device.gpu_manager import DeviceManager

class ConfigManager:
    """ç»Ÿä¸€é…ç½®ç®¡ç†ï¼ˆé‡åŒ–ã€ç”Ÿæˆå‚æ•°ï¼‰"""
    
    def __init__(self, device_manager: DeviceManager):
        self.device_manager = device_manager
    
    def get_quantization_config(self) -> BitsAndBytesConfig:
        """
        è·å–INT4é‡åŒ–é…ç½®ï¼ˆä»…CUDAæ”¯æŒï¼‰
        
        Returns:
            é‡åŒ–é…ç½®å¯¹è±¡
        """
        if not self.device_manager.is_cuda:
            return None
        
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
    
    def get_generation_config(self, **kwargs) -> Dict[str, Any]:
        """
        è·å–ç”Ÿæˆé…ç½®ï¼ˆåˆå¹¶é»˜è®¤å€¼å’Œç”¨æˆ·å‚æ•°ï¼‰
        
        Args:
            **kwargs: ç”¨æˆ·è‡ªå®šä¹‰å‚æ•°
            
        Returns:
            å®Œæ•´ç”Ÿæˆé…ç½®
        """
        # é»˜è®¤é…ç½®
        default_config = {
            "max_new_tokens": 512,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.1,
            "do_sample": True
        }
        
        # åˆå¹¶ç”¨æˆ·å‚æ•°ï¼ˆkwargsä¼˜å…ˆï¼‰
        default_config.update(kwargs)
        
        # ç‰¹æ®Šå¤„ç†ï¼štemperature=0æ—¶å…³é—­é‡‡æ ·
        if default_config["temperature"] == 0:
            default_config["do_sample"] = False
        
        return default_config
    
    def get_load_config(
        self, 
        quantize: bool, 
        model_size_gb: float,
        enable_flash_attention: bool = True
    ) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹åŠ è½½é…ç½®ï¼ˆæ ¹æ®æ˜¾å­˜ä¼˜åŒ–ï¼‰
        
        Args:
            quantize: æ˜¯å¦é‡åŒ–
            model_size_gb: æ¨¡å‹å¤§å°ï¼ˆINT4é‡åŒ–åï¼‰
            enable_flash_attention: æ˜¯å¦å¯ç”¨Flash Attention
            
        Returns:
            åŠ è½½é…ç½®å­—å…¸
        """
        config = {
            "trust_remote_code": True,
            "torch_dtype": torch.float16,
            "low_cpu_mem_usage": True,
        }
        
        # Flash Attentionï¼ˆå¯é€‰ï¼‰
        if enable_flash_attention:
            try:
                import flash_attn
                config["attn_implementation"] = "flash_attention_2"
            except ImportError:
                pass
        
        # é‡åŒ–é…ç½®
        if quantize and self.device_manager.is_cuda:
            config["quantization_config"] = self.get_quantization_config()
            
            # å°æ¨¡å‹ä¼˜åŒ–ï¼š<2GBä¸ä½¿ç”¨device_mapï¼ˆé¿å…é¢å¤–å¼€é”€ï¼‰
            if model_size_gb < 2.0:
                config["device_map"] = None
            else:
                config["device_map"] = "auto"
                config["max_memory"] = {0: "5.5GiB", "cpu": "0GiB"}
        elif self.device_manager.is_cuda:
            config["device_map"] = "auto"
        
        return config
```

##### æ¨¡å—1: PromptBuilder - æç¤ºè¯æ„å»ºï¼ˆ150è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/prompt_builder.py
from typing import List, Optional
from jinja2 import Template

class PromptBuilder:
    """ç»Ÿä¸€çš„æç¤ºè¯æ„å»ºå™¨ï¼ˆæ”¯æŒæ¨¡æ¿ï¼‰"""
    
    def __init__(self, template_path: Optional[str] = None):
        self.template = self._load_template(template_path)
    
    def build(
        self,
        messages: List[dict],
        system_prompt: Optional[str] = None,
        use_template: bool = True
    ) -> str:
        """
        æ„å»ºå®Œæ•´æç¤ºè¯
        
        Args:
            messages: å¯¹è¯å†å² [{"role": "user", "content": "..."}, ...]
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            use_template: æ˜¯å¦ä½¿ç”¨Jinja2æ¨¡æ¿
            
        Returns:
            æ ¼å¼åŒ–åçš„æç¤ºè¯
        """
        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            messages.insert(0, {"role": "system", "content": system_prompt})
        
        # ä½¿ç”¨æ¨¡æ¿æ¸²æŸ“
        if use_template and self.template:
            return self.template.render(messages=messages)
        
        # é»˜è®¤æ ¼å¼åŒ–
        return self._default_format(messages)
    
    def _load_template(self, path: Optional[str]) -> Optional[Template]:
        """åŠ è½½Jinja2æ¨¡æ¿"""
        if not path:
            return None
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return Template(f.read())
        except:
            return None
    
    def _default_format(self, messages: List[dict]) -> str:
        """é»˜è®¤æ ¼å¼åŒ–ï¼ˆChatMLé£æ ¼ï¼‰"""
        prompt_parts = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            prompt_parts.append(f"<|im_start|>{role}\n{content}<|im_end|>")
        prompt_parts.append("<|im_start|>assistant\n")
        return "\n".join(prompt_parts)
```

##### æ¨¡å—2: ResponseProcessor - å“åº”åå¤„ç†ï¼ˆ80è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/response_processor.py
import re

class ResponseProcessor:
    """å“åº”åå¤„ç†å™¨ï¼ˆæ¸…ç†ã€è¿‡æ»¤ã€æ ¼å¼åŒ–ï¼‰"""
    
    def __init__(self):
        self.stop_words = ["<|im_end|>", "<|endoftext|>", "</s>"]
    
    def process(
        self,
        raw_text: str,
        remove_prompt: bool = True,
        clean_special_tokens: bool = True
    ) -> str:
        """
        å¤„ç†æ¨¡å‹è¾“å‡º
        
        Args:
            raw_text: åŸå§‹è¾“å‡º
            remove_prompt: æ˜¯å¦ç§»é™¤è¾“å…¥æç¤º
            clean_special_tokens: æ˜¯å¦æ¸…ç†ç‰¹æ®Šæ ‡è®°
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        text = raw_text
        
        # 1. ç§»é™¤æç¤ºè¯éƒ¨åˆ†ï¼ˆå¦‚æœåŒ…å«ï¼‰
        if remove_prompt and "<|im_start|>assistant" in text:
            text = text.split("<|im_start|>assistant\n")[-1]
        
        # 2. æ¸…ç†åœæ­¢è¯
        if clean_special_tokens:
            for stop_word in self.stop_words:
                text = text.split(stop_word)[0]
        
        # 3. æ¸…ç†å¤šä½™ç©ºç™½
        text = text.strip()
        text = re.sub(r'\n{3,}', '\n\n', text)  # æœ€å¤š2ä¸ªæ¢è¡Œ
        
        return text
    
    def chunk_stream(self, text: str, chunk_size: int = 10) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å—ç”¨äºæµå¼è¾“å‡º
        
        Args:
            text: å®Œæ•´æ–‡æœ¬
            chunk_size: æ¯å—å­—ç¬¦æ•°
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
```

##### æ¨¡å—3: LoRAAdapter - LoRAç®¡ç†ï¼ˆ120è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/lora_adapter.py
from pathlib import Path
from typing import Optional
from peft import PeftModel
import torch

class LoRAAdapter:
    """LoRAé€‚é…å™¨ç®¡ç†ï¼ˆåŠ è½½/åˆ‡æ¢/å¸è½½ï¼‰"""
    
    def __init__(self, base_model):
        self.base_model = base_model
        self.current_adapter: Optional[str] = None
        self.active_model = base_model
    
    def load_adapter(self, lora_path: Path, adapter_name: str = "default") -> bool:
        """
        åŠ è½½LoRAé€‚é…å™¨
        
        Args:
            lora_path: LoRAæƒé‡è·¯å¾„
            adapter_name: é€‚é…å™¨åç§°
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # å¦‚æœå·²æœ‰é€‚é…å™¨ï¼Œå…ˆå¸è½½
            if self.current_adapter:
                self.unload_adapter()
            
            # åŠ è½½æ–°é€‚é…å™¨
            self.active_model = PeftModel.from_pretrained(
                self.base_model,
                str(lora_path),
                adapter_name=adapter_name,
                torch_dtype=torch.float16
            )
            self.current_adapter = adapter_name
            
            return True
        except Exception as e:
            print(f"åŠ è½½LoRAå¤±è´¥: {e}")
            return False
    
    def unload_adapter(self):
        """å¸è½½å½“å‰é€‚é…å™¨ï¼ˆæ¢å¤åŸºåº§æ¨¡å‹ï¼‰"""
        if self.current_adapter:
            # é‡Šæ”¾LoRAæ¨¡å‹
            del self.active_model
            torch.cuda.empty_cache()
            
            # æ¢å¤åŸºåº§æ¨¡å‹
            self.active_model = self.base_model
            self.current_adapter = None
    
    def get_model(self):
        """è·å–å½“å‰æ¿€æ´»çš„æ¨¡å‹"""
        return self.active_model
    
    def get_adapter_info(self) -> dict:
        """è·å–é€‚é…å™¨ä¿¡æ¯"""
        if not self.current_adapter:
            return {"loaded": False}
        
        return {
            "loaded": True,
            "adapter_name": self.current_adapter,
            "trainable_params": sum(
                p.numel() for p in self.active_model.parameters() if p.requires_grad
            )
        }
```

##### æ¨¡å—4: InferenceEngine - æ¨ç†å¼•æ“ï¼ˆ150è¡Œï¼Œé‡å‘½åï¼‰

```python
# Backend/app/core/llm/transformers/inference_engine.py
import torch
import asyncio
from typing import Dict, Any, AsyncGenerator
from transformers import TextIteratorStreamer
from threading import Thread
from app.utils.logger import get_logger

logger = get_logger(__name__)

class InferenceEngine:
    """çº¯æ¨ç†é€»è¾‘ï¼ˆåŒæ­¥/å¼‚æ­¥/æµå¼ï¼‰"""
    
    def __init__(self, device_manager):
        self.device_manager = device_manager
    
    async def generate_sync(
        self,
        model,
        tokenizer,
        inputs: Dict,
        gen_config: Dict[str, Any],
        timeout: int = 60
    ) -> torch.Tensor:
        """
        åŒæ­¥ç”Ÿæˆï¼ˆéæµå¼ï¼‰
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            tokenizer: åˆ†è¯å™¨
            inputs: è¾“å…¥å¼ é‡å­—å…¸
            gen_config: ç”Ÿæˆé…ç½®
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ç”Ÿæˆçš„token IDs
        """
        # æ·»åŠ pad_token_id
        gen_config["pad_token_id"] = tokenizer.eos_token_id
        
        # æ˜¾å­˜ç›‘æ§
        memory_before = self.device_manager.get_memory_info()
        if "allocated_gb" in memory_before:
            logger.info(f"æ¨ç†å‰æ˜¾å­˜: {memory_before['allocated_gb']:.2f}GB")
        
        # å¼‚æ­¥æ‰§è¡Œç”Ÿæˆ
        loop = asyncio.get_event_loop()
        try:
            with torch.no_grad():
                output_ids = await asyncio.wait_for(
                    loop.run_in_executor(
                        None,
                        lambda: model.generate(**inputs, **gen_config)
                    ),
                    timeout=timeout
                )
        except asyncio.TimeoutError:
            logger.error(f"ç”Ÿæˆè¶…æ—¶({timeout}ç§’)")
            raise RuntimeError("ç”Ÿæˆè¶…æ—¶")
        
        # æ˜¾å­˜æ¸…ç†
        self.device_manager.cleanup()
        
        return output_ids
    
    async def generate_stream(
        self,
        model,
        tokenizer,
        inputs: Dict,
        gen_config: Dict[str, Any]
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆ
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            tokenizer: åˆ†è¯å™¨
            inputs: è¾“å…¥å¼ é‡å­—å…¸
            gen_config: ç”Ÿæˆé…ç½®
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        # æ·»åŠ pad_token_id
        gen_config["pad_token_id"] = tokenizer.eos_token_id
        
        # åˆ›å»ºæµå¼è¾“å‡ºå™¨
        streamer = TextIteratorStreamer(
            tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )
        gen_config["streamer"] = streamer
        
        # åå°çº¿ç¨‹ç”Ÿæˆ
        thread = Thread(
            target=lambda: model.generate(**inputs, **gen_config)
        )
        thread.start()
        
        # é€å—è¾“å‡º
        for text_chunk in streamer:
            if text_chunk:
                yield text_chunk
        
        thread.join()
        
        # æ˜¾å­˜æ¸…ç†
        self.device_manager.cleanup()
```

##### æ¨¡å—5: TransformersLLM - åè°ƒå™¨ï¼ˆ280è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/transformers_llm.py
from pathlib import Path
from typing import List, Optional, AsyncGenerator
import torch

from app.core.llm.base_llm import BaseLLM, Message, LLMConfig
from app.core.device.gpu_manager import DeviceManager
from app.core.model.model_loader import ModelLoader
from app.core.llm.transformers.config_manager import ConfigManager
from app.core.llm.transformers.prompt_builder import PromptBuilder
from app.core.llm.transformers.response_processor import ResponseProcessor
from app.core.llm.transformers.lora_adapter import LoRAAdapter
from app.core.llm.transformers.inference_engine import InferenceEngine

class TransformersLLM(BaseLLM):
    """Transformersæœ¬åœ°æ¨¡å‹å®ç°ï¼ˆæ•´åˆæ‰€æœ‰æ¨¡å—ï¼‰"""
    
    def __init__(
        self,
        config: LLMConfig,
        model_path: Path,
        quantize: bool = True,
        lora_path: Optional[Path] = None
    ):
        super().__init__(config)
        self.model_path = model_path
        self.quantize = quantize
        self.lora_path = lora_path
        
        # ä¾èµ–ç»„ä»¶
        self.device_manager = DeviceManager()
        self.model_loader = ModelLoader(self.device_manager)
        self.config_manager = ConfigManager(self.device_manager)
        self.prompt_builder = PromptBuilder()
        self.response_processor = ResponseProcessor()
        self.inference_engine = InferenceEngine(self.device_manager)
        
        # æ¨¡å‹èµ„æº
        self.model = None
        self.tokenizer = None
        self.lora_adapter: Optional[LoRAAdapter] = None
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            # 1. åŠ è½½åŸºåº§æ¨¡å‹
            self.model, self.tokenizer = await self.model_loader.load(
                model_path=self.model_path,
                quantize=self.quantize
            )
            
            # 2. åˆå§‹åŒ–LoRAç®¡ç†å™¨
            self.lora_adapter = LoRAAdapter(self.model)
            
            # 3. åŠ è½½LoRAï¼ˆå¦‚æœæœ‰ï¼‰
            if self.lora_path:
                self.lora_adapter.load_adapter(self.lora_path)
            
            return True
        except Exception as e:
            print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def generate(
        self,
        messages: List[Message],
        stream: bool = False,
        system_prompt: Optional[str] = None,
        **kwargs
    ):
        """ç”Ÿæˆå›å¤ï¼ˆåè°ƒæ‰€æœ‰æ¨¡å—ï¼‰"""
        # 1. æ„å»ºæç¤ºè¯
        message_dicts = [{"role": m.role, "content": m.content} for m in messages]
        prompt = self.prompt_builder.build(message_dicts, system_prompt)
        
        # 2. Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.device_manager.device)
        
        # 3. è·å–ç”Ÿæˆé…ç½®ï¼ˆä½¿ç”¨ConfigManagerï¼‰
        gen_config = self.config_manager.get_generation_config(**kwargs)
        
        # 4. è·å–å½“å‰æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯LoRAæ¨¡å‹ï¼‰
        model = self.lora_adapter.get_model()
        
        # 5. æ¨ç†ç”Ÿæˆï¼ˆä½¿ç”¨InferenceEngineï¼‰
        if stream:
            # æµå¼ç”Ÿæˆ
            async for chunk in self.inference_engine.generate_stream(
                model, self.tokenizer, inputs, gen_config
            ):
                yield chunk
        else:
            # éæµå¼ç”Ÿæˆ
            output_ids = await self.inference_engine.generate_sync(
                model, self.tokenizer, inputs, gen_config
            )
            
            # è§£ç 
            input_length = inputs['input_ids'].shape[1]
            full_text = self.tokenizer.decode(
                output_ids[0][input_length:],
                skip_special_tokens=False
            )
            
            # åå¤„ç†
            return self.response_processor.process(full_text)
    
    async def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        device_info = self.device_manager.get_device_info()
        lora_info = (
            self.lora_adapter.get_adapter_info() 
            if self.lora_adapter 
            else {"loaded": False}
        )
        
        return {
            "name": self.model_name,
            "type": "transformers",
            "status": "loaded" if self.model else "unloaded",
            "device": device_info.device_type,
            "memory_usage_mb": device_info.allocated_memory_gb * 1024,
            "quantized": self.quantize,
            "lora_loaded": lora_info["loaded"],
            "lora_adapter": lora_info.get("adapter_name")
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.lora_adapter:
            self.lora_adapter.unload_adapter()
        
        del self.model
        del self.tokenizer
        self.device_manager.clear_cache()
```

---

#### T2.5 é‡æ„model_mgmtæ¨¡å—ï¼ˆ2å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
- `model_manager.py`ï¼ˆ214è¡Œï¼‰ï¼šæ¨¡å‹æ³¨å†Œå’Œç”Ÿå‘½å‘¨æœŸç®¡ç†
- `model_scanner.py`ï¼ˆ344è¡Œï¼‰ï¼šæ‰«æLLMæ¨¡å‹
- `lora_scanner_service.py`ï¼ˆ393è¡Œï¼‰ï¼šæ‰«æLoRAé€‚é…å™¨
- **é‡å¤ä»£ç **ï¼šæ–‡ä»¶éå†ã€æ ¼å¼è¯†åˆ«ã€å…ƒæ•°æ®æå–é€»è¾‘é‡å¤3æ¬¡

**ä¸ºä»€ä¹ˆæ”¾åœ¨æ¨¡å‹å±‚**ï¼šmodel_mgmtå±äºæ¨¡å‹ç®¡ç†çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ¨¡å‹åŠ è½½ã€LoRAé€‚é…å™¨åŒå±æ¨¡å‹å±‚åŸºç¡€è®¾æ–½ã€‚

**é‡æ„æ–¹æ¡ˆ**ï¼šåˆå¹¶ä¸º2ä¸ªæ–‡ä»¶ï¼ˆmodel_scanner.py + deployment.pyï¼‰

```python
# Backend/app/services/model_mgmt/model_scanner.py
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class ModelInfo:
    """ç»Ÿä¸€çš„æ¨¡å‹ä¿¡æ¯ç»“æ„"""
    model_id: str
    model_name: str
    model_type: str  # "llm" | "lora" | "embedding"
    model_path: str
    format: str  # "gguf" | "safetensors" | "pytorch"
    size_mb: float
    metadata: Dict[str, Any]

class ModelScanner:
    """ç»Ÿä¸€æ¨¡å‹æ‰«æå™¨ï¼ˆæ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰"""
    
    def scan(
        self,
        base_path: Path,
        model_type: str  # "llm" | "lora" | "embedding"
    ) -> List[ModelInfo]:
        """
        ç»Ÿä¸€æ‰«ææ¥å£
        
        Args:
            base_path: æ‰«ææ ¹ç›®å½•
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            æ ‡å‡†åŒ–çš„æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        if model_type == "llm":
            return self._scan_llm(base_path)
        elif model_type == "lora":
            return self._scan_lora(base_path)
        elif model_type == "embedding":
            return self._scan_embedding(base_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    def _scan_llm(self, base_path: Path) -> List[ModelInfo]:
        """æ‰«æLLMæ¨¡å‹ï¼ˆGGUF/Safetensorsï¼‰"""
        models = []
        
        for model_dir in base_path.iterdir():
            if not model_dir.is_dir():
                continue
            
            # æ£€æµ‹GGUFæ–‡ä»¶
            gguf_files = list(model_dir.glob("*.gguf"))
            if gguf_files:
                models.append(self._parse_gguf_model(gguf_files[0]))
                continue
            
            # æ£€æµ‹Safetensorsæ–‡ä»¶
            if (model_dir / "model.safetensors").exists():
                models.append(self._parse_safetensors_model(model_dir))
        
        return models
    
    def _scan_lora(self, base_path: Path) -> List[ModelInfo]:
        """æ‰«æLoRAé€‚é…å™¨"""
        adapters = []
        
        for adapter_dir in base_path.iterdir():
            if not adapter_dir.is_dir():
                continue
            
            # æ£€æµ‹adapter_config.json
            config_file = adapter_dir / "adapter_config.json"
            if config_file.exists():
                adapters.append(self._parse_lora_adapter(adapter_dir))
        
        return adapters
    
    def _parse_gguf_model(self, gguf_path: Path) -> ModelInfo:
        """è§£æGGUFæ¨¡å‹ä¿¡æ¯"""
        return ModelInfo(
            model_id=gguf_path.stem,
            model_name=gguf_path.stem,
            model_type="llm",
            model_path=str(gguf_path.parent),
            format="gguf",
            size_mb=gguf_path.stat().st_size / 1024 / 1024,
            metadata={"quantization": self._detect_quantization(gguf_path.name)}
        )
    
    # ... å…¶ä»–è§£ææ–¹æ³•
```

```python
# Backend/app/services/model_mgmt/deployment.py
from typing import Optional, Dict
from app.core.device.gpu_manager import DeviceManager

class ModelDeployment:
    """æ¨¡å‹éƒ¨ç½²ç®¡ç†"""
    
    def __init__(self):
        self.device_manager = DeviceManager()
        self.deployed_models: Dict[str, Any] = {}
    
    async def deploy(
        self,
        model_id: str,
        model_path: str,
        model_type: str
    ) -> bool:
        """
        éƒ¨ç½²æ¨¡å‹ï¼ˆåŠ è½½åˆ°å†…å­˜ï¼‰
        
        Args:
            model_id: æ¨¡å‹ID
            model_path: æ¨¡å‹è·¯å¾„
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # æ£€æŸ¥è®¾å¤‡èµ„æº
        device_info = self.device_manager.get_device_info()
        if device_info.allocated_memory_gb > 5.0:
            raise RuntimeError("æ˜¾å­˜ä¸è¶³ï¼Œæ— æ³•éƒ¨ç½²æ¨¡å‹")
        
        # åŠ è½½æ¨¡å‹
        if model_type == "llm":
            from app.core.llm.transformers.transformers_llm import TransformersLLM
            from app.core.llm.base_llm import LLMConfig
            
            config = LLMConfig(model_name=model_id)
            model = TransformersLLM(config, Path(model_path))
            await model.initialize()
            
            self.deployed_models[model_id] = model
        
        return True
    
    async def undeploy(self, model_id: str) -> bool:
        """å¸è½½æ¨¡å‹"""
        if model_id not in self.deployed_models:
            return False
        
        model = self.deployed_models.pop(model_id)
        await model.cleanup()
        
        return True
```

**é‡æ„æ•ˆæœ**ï¼š
- 3ä¸ªæ–‡ä»¶ï¼ˆ951è¡Œï¼‰â†’ 2ä¸ªæ–‡ä»¶ï¼ˆ530è¡Œï¼‰
- å‡å°‘421è¡Œï¼ˆ-44%ï¼‰
- ç»Ÿä¸€æ‰«ææ¥å£ï¼Œæ˜“äºæ‰©å±•

---

#### T2.6 é‡æ„trainingæ¨¡å—ï¼ˆ2å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
- `simple_lora_trainer.py`ï¼ˆ500è¡Œï¼‰ï¼šåŒ…å«é‡å¤çš„æ•°æ®é›†éªŒè¯ã€çŠ¶æ€ç®¡ç†ä»£ç 

**ä¸ºä»€ä¹ˆæ”¾åœ¨æ¨¡å‹å±‚**ï¼šLoRAè®­ç»ƒæ˜¯æ¨¡å‹ç®¡ç†çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ¨¡å‹åŠ è½½ã€é€‚é…å™¨ç®¡ç†ç´§å¯†ç›¸å…³ã€‚

**é‡æ„æ–¹æ¡ˆ**ï¼šä½¿ç”¨coreå±‚çš„TaskStateManagerå’ŒProcessManager

```python
# Backend/app/services/training/lora_trainer.pyï¼ˆé‡æ„åï¼‰
from pathlib import Path
from typing import Dict, Any
from app.core.utils.task_state_manager import TaskStateManager
import json

class LoRATrainer:
    """LoRAè®­ç»ƒæœåŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.state_manager = TaskStateManager()  # ä½¿ç”¨ç»Ÿä¸€çš„çŠ¶æ€ç®¡ç†
        self.training_processes = {}
    
    async def start_training(
        self,
        task_id: str,
        base_model_path: str,
        dataset_path: str,
        output_dir: str,
        **training_args
    ) -> bool:
        """
        å¯åŠ¨LoRAè®­ç»ƒ
        
        Args:
            task_id: ä»»åŠ¡ID
            base_model_path: åŸºåº§æ¨¡å‹è·¯å¾„
            dataset_path: æ•°æ®é›†è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            **training_args: è®­ç»ƒå‚æ•°
        """
        # 1. éªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨å·¥å…·ç±»ï¼‰
        if not self._validate_dataset(dataset_path):
            raise ValueError("æ•°æ®é›†æ ¼å¼é”™è¯¯")
        
        # 2. åˆ›å»ºè®­ç»ƒé…ç½®
        config = self._build_training_config(
            base_model_path,
            dataset_path,
            output_dir,
            **training_args
        )
        
        # 3. åˆå§‹åŒ–çŠ¶æ€
        self.state_manager.create_task(
            task_id=task_id,
            initial_state="preparing"
        )
        
        # 4. å¯åŠ¨è®­ç»ƒè¿›ç¨‹
        from app.core.utils.process_manager import ProcessManager
        process_manager = ProcessManager()
        
        process = await process_manager.start_python_script(
            script_path="scripts/train_lora.py",
            args=["--config", str(config)],
            on_output=lambda line: self._handle_training_output(task_id, line)
        )
        
        self.training_processes[task_id] = process
        self.state_manager.update_state(task_id, "running")
        
        return True
    
    def _validate_dataset(self, dataset_path: str) -> bool:
        """éªŒè¯æ•°æ®é›†æ ¼å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                return False
            
            for item in data[:5]:  # åªæ£€æŸ¥å‰5æ¡
                if "instruction" not in item or "output" not in item:
                    return False
            
            return True
        except:
            return False
    
    def _handle_training_output(self, task_id: str, line: str):
        """å¤„ç†è®­ç»ƒè¾“å‡ºï¼ˆæ›´æ–°è¿›åº¦ï¼‰"""
        if "loss" in line.lower():
            self.state_manager.update_metadata(
                task_id,
                {"latest_output": line}
            )
```

**é‡æ„æ•ˆæœ**ï¼š
- 500è¡Œ â†’ 350è¡Œï¼ˆ-30%ï¼‰
- ä½¿ç”¨ç»Ÿä¸€çš„TaskStateManagerï¼ˆåˆ é™¤é‡å¤çŠ¶æ€ç®¡ç†ä»£ç ï¼‰
- ä½¿ç”¨ç»Ÿä¸€çš„ProcessManagerï¼ˆåˆ é™¤é‡å¤è¿›ç¨‹ç®¡ç†ä»£ç ï¼‰

---

#### T2.7 æ›´æ–°æœåŠ¡å±‚è°ƒç”¨ï¼ˆ1å¤©ï¼‰

**æ—§ä»£ç ** (`transformers_service.py`):
```python
# æ—§ä»£ç ï¼ˆ835è¡Œï¼‰åŒ…å«æ‰€æœ‰é€»è¾‘
class TransformersService:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        # ... 800å¤šè¡Œå®ç°
```

**æ–°ä»£ç ** (`llm_service.py`):
```python
# Backend/app/services/llm_service.py
from pathlib import Path
from typing import Optional, Dict
from app.core.llm.base_llm import BaseLLM, LLMConfig, Message
from app.core.llm.transformers.transformers_llm import TransformersLLM
from app.core.llm.ollama_llm import OllamaLLM
from app.core.utils.path_resolver import PathResolver

class LLMService:
    """ç»Ÿä¸€çš„LLMæœåŠ¡ï¼ˆå·¥å‚æ¨¡å¼ï¼‰"""
    
    def __init__(self):
        self.path_resolver = PathResolver()
        self.llm_instances: Dict[str, BaseLLM] = {}  # æ¨¡å‹ç¼“å­˜
    
    async def get_llm(
        self,
        model_type: str,  # transformers/ollama
        model_name: str,
        config: Optional[LLMConfig] = None,
        **kwargs
    ) -> BaseLLM:
        """
        è·å–LLMå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            model_name: æ¨¡å‹åç§°
            config: LLMé…ç½®
            **kwargs: é¢å¤–å‚æ•°ï¼ˆlora_pathç­‰ï¼‰
            
        Returns:
            LLMå®ä¾‹
        """
        cache_key = f"{model_type}:{model_name}"
        
        # ä»ç¼“å­˜è·å–
        if cache_key in self.llm_instances:
            return self.llm_instances[cache_key]
        
        # åˆ›å»ºæ–°å®ä¾‹
        if config is None:
            config = LLMConfig(model_name=model_name)
        
        if model_type == "transformers":
            model_path = self.path_resolver.get_model_path("llm", model_name)
            lora_path = kwargs.get("lora_path")
            if lora_path:
                lora_path = self.path_resolver.get_model_path("lora", lora_path)
            
            llm = TransformersLLM(
                config=config,
                model_path=model_path,
                quantize=kwargs.get("quantize", True),
                lora_path=lora_path
            )
        elif model_type == "ollama":
            llm = OllamaLLM(
                config=config,
                base_url=kwargs.get("base_url", "http://localhost:11434")
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # åˆå§‹åŒ–
        success = await llm.initialize()
        if not success:
            raise RuntimeError(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_name}")
        
        # ç¼“å­˜
        self.llm_instances[cache_key] = llm
        
        return llm
    
    async def unload_model(self, model_type: str, model_name: str):
        """å¸è½½æ¨¡å‹"""
        cache_key = f"{model_type}:{model_name}"
        if cache_key in self.llm_instances:
            await self.llm_instances[cache_key].cleanup()
            del self.llm_instances[cache_key]
```

---

### é˜¶æ®µ2æ€»ç»“ï¼ˆåŸºäºå¯è¡Œæ€§éªŒè¯è°ƒæ•´ï¼‰

**å®Œæˆæ ‡å‡†**ï¼š
- âœ… BaseLLMæ¥å£å•å…ƒæµ‹è¯•é€šè¿‡
- âœ… OllamaLLMå’ŒTransformersLLMå®ç°å®Œæˆ
- âœ… ConfigManageré…ç½®ç®¡ç†æ¨¡å—å®Œæˆï¼ˆæ–°å¢ï¼‰
- âœ… InferenceEngineæ¨ç†å¼•æ“æµ‹è¯•é€šè¿‡ï¼ˆé‡å‘½åï¼‰
- âœ… å…¼å®¹å±‚è®¾è®¡å®Œæˆï¼Œæ—§APIä»å¯ç”¨
- âœ… æ‰€æœ‰æ¨¡å—é›†æˆæµ‹è¯•é€šè¿‡
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•æ— ä¸‹é™ï¼ˆ<0.1%å¼€é”€ï¼‰

**é¢„æœŸæ•ˆæœ**ï¼š
- â¬‡ï¸ transformers_service: 835è¡Œ â†’ 7ä¸ªæ¨¡å—ï¼ˆ~830è¡Œï¼Œä½†èŒè´£æ¸…æ™°ï¼‰
  - ConfigManager: 100è¡Œ
  - PromptBuilder: 150è¡Œ
  - ResponseProcessor: 80è¡Œ
  - LoRAAdapter: 120è¡Œ
  - InferenceEngine: 150è¡Œ
  - TransformersLLM: 280è¡Œï¼ˆåè°ƒå™¨ï¼‰
- â¬‡ï¸ ollamaé›†æˆ: 486è¡Œ â†’ 300è¡Œ (-38%)
- â¬‡ï¸ embeddingæœåŠ¡: 538è¡Œ â†’ 250è¡Œ (-54%)
- â¬‡ï¸ model_mgmt: 951è¡Œ â†’ 530è¡Œ (-44%)
- â¬†ï¸ æ’ä»¶åŒ–æ¶æ„: å¯éšæ—¶æ·»åŠ æ–°çš„LLMåç«¯ï¼ˆOpenAI/Claudeç­‰ï¼‰
- â¬†ï¸ æ¥å£ç»Ÿä¸€æ€§: æ‰€æœ‰LLMå…±äº«BaseLLMæ¥å£
- â¬†ï¸ å¯æµ‹è¯•æ€§: æ¯ä¸ªæ¨¡å—å¯ç‹¬ç«‹æµ‹è¯•
- â¬†ï¸ å¯ç»´æŠ¤æ€§: èŒè´£å•ä¸€ï¼Œä¿®æ”¹å½±å“èŒƒå›´å°

**æ—¶é—´è°ƒæ•´**ï¼š15å¤© â†’ 17å¤©
- Week 3: T2.1-T2.3ï¼ˆ5å¤©ï¼‰æ— å˜åŒ–
- Week 4-5: T2.4-T2.6ï¼ˆ10å¤© â†’ 12å¤©ï¼‰
  - T2.4 æ‹†åˆ†Transformers: 5å¤© â†’ 6å¤©ï¼ˆ+ConfigManagerï¼‰
  - T2.5 TransformersLLM: 3å¤© â†’ 4å¤©ï¼ˆ+å…¼å®¹å±‚ï¼‰
  - T2.6 æµ‹è¯•ä¸é›†æˆ: 2å¤© â†’ 2å¤©

**é£é™©æ§åˆ¶**ï¼š
- âœ… ä¿ç•™å…¼å®¹å±‚ï¼ˆtransformers_service.pyå†…éƒ¨è°ƒç”¨TransformersLLMï¼‰
- âœ… æ¸è¿›å¼è¿ç§»ï¼ˆAPIç«¯ç‚¹é€æ­¥åˆ‡æ¢åˆ°æ–°æ¥å£ï¼‰
- âœ… å›æ»šç­–ç•¥ï¼ˆæ—§ä»£ç ä¿ç•™åˆ°Week 11å†åˆ é™¤ï¼‰

**ä¸‹ä¸€æ­¥**ï¼šè¿›å…¥é˜¶æ®µ3 - ä¸šåŠ¡å±‚é‡æ„

---

## äº”ã€é˜¶æ®µ3ï¼šä¸šåŠ¡å±‚ (Week 6-8)

### ç›®æ ‡
ğŸ¯ ç»Ÿä¸€æ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡/å…¨æ–‡/æ··åˆ/å›¾è°±ï¼‰  
ğŸ¯ é‡æ„knowledge_base_serviceï¼ˆ558è¡Œ â†’ 350è¡Œï¼‰  
ğŸ¯ ä¼˜åŒ–RAGæ£€ç´¢æ€§èƒ½

### Week 6: æ£€ç´¢ç­–ç•¥ç»Ÿä¸€

#### å½“å‰é—®é¢˜
- æ£€ç´¢é€»è¾‘åˆ†æ•£åœ¨3ä¸ªæ–‡ä»¶ï¼ˆchat_service, knowledge_base_service, graph_serviceï¼‰
- æ²¡æœ‰ç»Ÿä¸€çš„ç­–ç•¥æ¥å£
- æ— æ³•çµæ´»åˆ‡æ¢æ£€ç´¢ç®—æ³•

#### T3.1 å®šä¹‰æ£€ç´¢ç­–ç•¥æ¥å£ï¼ˆ1å¤©ï¼‰

**ç­–ç•¥æ¨¡å¼è®¾è®¡**ï¼š

```python
# Backend/app/core/retrieval/base_retriever.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®"""
    top_k: int = 5
    score_threshold: float = 0.6
    enable_rerank: bool = False

@dataclass
class Document:
    """æ–‡æ¡£ç»“æ„"""
    content: str
    metadata: Dict[str, Any]
    score: float
    doc_id: str

class BaseRetriever(ABC):
    """æ£€ç´¢å™¨åŸºç±»"""
    
    def __init__(self, config: RetrievalConfig):
        self.config = config
    
    @abstractmethod
    async def retrieve(
        self,
        query: str,
        kb_id: int,
        **kwargs
    ) -> List[Document]:
        """
        æ‰§è¡Œæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_id: çŸ¥è¯†åº“ID
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
        """
        pass
    
    @abstractmethod
    def get_retriever_info(self) -> Dict[str, Any]:
        """
        è·å–æ£€ç´¢å™¨ä¿¡æ¯
        
        Returns:
            {
                "type": "vector/bm25/hybrid/graph",
                "config": {...}
            }
        """
        pass
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š

```python
# Backend/app/tests/test_retriever.py
import pytest
from app.core.retrieval.base_retriever import BaseRetriever, RetrievalConfig, Document

class MockRetriever(BaseRetriever):
    async def retrieve(self, query, kb_id, **kwargs):
        return [
            Document(
                content="æµ‹è¯•æ–‡æ¡£1",
                metadata={"source": "test.txt"},
                score=0.95,
                doc_id="doc1"
            )
        ]
    
    def get_retriever_info(self):
        return {"type": "mock", "config": self.config.__dict__}

@pytest.mark.asyncio
async def test_retriever_interface():
    config = RetrievalConfig(top_k=10)
    retriever = MockRetriever(config)
    
    results = await retriever.retrieve("æµ‹è¯•æŸ¥è¯¢", kb_id=1)
    assert len(results) == 1
    assert results[0].score == 0.95
```

---

#### T3.2 å®ç°å‘é‡æ£€ç´¢å™¨ï¼ˆ2å¤©ï¼‰

```python
# Backend/app/core/retrieval/vector_retriever.py
from typing import List
import numpy as np
from app.core.retrieval.base_retriever import BaseRetriever, Document
from app.services.embedding_service import EmbeddingService

class VectorRetriever(BaseRetriever):
    """å‘é‡æ£€ç´¢å™¨ï¼ˆä½¿ç”¨ChromaDBï¼‰"""
    
    def __init__(self, config, embedding_service: EmbeddingService):
        super().__init__(config)
        self.embedding_service = embedding_service
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """å‘é‡æ£€ç´¢"""
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = await self.embedding_service.embed_text(query)
        
        # 2. ä»ChromaDBæ£€ç´¢
        from app.services.vector_store_service import VectorStoreService
        vector_store = VectorStoreService()
        
        raw_results = vector_store.similarity_search(
            collection_name=f"kb_{kb_id}",
            query_embedding=query_embedding,
            top_k=self.config.top_k
        )
        
        # 3. è½¬æ¢ä¸ºDocumentå¯¹è±¡
        documents = []
        for result in raw_results:
            if result['score'] >= self.config.score_threshold:
                documents.append(Document(
                    content=result['content'],
                    metadata=result['metadata'],
                    score=result['score'],
                    doc_id=result['id']
                ))
        
        # 4. Rerankï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.enable_rerank:
            documents = await self._rerank(query, documents)
        
        return documents
    
    async def _rerank(self, query: str, documents: List[Document]) -> List[Document]:
        """é‡æ’åºï¼ˆä½¿ç”¨BGE-Rerankerï¼‰"""
        # TODO: é›†æˆRerankeræ¨¡å‹
        # æš‚æ—¶è¿”å›åŸç»“æœ
        return documents
    
    def get_retriever_info(self):
        return {
            "type": "vector",
            "config": {
                "top_k": self.config.top_k,
                "score_threshold": self.config.score_threshold,
                "enable_rerank": self.config.enable_rerank
            }
        }
```

---

#### T3.3 å®ç°BM25å…¨æ–‡æ£€ç´¢å™¨ï¼ˆ2å¤©ï¼‰

```python
# Backend/app/core/retrieval/bm25_retriever.py
from typing import List
from rank_bm25 import BM25Okapi
import jieba
from app.core.retrieval.base_retriever import BaseRetriever, Document

class BM25Retriever(BaseRetriever):
    """BM25å…¨æ–‡æ£€ç´¢å™¨"""
    
    def __init__(self, config):
        super().__init__(config)
        self.bm25_index = {}  # {kb_id: BM25Okapiå®ä¾‹}
        self.doc_store = {}   # {kb_id: [Document]}
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """BM25æ£€ç´¢"""
        # 1. æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if kb_id not in self.bm25_index:
            await self._build_index(kb_id)
        
        # 2. åˆ†è¯
        query_tokens = list(jieba.cut(query))
        
        # 3. BM25æ‰“åˆ†
        bm25 = self.bm25_index[kb_id]
        scores = bm25.get_scores(query_tokens)
        
        # 4. æ’åºå¹¶è¿‡æ»¤
        doc_store = self.doc_store[kb_id]
        ranked_indices = np.argsort(scores)[::-1]  # é™åº
        
        documents = []
        for idx in ranked_indices[:self.config.top_k]:
            score = scores[idx]
            if score >= self.config.score_threshold:
                doc = doc_store[idx]
                doc.score = float(score)
                documents.append(doc)
        
        return documents
    
    async def _build_index(self, kb_id: int):
        """æ„å»ºBM25ç´¢å¼•"""
        # 1. ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£
        from app.services.database_service import DatabaseService
        db = DatabaseService()
        
        chunks = db.get_knowledge_chunks(kb_id)
        
        # 2. åˆ†è¯
        tokenized_corpus = []
        doc_store = []
        
        for chunk in chunks:
            tokens = list(jieba.cut(chunk['content']))
            tokenized_corpus.append(tokens)
            
            doc_store.append(Document(
                content=chunk['content'],
                metadata=chunk['metadata'],
                score=0.0,
                doc_id=str(chunk['id'])
            ))
        
        # 3. æ„å»ºBM25
        self.bm25_index[kb_id] = BM25Okapi(tokenized_corpus)
        self.doc_store[kb_id] = doc_store
    
    def get_retriever_info(self):
        return {
            "type": "bm25",
            "config": {
                "top_k": self.config.top_k,
                "indexed_kbs": list(self.bm25_index.keys())
            }
        }
```

---

### Week 6æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… BaseRetrieveræ¥å£å®šä¹‰
- âœ… VectorRetrieverå®ç°ï¼ˆChromaDBï¼‰
- âœ… BM25Retrieverå®ç°ï¼ˆå…¨æ–‡æ£€ç´¢ï¼‰

**ä»£ç é‡**ï¼š
- æ–°å¢: ~350è¡Œï¼ˆ3ä¸ªæ–‡ä»¶ï¼‰
- ä¸‹ä¸€æ­¥: å®ç°HybridRetriever + GraphRetriever

---

### Week 7: æ··åˆæ£€ç´¢ä¸çŸ¥è¯†å›¾è°±

#### T3.4 å®ç°æ··åˆæ£€ç´¢å™¨ï¼ˆ2å¤©ï¼‰

**æ··åˆæ£€ç´¢ç­–ç•¥**ï¼šå‘é‡æ£€ç´¢ + BM25 + èåˆç®—æ³•

```python
# Backend/app/core/retrieval/hybrid_retriever.py
from typing import List, Dict
import numpy as np
from app.core.retrieval.base_retriever import BaseRetriever, Document, RetrievalConfig
from app.core.retrieval.vector_retriever import VectorRetriever
from app.core.retrieval.bm25_retriever import BM25Retriever

class HybridRetriever(BaseRetriever):
    """æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡+BM25èåˆï¼‰"""
    
    def __init__(
        self,
        config: RetrievalConfig,
        vector_retriever: VectorRetriever,
        bm25_retriever: BM25Retriever,
        vector_weight: float = 0.7  # å‘é‡æ£€ç´¢æƒé‡
    ):
        super().__init__(config)
        self.vector_retriever = vector_retriever
        self.bm25_retriever = bm25_retriever
        self.vector_weight = vector_weight
        self.bm25_weight = 1.0 - vector_weight
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """æ··åˆæ£€ç´¢ï¼ˆRRFèåˆï¼‰"""
        # 1. å¹¶è¡Œæ‰§è¡Œä¸¤ç§æ£€ç´¢
        vector_results = await self.vector_retriever.retrieve(query, kb_id)
        bm25_results = await self.bm25_retriever.retrieve(query, kb_id)
        
        # 2. RRFèåˆï¼ˆReciprocal Rank Fusionï¼‰
        fused_results = self._rrf_fusion(vector_results, bm25_results)
        
        # 3. æˆªæ–­åˆ°top_k
        return fused_results[:self.config.top_k]
    
    def _rrf_fusion(
        self,
        vector_docs: List[Document],
        bm25_docs: List[Document],
        k: int = 60  # RRFå‚æ•°
    ) -> List[Document]:
        """
        RRFèåˆç®—æ³•
        Score(d) = Î£ 1/(k + rank_i(d))
        """
        # 1. æ„å»ºæ–‡æ¡£å­—å…¸
        doc_dict: Dict[str, Document] = {}
        doc_scores: Dict[str, float] = {}
        
        # 2. å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        for rank, doc in enumerate(vector_docs, start=1):
            doc_id = doc.doc_id
            rrf_score = self.vector_weight / (k + rank)
            
            doc_dict[doc_id] = doc
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score
        
        # 3. å¤„ç†BM25æ£€ç´¢ç»“æœ
        for rank, doc in enumerate(bm25_docs, start=1):
            doc_id = doc.doc_id
            rrf_score = self.bm25_weight / (k + rank)
            
            if doc_id not in doc_dict:
                doc_dict[doc_id] = doc
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score
        
        # 4. æŒ‰èåˆåˆ†æ•°æ’åº
        sorted_ids = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        # 5. æ›´æ–°åˆ†æ•°å¹¶è¿”å›
        fused_docs = []
        for doc_id, score in sorted_ids:
            doc = doc_dict[doc_id]
            doc.score = score
            fused_docs.append(doc)
        
        return fused_docs
    
    def get_retriever_info(self):
        return {
            "type": "hybrid",
            "config": {
                "vector_weight": self.vector_weight,
                "bm25_weight": self.bm25_weight,
                "top_k": self.config.top_k
            }
        }
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š

```python
# Backend/app/tests/test_hybrid_retriever.py
import pytest
from app.core.retrieval.hybrid_retriever import HybridRetriever
from app.core.retrieval.base_retriever import Document

@pytest.mark.asyncio
async def test_rrf_fusion():
    # Mockæ£€ç´¢å™¨
    class MockVectorRetriever:
        async def retrieve(self, query, kb_id):
            return [
                Document("doc1", {}, 0.9, "1"),
                Document("doc2", {}, 0.8, "2")
            ]
    
    class MockBM25Retriever:
        async def retrieve(self, query, kb_id):
            return [
                Document("doc2", {}, 0.95, "2"),  # doc2åœ¨ä¸¤ä¸ªç»“æœä¸­
                Document("doc3", {}, 0.7, "3")
            ]
    
    config = RetrievalConfig(top_k=5)
    retriever = HybridRetriever(
        config,
        MockVectorRetriever(),
        MockBM25Retriever(),
        vector_weight=0.7
    )
    
    results = await retriever.retrieve("test", kb_id=1)
    
    # doc2åº”è¯¥æ’ç¬¬ä¸€ï¼ˆä¸¤ä¸ªæ£€ç´¢å™¨éƒ½è¿”å›äº†ï¼‰
    assert results[0].doc_id == "2"
    assert len(results) == 3
```

---

#### T3.5 å®ç°çŸ¥è¯†å›¾è°±æ£€ç´¢å™¨ï¼ˆ3å¤©ï¼‰

```python
# Backend/app/core/retrieval/graph_retriever.py
from typing import List, Set, Dict
from app.core.retrieval.base_retriever import BaseRetriever, Document

class GraphRetriever(BaseRetriever):
    """çŸ¥è¯†å›¾è°±æ£€ç´¢å™¨ï¼ˆå®ä½“å…³ç³»æ‰©å±•ï¼‰"""
    
    def __init__(self, config, graph_service):
        super().__init__(config)
        self.graph_service = graph_service
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """
        å›¾è°±æ£€ç´¢æµç¨‹ï¼š
        1. æå–æŸ¥è¯¢ä¸­çš„å®ä½“
        2. æŸ¥æ‰¾å®ä½“çš„å…³ç³»ä¸‰å…ƒç»„
        3. æ‰©å±•åˆ°ç›¸å…³å®ä½“
        4. æ”¶é›†å…³è”æ–‡æ¡£
        """
        # 1. æå–å®ä½“
        entities = await self._extract_entities(query, kb_id)
        if not entities:
            return []
        
        # 2. æŸ¥æ‰¾å…³ç³»
        triples = await self._find_triples(entities, kb_id)
        
        # 3. æ‰©å±•ç›¸å…³å®ä½“
        related_entities = self._expand_entities(triples, entities)
        
        # 4. æ”¶é›†æ–‡æ¡£
        documents = await self._collect_documents(
            entities | related_entities,
            kb_id
        )
        
        return documents[:self.config.top_k]
    
    async def _extract_entities(self, query: str, kb_id: int) -> Set[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å®ä½“"""
        # è°ƒç”¨entity_extraction_service
        from app.services.entity_extraction_service import EntityExtractionService
        
        service = EntityExtractionService()
        result = await service.extract_entities(query, kb_id)
        
        return set(result.get("entities", []))
    
    async def _find_triples(
        self,
        entities: Set[str],
        kb_id: int
    ) -> List[Dict]:
        """æŸ¥æ‰¾å®ä½“çš„ä¸‰å…ƒç»„"""
        triples = []
        
        for entity in entities:
            # æŸ¥è¯¢æ•°æ®åº“
            entity_triples = self.graph_service.get_entity_triples(
                kb_id=kb_id,
                entity=entity
            )
            triples.extend(entity_triples)
        
        return triples
    
    def _expand_entities(
        self,
        triples: List[Dict],
        seed_entities: Set[str]
    ) -> Set[str]:
        """æ‰©å±•ç›¸å…³å®ä½“ï¼ˆ1è·³ï¼‰"""
        related = set()
        
        for triple in triples:
            head = triple["head_entity"]
            tail = triple["tail_entity"]
            
            # å¦‚æœå¤´å®ä½“æ˜¯ç§å­ï¼Œæ·»åŠ å°¾å®ä½“
            if head in seed_entities:
                related.add(tail)
            
            # å¦‚æœå°¾å®ä½“æ˜¯ç§å­ï¼Œæ·»åŠ å¤´å®ä½“
            if tail in seed_entities:
                related.add(head)
        
        return related - seed_entities  # æ’é™¤ç§å­å®ä½“
    
    async def _collect_documents(
        self,
        entities: Set[str],
        kb_id: int
    ) -> List[Document]:
        """æ”¶é›†åŒ…å«å®ä½“çš„æ–‡æ¡£"""
        from app.services.database_service import DatabaseService
        
        db = DatabaseService()
        documents = []
        
        for entity in entities:
            # æŸ¥è¯¢åŒ…å«è¯¥å®ä½“çš„æ–‡æ¡£å—
            chunks = db.search_chunks_by_entity(kb_id, entity)
            
            for chunk in chunks:
                documents.append(Document(
                    content=chunk['content'],
                    metadata={
                        **chunk['metadata'],
                        "matched_entity": entity
                    },
                    score=0.8,  # å›ºå®šåˆ†æ•°
                    doc_id=str(chunk['id'])
                ))
        
        # å»é‡
        seen = set()
        unique_docs = []
        for doc in documents:
            if doc.doc_id not in seen:
                seen.add(doc.doc_id)
                unique_docs.append(doc)
        
        return unique_docs
    
    def get_retriever_info(self):
        return {
            "type": "graph",
            "config": {
                "top_k": self.config.top_k,
                "expand_hops": 1
            }
        }
```

---

### Week 7æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… HybridRetrieverï¼ˆRRFèåˆç®—æ³•ï¼‰
- âœ… GraphRetrieverï¼ˆçŸ¥è¯†å›¾è°±æ‰©å±•ï¼‰
- âœ… 4ç§æ£€ç´¢ç­–ç•¥å…¨éƒ¨å®ç°

**æ£€ç´¢ç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|
| Vector | è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ | ç†è§£è¯­ä¹‰ | å¬å›ç²¾ç¡®è¯å›°éš¾ |
| BM25 | å…³é”®è¯ç²¾ç¡®åŒ¹é… | é€Ÿåº¦å¿« | ä¸ç†è§£è¯­ä¹‰ |
| Hybrid | é€šç”¨åœºæ™¯ | å…¼é¡¾è¯­ä¹‰+å…³é”®è¯ | å¤æ‚åº¦é«˜ |
| Graph | éœ€è¦å…³ç³»æ¨ç† | å¯æ‰©å±•å®ä½“ | ä¾èµ–å›¾è°±è´¨é‡ |

---

### Week 8: çŸ¥è¯†åº“æœåŠ¡é‡æ„

#### T3.10 æ‹†åˆ†knowledge_base_serviceï¼ˆ4å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
`knowledge_base_service.py`ï¼ˆ558è¡Œï¼‰åŒ…å«4ä¸ªèŒè´£æ··åˆï¼š
1. CRUDæ“ä½œï¼ˆåˆ›å»º/åˆ é™¤çŸ¥è¯†åº“ï¼‰
2. æ–‡æ¡£ç®¡ç†ï¼ˆä¸Šä¼ /åˆ†å—/å‘é‡åŒ–ï¼‰
3. æ£€ç´¢é€»è¾‘ï¼ˆå‘é‡æœç´¢ï¼‰
4. çŸ¥è¯†å›¾è°±æ“ä½œ

**é‡æ„æ–¹æ¡ˆ**ï¼šèŒè´£åˆ†ç¦»ä¸º4ä¸ªç‹¬ç«‹æœåŠ¡ï¼ˆåŸºäºå¯è¡Œæ€§éªŒè¯è°ƒæ•´ï¼‰

##### æœåŠ¡1: KnowledgeBaseService - çº¯CRUDï¼ˆä¿ç•™ï¼Œ200è¡Œï¼‰

```python
# Backend/app/services/knowledge/knowledge_base_service.pyï¼ˆé‡æ„åï¼‰
from app.core.retrieval.retriever_manager import RetrieverManager

class KnowledgeBaseService:
    """çŸ¥è¯†åº“æœåŠ¡ï¼ˆä»…è´Ÿè´£CRUD + ç»Ÿè®¡ï¼‰"""
    
    def __init__(self):
        self.db = DatabaseService()
        self.retriever_manager = RetrieverManager()  # ä½¿ç”¨ç­–ç•¥ç®¡ç†å™¨
    
    async def create_knowledge_base(
        self,
        name: str,
        description: str,
        embedding_model: str = "bge-small-zh-v1.5"
    ) -> int:
        """åˆ›å»ºçŸ¥è¯†åº“"""
        kb_id = await self.db.insert_knowledge_base(
            name=name,
            description=description,
            embedding_model=embedding_model
        )
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        from app.services.vector_store_service import VectorStoreService
        vector_store = VectorStoreService()
        await vector_store.create_collection(f"kb_{kb_id}")
        
        return kb_id
    
    async def delete_knowledge_base(self, kb_id: int) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“ï¼ˆçº§è”åˆ é™¤æ–‡æ¡£å’Œå‘é‡ï¼‰"""
        # 1. åˆ é™¤å‘é‡é›†åˆ
        from app.services.vector_store_service import VectorStoreService
        vector_store = VectorStoreService()
        await vector_store.delete_collection(f"kb_{kb_id}")
        
        # 2. åˆ é™¤æ•°æ®åº“è®°å½•
        await self.db.delete_knowledge_base(kb_id)
        
        return True
    
    async def search(
        self,
        query: str,
        kb_id: int,
        strategy: str = "hybrid",
        top_k: int = 5
    ) -> List[dict]:
        """ç»Ÿä¸€æ£€ç´¢å…¥å£ï¼ˆå§”æ‰˜ç»™RetrieverManagerï¼‰"""
        documents = await self.retriever_manager.retrieve(
            query=query,
            kb_id=kb_id,
            strategy=strategy,
            top_k=top_k
        )
        
        return [
            {
                "content": doc.content,
                "metadata": doc.metadata,
                "score": doc.score,
                "doc_id": doc.doc_id
            }
            for doc in documents
        ]
```

##### æœåŠ¡2: DocumentService - æ–‡æ¡£ç®¡ç†ï¼ˆæ–°å¢ï¼Œ120è¡Œï¼‰

```python
# Backend/app/services/knowledge/document_service.py
class DocumentService:
    """æ–‡æ¡£ç®¡ç†æœåŠ¡ï¼ˆçº¯æ–‡æ¡£CRUDï¼Œä¸åŒ…å«åˆ†å—å’Œå‘é‡åŒ–ï¼‰"""
    
    def __init__(self):
        self.db = DatabaseService()
    
    async def upload_document(
        self,
        kb_id: int,
        filename: str,
        filepath: str,
        file_size: int,
        metadata: dict = None
    ) -> int:
        """åˆ›å»ºæ–‡æ¡£è®°å½•"""
        doc_id = await self.db.insert_document(
            kb_id=kb_id,
            filename=filename,
            filepath=filepath,
            file_size=file_size,
            metadata=metadata or {}
        )
        return doc_id
    
    async def get_document(self, doc_id: int) -> dict:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        return await self.db.get_document(doc_id)
    
    async def delete_document(self, doc_id: int) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        # 1. è·å–æ–‡æ¡£æ‰€å±çš„çŸ¥è¯†åº“
        doc = await self.get_document(doc_id)
        kb_id = doc['kb_id']
        
        # 2. åˆ é™¤å‘é‡ï¼ˆç”±VectorizationServiceå¤„ç†ï¼‰
        from app.services.knowledge.vectorization_service import VectorizationService
        vector_service = VectorizationService()
        await vector_service.delete_document_vectors(kb_id, doc_id)
        
        # 3. åˆ é™¤æ•°æ®åº“è®°å½•
        await self.db.delete_document(doc_id)
        
        return True
```

##### æœåŠ¡3: ChunkingService - æ–‡æœ¬åˆ†å—ï¼ˆæ–°å¢ï¼Œ150è¡Œï¼‰

```python
# Backend/app/services/knowledge/chunking_service.py
from app.core.utils.text_splitter import RecursiveTextSplitter

class ChunkingService:
    """æ–‡æœ¬åˆ†å—æœåŠ¡ï¼ˆä¸“æ³¨åˆ†å—é€»è¾‘ï¼‰"""
    
    def __init__(self):
        self.splitters = {
            "recursive": RecursiveTextSplitter(chunk_size=500, chunk_overlap=50),
            "semantic": None  # TODO: æ·»åŠ è¯­ä¹‰åˆ†å‰²
        }
    
    async def chunk_text(
        self,
        text: str,
        strategy: str = "recursive",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å—"""
        splitter = self.splitters.get(strategy)
        if not splitter:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å—ç­–ç•¥: {strategy}")
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        if strategy == "recursive":
            splitter.chunk_size = chunk_size
            splitter.chunk_overlap = chunk_overlap
        
        chunks = splitter.split(text)
        return chunks
    
    async def chunk_document(
        self,
        doc_id: int,
        filepath: str,
        strategy: str = "recursive"
    ) -> List[dict]:
        """åˆ†å—æ–‡æ¡£å¹¶ä¿å­˜"""
        # 1. è¯»å–æ–‡ä»¶
        text = self._read_file(filepath)
        
        # 2. åˆ†å—
        chunks = await self.chunk_text(text, strategy)
        
        # 3. ä¿å­˜åˆ†å—è®°å½•
        from app.services.database_service import DatabaseService
        db = DatabaseService()
        
        chunk_records = []
        for i, chunk in enumerate(chunks):
            chunk_id = await db.insert_chunk(
                doc_id=doc_id,
                content=chunk,
                chunk_index=i,
                metadata={"strategy": strategy}
            )
            chunk_records.append({
                "chunk_id": chunk_id,
                "content": chunk,
                "index": i
            })
        
        return chunk_records
    
    def _read_file(self, filepath: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        from pathlib import Path
        path = Path(filepath)
        
        if path.suffix == '.txt':
            return path.read_text(encoding='utf-8')
        elif path.suffix == '.md':
            return path.read_text(encoding='utf-8')
        # TODO: æ·»åŠ PDFã€DOCXæ”¯æŒ
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {path.suffix}")
```

##### æœåŠ¡4: VectorizationService - å‘é‡åŒ–ç®¡ç†ï¼ˆæ–°å¢ï¼Œ180è¡Œï¼‰

```python
# Backend/app/services/knowledge/vectorization_service.py
from typing import List
from app.services.embedding_service import EmbeddingService
from app.services.vector_store_service import VectorStoreService

class VectorizationService:
    """å‘é‡åŒ–ç®¡ç†æœåŠ¡ï¼ˆæ‰¹é‡å‘é‡åŒ–ã€å¢é‡æ›´æ–°ï¼‰"""
    
    def __init__(self):
        self.embedding = EmbeddingService()
        self.vector_store = VectorStoreService()
    
    async def vectorize_chunks(
        self,
        kb_id: int,
        chunks: List[dict],
        embedding_model: str = "bge-small-zh-v1.5"
    ) -> int:
        """
        æ‰¹é‡å‘é‡åŒ–å¹¶å­˜å‚¨
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            chunks: åˆ†å—åˆ—è¡¨ [{"chunk_id": 1, "content": "...", "metadata": {...}}]
            embedding_model: åµŒå…¥æ¨¡å‹
            
        Returns:
            æˆåŠŸå‘é‡åŒ–çš„æ•°é‡
        """
        if not chunks:
            return 0
        
        # 1. æ‰¹é‡å‘é‡åŒ–ï¼ˆä½¿ç”¨æŒ‡å®šæ¨¡å‹ï¼‰
        texts = [chunk['content'] for chunk in chunks]
        embeddings = await self.embedding.embed_batch(
            texts,
            model_name=embedding_model
        )
        
        # 2. å‡†å¤‡å‘é‡å­˜å‚¨æ•°æ®
        collection_name = f"kb_{kb_id}"
        ids = [str(chunk['chunk_id']) for chunk in chunks]
        metadatas = [chunk.get('metadata', {}) for chunk in chunks]
        
        # 3. æ‰¹é‡æ’å…¥å‘é‡åº“
        await self.vector_store.add(
            collection_name=collection_name,
            ids=ids,
            embeddings=embeddings,
            documents=texts,
            metadatas=metadatas
        )
        
        return len(chunks)
    
    async def update_chunk_vector(
        self,
        kb_id: int,
        chunk_id: int,
        new_content: str,
        embedding_model: str
    ) -> bool:
        """æ›´æ–°å•ä¸ªåˆ†å—çš„å‘é‡"""
        # 1. é‡æ–°å‘é‡åŒ–
        embedding = await self.embedding.embed_single(
            new_content,
            model_name=embedding_model
        )
        
        # 2. æ›´æ–°å‘é‡åº“
        collection_name = f"kb_{kb_id}"
        await self.vector_store.update(
            collection_name=collection_name,
            ids=[str(chunk_id)],
            embeddings=[embedding],
            documents=[new_content]
        )
        
        return True
    
    async def delete_document_vectors(
        self,
        kb_id: int,
        doc_id: int
    ) -> bool:
        """åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡"""
        # 1. æŸ¥è¯¢æ–‡æ¡£çš„æ‰€æœ‰chunk_id
        from app.services.database_service import DatabaseService
        db = DatabaseService()
        chunks = await db.get_document_chunks(doc_id)
        
        if not chunks:
            return True
        
        # 2. æ‰¹é‡åˆ é™¤å‘é‡
        collection_name = f"kb_{kb_id}"
        chunk_ids = [str(chunk['id']) for chunk in chunks]
        
        await self.vector_store.delete(
            collection_name=collection_name,
            ids=chunk_ids
        )
        
        return True
```

**é‡æ„æ•ˆæœ**ï¼š
- 558è¡Œ â†’ 4ä¸ªæ–‡ä»¶ï¼ˆ650è¡Œï¼Œä½†èŒè´£æ¸…æ™°ï¼‰
  - KnowledgeBaseService: 200è¡Œï¼ˆCRUD + ç»Ÿè®¡ï¼‰
  - DocumentService: 120è¡Œï¼ˆæ–‡æ¡£ç®¡ç†ï¼‰
  - ChunkingService: 150è¡Œï¼ˆæ–‡æœ¬åˆ†å—ï¼‰
  - VectorizationService: 180è¡Œï¼ˆå‘é‡åŒ–ï¼‰
- èŒè´£å•ä¸€ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
- æ”¯æŒç‹¬ç«‹ä¼˜åŒ–ï¼ˆä¾‹å¦‚æ›´æ¢åˆ†å—ç­–ç•¥ï¼‰

---

#### T3.11 ç‹¬ç«‹graph_serviceï¼ˆ3å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
çŸ¥è¯†å›¾è°±é€»è¾‘æ··åœ¨`knowledge_base_service.py`ä¸­ï¼ˆ~180è¡Œï¼‰

**é‡æ„æ–¹æ¡ˆ**ï¼šå®Œå…¨ç‹¬ç«‹ä¸ºgraphæ¨¡å—

```python
# Backend/app/services/graph/graph_service.pyï¼ˆæ–°å¢ï¼Œ180è¡Œï¼‰
from neo4j import GraphDatabase

class GraphService:
    """çŸ¥è¯†å›¾è°±æœåŠ¡ï¼ˆç‹¬ç«‹æ¨¡å—ï¼‰"""
    
    def __init__(self):
        self.driver = GraphDatabase.driver(
            "bolt://localhost:7687",
            auth=("neo4j", "password")
        )
    
    async def build_graph(
        self,
        kb_id: int,
        chunks: List[dict],
        force_rebuild: bool = False
    ) -> dict:
        """
        æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            chunks: æ–‡æœ¬åˆ†å—åˆ—è¡¨
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º
            
        Returns:
            {"nodes": 100, "relationships": 250}
        """
        # 1. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not force_rebuild:
            stats = await self.get_stats(kb_id)
            if stats['nodes'] > 0:
                return stats
        
        # 2. æå–å®ä½“å’Œå…³ç³»ï¼ˆä½¿ç”¨NER + å…³ç³»æŠ½å–ï¼‰
        entities, relations = await self._extract_entities_relations(chunks)
        
        # 3. å†™å…¥Neo4j
        with self.driver.session() as session:
            # æ¸…ç©ºæ—§å›¾è°±
            if force_rebuild:
                session.run("MATCH (n:Entity {kb_id: $kb_id}) DETACH DELETE n", kb_id=kb_id)
            
            # åˆ›å»ºèŠ‚ç‚¹
            for entity in entities:
                session.run(
                    """
                    MERGE (e:Entity {name: $name, kb_id: $kb_id})
                    SET e.type = $type, e.mentions = $mentions
                    """,
                    name=entity['name'],
                    kb_id=kb_id,
                    type=entity['type'],
                    mentions=entity['mentions']
                )
            
            # åˆ›å»ºå…³ç³»
            for rel in relations:
                session.run(
                    """
                    MATCH (a:Entity {name: $head, kb_id: $kb_id})
                    MATCH (b:Entity {name: $tail, kb_id: $kb_id})
                    MERGE (a)-[r:RELATION {type: $rel_type}]->(b)
                    """,
                    head=rel['head'],
                    tail=rel['tail'],
                    rel_type=rel['relation'],
                    kb_id=kb_id
                )
        
        # 4. è¿”å›ç»Ÿè®¡
        return await self.get_stats(kb_id)
    
    async def query_graph(
        self,
        kb_id: int,
        entity: str,
        max_hops: int = 2
    ) -> dict:
        """æŸ¥è¯¢å®ä½“çš„å…³ç³»å›¾è°±"""
        with self.driver.session() as session:
            result = session.run(
                """
                MATCH path = (e:Entity {name: $entity, kb_id: $kb_id})-[*1..$max_hops]-(related)
                RETURN e, related, relationships(path)
                """,
                entity=entity,
                kb_id=kb_id,
                max_hops=max_hops
            )
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            nodes = []
            relationships = []
            for record in result:
                # TODO: æ ¼å¼åŒ–èŠ‚ç‚¹å’Œå…³ç³»
                pass
            
            return {"nodes": nodes, "relationships": relationships}
    
    async def get_stats(self, kb_id: int) -> dict:
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        with self.driver.session() as session:
            result = session.run(
                """
                MATCH (n:Entity {kb_id: $kb_id})
                OPTIONAL MATCH (n)-[r]-()
                RETURN count(DISTINCT n) as nodes, count(r) as relationships
                """,
                kb_id=kb_id
            )
            record = result.single()
            return {
                "nodes": record["nodes"],
                "relationships": record["relationships"]
            }
```

**é‡æ„æ•ˆæœ**ï¼š
- çŸ¥è¯†å›¾è°±å®Œå…¨è§£è€¦
- KnowledgeBaseServiceä¸å†åŒ…å«å›¾è°±ä»£ç 
- æ”¯æŒç‹¬ç«‹ä¼˜åŒ–å›¾è°±ç®—æ³•

---

#### T3.12 åˆ›å»ºRetrieverManagerï¼ˆ1.5å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼ˆåŸºäºå¯è¡Œæ€§éªŒè¯è°ƒæ•´ï¼‰ï¼š
1. æ£€ç´¢å™¨åˆ†æ•£åœ¨å„ä¸ªæœåŠ¡ä¸­åˆå§‹åŒ–ï¼Œç¼ºå°‘ç»Ÿä¸€ç®¡ç†
2. å‘½åå†²çªé£é™©ï¼šä¹‹å‰è®¾è®¡çš„"RetrievalService"ä¸BaseRetrieveræ¥å£æ··æ·†
3. chat_service.pyä¸­æ£€ç´¢é€»è¾‘è°ƒç”¨åˆ†æ•£ï¼Œä¸ä¾¿ç»´æŠ¤

**é‡æ„æ–¹æ¡ˆ**ï¼šåˆ›å»ºRetrieverManagerç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ£€ç´¢ç­–ç•¥

```python
# Backend/app/core/retrieval/retriever_manager.pyï¼ˆæ–°å¢ï¼Œ180è¡Œï¼‰
from typing import Dict, List
from app.core.retrieval.base_retriever import BaseRetriever, RetrievalConfig, Document
from app.core.retrieval.vector_retriever import VectorRetriever
from app.core.retrieval.bm25_retriever import BM25Retriever
from app.core.retrieval.hybrid_retriever import HybridRetriever
from app.core.retrieval.graph_retriever import GraphRetriever

class RetrieverManager:
    """
    æ£€ç´¢ç­–ç•¥ç®¡ç†å™¨ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
    
    èŒè´£ï¼š
    1. ç®¡ç†æ‰€æœ‰æ£€ç´¢å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    2. æä¾›ç»Ÿä¸€çš„æ£€ç´¢æ¥å£
    3. åŠ¨æ€é…ç½®æ£€ç´¢å‚æ•°
    
    è§£å†³é—®é¢˜ï¼š
    - é¿å…å‘½åå†²çªï¼ˆRetrievalService â†’ RetrieverManagerï¼‰
    - é›†ä¸­ç®¡ç†æ£€ç´¢å™¨ï¼ˆé¿å…åˆ†æ•£åˆå§‹åŒ–ï¼‰
    - ç®€åŒ–KnowledgeBaseServiceå’ŒChatServiceçš„è°ƒç”¨
    """
    
    _instance = None
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–æ£€ç´¢å™¨ï¼‰"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.retrievers: Dict[str, BaseRetriever] = {}
        self.default_config = RetrievalConfig(
            top_k=5,
            score_threshold=0.6,
            enable_rerank=False
        )
        self._init_retrievers()
        self._initialized = True
    
    def _init_retrievers(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ£€ç´¢å™¨"""
        from app.services.embedding_service import EmbeddingService
        from app.services.graph.graph_service import GraphService
        
        embedding_service = EmbeddingService()
        graph_service = GraphService()
        
        # å‘é‡æ£€ç´¢å™¨
        vector_retriever = VectorRetriever(
            self.default_config,
            embedding_service
        )
        
        # BM25æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever(self.default_config)
        
        # æ··åˆæ£€ç´¢å™¨
        hybrid_retriever = HybridRetriever(
            self.default_config,
            vector_retriever,
            bm25_retriever,
            vector_weight=0.7
        )
        
        # çŸ¥è¯†å›¾è°±æ£€ç´¢å™¨
        graph_retriever = GraphRetriever(
            self.default_config,
            graph_service
        )
        
        self.retrievers = {
            "vector": vector_retriever,
            "bm25": bm25_retriever,
            "hybrid": hybrid_retriever,
            "graph": graph_retriever
        }
    
    async def retrieve(
        self,
        query: str,
        kb_id: int,
        strategy: str = "hybrid",
        top_k: int = None,
        **kwargs
    ) -> List[Document]:
        """
        ç»Ÿä¸€æ£€ç´¢å…¥å£ï¼ˆè§£å†³chat_serviceä¸­æ£€ç´¢é€»è¾‘åˆ†æ•£é—®é¢˜ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_id: çŸ¥è¯†åº“ID
            strategy: æ£€ç´¢ç­–ç•¥ (vector/bm25/hybrid/graph)
            top_k: è¿”å›ç»“æœæ•°é‡ï¼ˆè¦†ç›–é»˜è®¤é…ç½®ï¼‰
            **kwargs: é¢å¤–å‚æ•°ï¼ˆå¦‚score_thresholdï¼‰
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
        """
        retriever = self.retrievers.get(strategy)
        if not retriever:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ£€ç´¢ç­–ç•¥: {strategy}. "
                f"å¯ç”¨ç­–ç•¥: {list(self.retrievers.keys())}"
            )
        
        # ä¸´æ—¶è¦†ç›–é…ç½®
        if top_k is not None:
            original_top_k = retriever.config.top_k
            retriever.config.top_k = top_k
            
            documents = await retriever.retrieve(query, kb_id, **kwargs)
            
            # æ¢å¤åŸé…ç½®
            retriever.config.top_k = original_top_k
        else:
            documents = await retriever.retrieve(query, kb_id, **kwargs)
        
        return documents
    
    def get_available_strategies(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ£€ç´¢ç­–ç•¥"""
        return list(self.retrievers.keys())
    
    def get_retriever_info(self, strategy: str) -> dict:
        """è·å–æ£€ç´¢å™¨ä¿¡æ¯"""
        retriever = self.retrievers.get(strategy)
        if not retriever:
            return {}
        return retriever.get_retriever_info()
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼ˆè§£å†³knowledge_base_serviceå’Œchat_serviceçš„è°ƒç”¨é—®é¢˜ï¼‰ï¼š

```python
# åœ¨KnowledgeBaseServiceä¸­ä½¿ç”¨ï¼ˆä»£ç å·²åœ¨T3.10ä¸­å±•ç¤ºï¼‰
from app.core.retrieval.retriever_manager import RetrieverManager

class KnowledgeBaseService:
    def __init__(self):
        self.db = DatabaseService()
        self.retriever_manager = RetrieverManager()  # ä½¿ç”¨ç»Ÿä¸€ç®¡ç†å™¨
    
    async def search(
        self,
        query: str,
        kb_id: int,
        strategy: str = "hybrid",
        top_k: int = 5
    ) -> List[dict]:
        """ç»Ÿä¸€æ£€ç´¢å…¥å£"""
        documents = await self.retriever_manager.retrieve(
            query=query,
            kb_id=kb_id,
            strategy=strategy,
            top_k=top_k
        )
        
        return [
            {
                "content": doc.content,
                "metadata": doc.metadata,
                "score": doc.score,
                "doc_id": doc.doc_id
            }
            for doc in documents
        ]

# åœ¨ChatServiceä¸­ä½¿ç”¨ï¼ˆç®€åŒ–æ£€ç´¢é€»è¾‘ï¼‰
class ChatService:
    def __init__(self):
        self.retriever_manager = RetrieverManager()
    
    async def generate_with_context(
        self,
        query: str,
        kb_id: int,
        history: List[dict] = None
    ) -> str:
        """RAGç”Ÿæˆï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨ç»Ÿä¸€ç®¡ç†å™¨ï¼‰
        documents = await self.retriever_manager.retrieve(
            query=query,
            kb_id=kb_id,
            strategy="hybrid"  # é»˜è®¤ä½¿ç”¨æ··åˆæ£€ç´¢
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.content for doc in documents[:3]])
        
        # 3. ç”Ÿæˆå›å¤
        prompt = self._build_prompt(query, context, history)
        response = await self.llm_service.generate(prompt)
        
        return response
```

**é‡æ„æ•ˆæœ**ï¼š
- âœ… è§£å†³å‘½åå†²çªï¼šä½¿ç”¨RetrieverManagerä»£æ›¿RetrievalService
- âœ… é›†ä¸­ç®¡ç†æ‰€æœ‰æ£€ç´¢ç­–ç•¥ï¼ˆå•ä¾‹æ¨¡å¼èŠ‚çœå†…å­˜ï¼‰
- âœ… ç®€åŒ–KnowledgeBaseServiceå’ŒChatServiceçš„æ£€ç´¢é€»è¾‘
- âœ… ç»Ÿä¸€çš„æ£€ç´¢å…¥å£ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
- âœ… æ”¯æŒåŠ¨æ€é…ç½®æ£€ç´¢å‚æ•°

---

### Week 8æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… KnowledgeBaseServiceèŒè´£æ‹†åˆ†ï¼ˆ558è¡Œ â†’ 200è¡ŒCRUDï¼‰
- âœ… DocumentServiceåˆ›å»ºï¼ˆ120è¡Œï¼Œçº¯æ–‡æ¡£ç®¡ç†ï¼‰
- âœ… ChunkingServiceåˆ›å»ºï¼ˆ150è¡Œï¼Œæ–‡æœ¬åˆ†å—ï¼‰
- âœ… VectorizationServiceåˆ›å»ºï¼ˆ180è¡Œï¼Œå‘é‡åŒ–ç®¡ç†ï¼‰
- âœ… GraphServiceç‹¬ç«‹ï¼ˆ180è¡Œï¼ŒçŸ¥è¯†å›¾è°±ï¼‰
- âœ… RetrieverManageråˆ›å»ºï¼ˆ180è¡Œï¼Œæ£€ç´¢ç»Ÿä¸€ç®¡ç†ï¼‰

**ä»£ç é‡å˜åŒ–**ï¼š
- é‡æ„å‰: knowledge_base_service.py (558è¡Œï¼ŒèŒè´£æ··æ‚)
- é‡æ„å: 6ä¸ªç‹¬ç«‹æ–‡ä»¶ï¼ˆ1010è¡Œï¼ŒèŒè´£æ¸…æ™°ï¼‰
  - KnowledgeBaseService: 200è¡Œï¼ˆCRUDï¼‰
  - DocumentService: 120è¡Œï¼ˆæ–‡æ¡£ç®¡ç†ï¼‰
  - ChunkingService: 150è¡Œï¼ˆåˆ†å—ï¼‰
  - VectorizationService: 180è¡Œï¼ˆå‘é‡åŒ–ï¼‰
  - GraphService: 180è¡Œï¼ˆå›¾è°±ï¼‰
  - RetrieverManager: 180è¡Œï¼ˆæ£€ç´¢ç®¡ç†ï¼‰

**æ—¶é—´ä¼°ç®—**ï¼š8.5å¤©ï¼ˆå®é™…å»ºè®®10å¤©ï¼Œå«æµ‹è¯•ï¼‰

---

### é˜¶æ®µ3æ€»ç»“ï¼ˆWeek 6-8ï¼‰

**é‡æ„ç›®æ ‡**ï¼š
- âœ… ç»Ÿä¸€æ£€ç´¢ç­–ç•¥ï¼ˆ4ç§æ£€ç´¢å™¨ + RetrieverManagerï¼‰
- âœ… çŸ¥è¯†åº“æœåŠ¡é‡æ„ï¼ˆ1ä¸ªå·¨å¤§æœåŠ¡ â†’ 6ä¸ªç‹¬ç«‹æœåŠ¡ï¼‰
- âœ… çŸ¥è¯†å›¾è°±ç‹¬ç«‹ï¼ˆGraphServiceï¼‰

**æ¶æ„å˜åŒ–**ï¼š

```
é‡æ„å‰ï¼ˆ558è¡Œknowledge_base_serviceï¼‰ï¼š
knowledge_base_service.py
â”œâ”€â”€ CRUDæ“ä½œ
â”œâ”€â”€ æ–‡æ¡£ç®¡ç†
â”œâ”€â”€ æ–‡æœ¬åˆ†å—
â”œâ”€â”€ å‘é‡åŒ–
â”œâ”€â”€ æ£€ç´¢é€»è¾‘
â””â”€â”€ çŸ¥è¯†å›¾è°±

é‡æ„åï¼ˆæ¨¡å—åŒ–è®¾è®¡ï¼‰ï¼š
core/retrieval/
â”œâ”€â”€ base_retriever.py (100è¡Œ) - æ£€ç´¢æ¥å£
â”œâ”€â”€ vector_retriever.py (150è¡Œ) - å‘é‡æ£€ç´¢
â”œâ”€â”€ bm25_retriever.py (180è¡Œ) - å…¨æ–‡æ£€ç´¢
â”œâ”€â”€ hybrid_retriever.py (200è¡Œ) - æ··åˆæ£€ç´¢
â”œâ”€â”€ graph_retriever.py (150è¡Œ) - å›¾è°±æ£€ç´¢
â””â”€â”€ retriever_manager.py (180è¡Œ) - ç»Ÿä¸€ç®¡ç†

services/knowledge/
â”œâ”€â”€ knowledge_base_service.py (200è¡Œ) - CRUD
â”œâ”€â”€ document_service.py (120è¡Œ) - æ–‡æ¡£ç®¡ç†
â”œâ”€â”€ chunking_service.py (150è¡Œ) - æ–‡æœ¬åˆ†å—
â””â”€â”€ vectorization_service.py (180è¡Œ) - å‘é‡åŒ–

services/graph/
â””â”€â”€ graph_service.py (180è¡Œ) - çŸ¥è¯†å›¾è°±
```

**æ—¶é—´ä¼°ç®—**ï¼š
- Week 6: æ£€ç´¢ç­–ç•¥ç»Ÿä¸€ï¼ˆ5å¤©ï¼‰
- Week 7: æ··åˆæ£€ç´¢ä¸å›¾è°±ï¼ˆ5å¤©ï¼‰
- Week 8: çŸ¥è¯†åº“æœåŠ¡æ‹†åˆ†ï¼ˆ8.5å¤©ï¼Œå»ºè®®10å¤©ï¼‰
- **æ€»è®¡**: 18.5å¤© â†’ **å»ºè®®20å¤©**ï¼ˆå«é›†æˆæµ‹è¯•ï¼‰

**å®Œæˆæ ‡å‡†**ï¼š
1. âœ… æ‰€æœ‰æ£€ç´¢ç­–ç•¥é€šè¿‡å•å…ƒæµ‹è¯•
2. âœ… KnowledgeBaseServiceé‡æ„å®Œæˆ
3. âœ… GraphServiceç‹¬ç«‹è¿è¡Œ
4. âœ… RetrieverManagerç»Ÿä¸€ç®¡ç†æ£€ç´¢é€»è¾‘
5. âœ… ä»£ç è¦†ç›–ç‡ > 70%

---

## å…­ã€é˜¶æ®µ4ï¼šåº”ç”¨å±‚ (Week 9-10)

### ç›®æ ‡

ğŸ¯ é‡æ„chat_serviceï¼ˆ624è¡Œ â†’ 400è¡Œï¼‰  
ğŸ¯ æå–RAG Pipelineç‹¬ç«‹æ¨¡å—  
ğŸ¯ ä¼˜åŒ–AgentæœåŠ¡

---

### Week 9: RAG Pipelineé‡æ„

#### T4.1a åˆ›å»ºRAG PipelineåŸºç¡€ç‰ˆï¼ˆ2å¤©ï¼‰â­

**å½“å‰é—®é¢˜åˆ†æ**ï¼š
- `chat_service.py`ï¼ˆ624è¡Œï¼‰èŒè´£æ··æ‚ï¼šå¯¹è¯ç®¡ç†ã€RAGé€»è¾‘ã€æµå¼è¾“å‡ºã€æ··åˆæ£€ç´¢
- éœ€è¦æ‹†åˆ†ä½†ä¿ç•™ç°æœ‰ç‰¹æ€§ï¼ˆå†å²æ¶ˆæ¯å¢å¼ºã€æ··åˆæ£€ç´¢æ”¯æŒï¼‰

**åŸºç¡€Pipelineæ¶æ„**ï¼š

```python
# Backend/app/core/rag/rag_pipeline.pyï¼ˆæ–°å¢ï¼Œ200è¡Œï¼‰
from typing import List, Dict, Optional, AsyncGenerator
from app.core.llm.base_llm import BaseLLM, Message
from app.core.retrieval.retriever_manager import RetrieverManager  # ä½¿ç”¨é˜¶æ®µ3çš„RetrieverManager

class RAGPipeline:
    """
    RAGå¤„ç†æµæ°´çº¿ï¼ˆåŸºç¡€ç‰ˆï¼‰
    
    èŒè´£ï¼š
    1. åè°ƒæ£€ç´¢å’Œç”Ÿæˆæµç¨‹
    2. æ„å»ºä¸Šä¸‹æ–‡å’Œæç¤ºè¯
    3. æ”¯æŒæµå¼å’Œéæµå¼è¾“å‡º
    """
    
    def __init__(
        self,
        llm: BaseLLM,
        retriever_manager: RetrieverManager,
        prompt_template: str = None
    ):
        self.llm = llm
        self.retriever_manager = retriever_manager
        self.prompt_template = prompt_template or self._default_template()
    
    async def generate(
        self,
        query: str,
        kb_id: int,
        strategy: str = "hybrid",
        chat_history: List[Message] = None,
        stream: bool = False,
        top_k: int = 5,
        **kwargs
    ):
        """
        RAGç”Ÿæˆæµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            kb_id: çŸ¥è¯†åº“ID
            strategy: æ£€ç´¢ç­–ç•¥ï¼ˆvector/bm25/hybrid/graphï¼‰
            chat_history: å¯¹è¯å†å²
            stream: æ˜¯å¦æµå¼è¾“å‡º
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœï¼ˆå­—ç¬¦ä¸²æˆ–æµï¼‰
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨RetrieverManagerç»Ÿä¸€æ¥å£ï¼‰
        documents = await self.retriever_manager.retrieve(
            query=query,
            kb_id=kb_id,
            strategy=strategy,
            top_k=top_k
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(documents)
        
        # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆæ³¨æ„ï¼šè¿™é‡Œå…ˆç”¨åŸºç¡€ç‰ˆï¼Œä¸‹ä¸ªä»»åŠ¡æ·»åŠ å†å²å¢å¼ºï¼‰
        messages = self._build_messages(query, context, chat_history)
        
        # 4. ç”Ÿæˆå›å¤
        if stream:
            return self._generate_stream(messages, documents, **kwargs)
        else:
            response = await self.llm.generate(messages, stream=False, **kwargs)
            return {
                "answer": response,
                "citations": self._format_citations(documents),
                "retrieval_count": len(documents)
            }
    
    def _build_context(self, documents: List) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not documents:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            # é™åˆ¶æ¯ä¸ªæ–‡æ¡£é•¿åº¦
            content = doc.content[:500] if len(doc.content) > 500 else doc.content
            context_parts.append(f"[æ–‡æ¡£{i}] (ç›¸ä¼¼åº¦: {doc.score:.2%})\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _build_messages(
        self,
        query: str,
        context: str,
        chat_history: List[Message] = None
    ) -> List[Message]:
        """
        æ„å»ºå¯¹è¯æ¶ˆæ¯ï¼ˆåŸºç¡€ç‰ˆï¼Œä¸åŒ…å«å†å²å¢å¼ºï¼‰
        
        æ³¨æ„ï¼šå†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘å°†åœ¨T4.1bä¸­æ·»åŠ 
        """
        messages = []
        
        # æ·»åŠ å†å²å¯¹è¯
        if chat_history:
            messages.extend(chat_history)
        
        # æ·»åŠ å½“å‰æŸ¥è¯¢ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
        if context:
            user_message = self.prompt_template.format(
                context=context,
                question=query
            )
        else:
            user_message = query
        
        messages.append(Message(role="user", content=user_message))
        
        return messages
    
    async def _generate_stream(
        self,
        messages: List[Message],
        documents: List,
        **kwargs
    ) -> AsyncGenerator:
        """
        æµå¼ç”Ÿæˆï¼ˆå…ˆå‘é€sourcesï¼Œå†æµå¼è¾“å‡ºç­”æ¡ˆï¼‰
        
        æ³¨æ„ï¼šä¸chat_serviceç°æœ‰å®ç°ä¸€è‡´ï¼ˆç¬¬435-447è¡Œï¼‰
        """
        # 1. å…ˆå‘é€æ£€ç´¢ç»“æœ
        yield {
            "type": "sources",
            "data": {
                "sources": [
                    {
                        "content": doc.content[:200] + ("..." if len(doc.content) > 200 else ""),
                        "similarity": doc.score,
                        "source": doc.metadata.get("source", "unknown"),
                        "metadata": doc.metadata
                    }
                    for doc in documents[:5]
                ],
                "retrieval_count": len(documents)
            }
        }
        
        # 2. æµå¼è¾“å‡ºç­”æ¡ˆ
        async for chunk in await self.llm.generate(messages, stream=True, **kwargs):
            yield {
                "type": "text",
                "data": chunk
            }
        
        # 3. å‘é€å®Œæˆä¿¡å·
        yield {
            "type": "done",
            "data": {}
        }
    
    def _format_citations(self, documents: List) -> List[Dict]:
        """æ ¼å¼åŒ–å¼•ç”¨"""
        citations = []
        for i, doc in enumerate(documents[:3], 1):  # åªè¿”å›å‰3ä¸ªæ¥æº
            citations.append({
                "content": doc.content[:200] + ("..." if len(doc.content) > 200 else ""),
                "similarity": doc.score,
                "source": doc.metadata.get("source", "unknown"),
                "file_id": doc.metadata.get("file_id")
            })
        
        return citations
    
    def _default_template(self) -> str:
        """é»˜è®¤æç¤ºè¯æ¨¡æ¿"""
        return """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
```

**é‡æ„åçš„ChatServiceï¼ˆåŸºç¡€ç‰ˆï¼‰**ï¼š

```python
# Backend/app/services/chat_service.pyï¼ˆé‡æ„åï¼Œç®€åŒ–ä¸º~380è¡Œï¼‰
from typing import List, Dict, Optional, AsyncGenerator
from app.core.rag.rag_pipeline import RAGPipeline
from app.core.retrieval.retriever_manager import RetrieverManager
from app.services.llm_service import LLMService
from app.services.database_service import DatabaseService

class ChatService:
    """å¯¹è¯æœåŠ¡ï¼ˆä¸“æ³¨ä¼šè¯ç®¡ç†ï¼‰"""
    
    def __init__(self, db_manager):
        self.db = DatabaseService(db_manager)
        self.llm_service = LLMService()
        self.retriever_manager = RetrieverManager()  # å•ä¾‹
    
    async def chat_with_assistant(
        self,
        kb_ids: Optional[List[int]],
        query: str,
        history_messages: Optional[List[Dict[str, str]]] = None,
        system_prompt: Optional[str] = None,
        top_k: int = 5,
        llm_model: Optional[str] = None,
        llm_provider: str = "local",
        temperature: float = 0.7,
        use_hybrid_retrieval: bool = False
    ) -> Dict:
        """
        æ™ºèƒ½åŠ©æ‰‹å¯¹è¯ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
        
        æ³¨æ„ï¼šä¿ç•™åŸæœ‰å‚æ•°å…¼å®¹æ€§
        """
        try:
            # 1. å¦‚æœæœ‰çŸ¥è¯†åº“ï¼Œä½¿ç”¨RAGæ¨¡å¼
            if kb_ids and len(kb_ids) > 0:
                return await self._rag_chat(
                    kb_id=kb_ids[0],  # åŸºç¡€ç‰ˆå…ˆæ”¯æŒå•çŸ¥è¯†åº“
                    query=query,
                    history_messages=history_messages,
                    system_prompt=system_prompt,
                    top_k=top_k,
                    llm_model=llm_model,
                    llm_provider=llm_provider,
                    temperature=temperature,
                    strategy="hybrid" if use_hybrid_retrieval else "vector"
                )
            else:
                # 2. çº¯å¯¹è¯æ¨¡å¼
                return await self._normal_chat(
                    query=query,
                    history_messages=history_messages,
                    system_prompt=system_prompt,
                    llm_model=llm_model,
                    llm_provider=llm_provider,
                    temperature=temperature
                )
        
        except Exception as e:
            logger.error(f"å¯¹è¯å¤±è´¥: {str(e)}")
            raise
    
    async def _rag_chat(
        self,
        kb_id: int,
        query: str,
        history_messages: Optional[List[Dict[str, str]]],
        system_prompt: Optional[str],
        top_k: int,
        llm_model: Optional[str],
        llm_provider: str,
        temperature: float,
        strategy: str
    ) -> Dict:
        """RAGå¯¹è¯ï¼ˆä½¿ç”¨Pipelineï¼‰"""
        # 1. è·å–LLM
        llm = await self._get_llm(llm_provider, llm_model, temperature)
        
        # 2. åˆ›å»ºRAG Pipeline
        pipeline = RAGPipeline(
            llm=llm,
            retriever_manager=self.retriever_manager
        )
        
        # 3. è½¬æ¢å†å²æ¶ˆæ¯æ ¼å¼
        chat_history = self._convert_history(history_messages) if history_messages else None
        
        # 4. ç”Ÿæˆå›å¤
        result = await pipeline.generate(
            query=query,
            kb_id=kb_id,
            strategy=strategy,
            chat_history=chat_history,
            top_k=top_k,
            stream=False
        )
        
        return {
            "answer": result["answer"],
            "sources": result["citations"],
            "retrieval_count": result["retrieval_count"],
            "embedding_model": await self._get_kb_embedding_model(kb_id)
        }
    
    async def _normal_chat(
        self,
        query: str,
        history_messages: Optional[List[Dict[str, str]]],
        system_prompt: Optional[str],
        llm_model: Optional[str],
        llm_provider: str,
        temperature: float
    ) -> Dict:
        """æ™®é€šå¯¹è¯ï¼ˆæ— RAGï¼‰"""
        # 1. è·å–LLM
        llm = await self._get_llm(llm_provider, llm_model, temperature)
        
        # 2. æ„å»ºæ¶ˆæ¯
        messages = []
        if system_prompt:
            messages.append(Message(role="system", content=system_prompt))
        if history_messages:
            messages.extend(self._convert_history(history_messages))
        messages.append(Message(role="user", content=query))
        
        # 3. ç”Ÿæˆå›å¤
        response = await llm.generate(messages, stream=False)
        
        return {
            "answer": response,
            "sources": [],
            "retrieval_count": 0
        }
    
    async def _get_llm(self, provider: str, model: str, temperature: float):
        """è·å–LLMå®ä¾‹ï¼ˆç»Ÿä¸€å°è£…ï¼‰"""
        if provider in ["local", "transformers"]:
            from app.services.transformers_service import get_transformers_service
            service = get_transformers_service()
            # TODO: åŒ…è£…ä¸ºBaseLLMæ¥å£
            return service
        elif provider == "ollama":
            from app.services.ollama_llm_service import get_ollama_llm_service
            service = get_ollama_llm_service()
            # TODO: åŒ…è£…ä¸ºBaseLLMæ¥å£
            return service
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›æ–¹: {provider}")
    
    def _convert_history(self, history_messages: List[Dict]) -> List[Message]:
        """è½¬æ¢å†å²æ¶ˆæ¯æ ¼å¼"""
        return [
            Message(role=msg["role"], content=msg["content"])
            for msg in history_messages
        ]
    
    async def _get_kb_embedding_model(self, kb_id: int) -> str:
        """è·å–çŸ¥è¯†åº“ä½¿ç”¨çš„embeddingæ¨¡å‹"""
        from app.services.knowledge_base_service import KnowledgeBaseService
        kb_service = KnowledgeBaseService(self.db.db_manager)
        kb = await kb_service.get_knowledge_base(kb_id)
        return kb.embedding_model if kb else "unknown"
    
    # æµå¼å¯¹è¯æ–¹æ³•ä¿æŒç±»ä¼¼ç»“æ„...
```

**é‡æ„æ•ˆæœï¼ˆT4.1aï¼‰**ï¼š
- âœ… RAG Pipelineç‹¬ç«‹æ¨¡å—ï¼ˆ200è¡Œï¼‰
- âœ… ä½¿ç”¨RetrieverManagerç»Ÿä¸€æ£€ç´¢æ¥å£ï¼ˆè§£å†³éªŒè¯é—®é¢˜1ï¼‰
- âœ… æµå¼è¾“å‡ºå…ˆå‘sourceså†æµç­”æ¡ˆï¼ˆè§£å†³éªŒè¯é—®é¢˜4ï¼‰
- âœ… ChatServiceç®€åŒ–ä¸º380è¡Œï¼ˆ-39%ï¼‰

---

#### T4.1b æ·»åŠ å†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘ï¼ˆ1å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
- chat_service.pyçš„`_build_user_message`æ–¹æ³•ï¼ˆ50-90è¡Œï¼‰æœ‰å¤æ‚çš„å†å²çº¦å®šä¼˜å…ˆé€»è¾‘
- åŸºç¡€Pipelineæœªä¿ç•™æ­¤é‡è¦ç‰¹æ€§

**å¢å¼ºæ–¹æ¡ˆ**ï¼š

```python
# Backend/app/core/rag/rag_pipeline.pyï¼ˆæ›´æ–°ï¼‰
class RAGPipeline:
    # ... ä¿ç•™ä¸Šé¢çš„ä»£ç  ...
    
    def _build_messages_enhanced(
        self,
        query: str,
        context: str,
        chat_history: List[Message] = None,
        enable_history_priority: bool = True
    ) -> List[Message]:
        """
        æ„å»ºå¯¹è¯æ¶ˆæ¯ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå†å²çº¦å®šä¼˜å…ˆï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢ä¸Šä¸‹æ–‡
            chat_history: å¯¹è¯å†å²
            enable_history_priority: æ˜¯å¦å¯ç”¨å†å²çº¦å®šä¼˜å…ˆï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        
        # 1. å¢å¼ºç³»ç»Ÿæç¤ºï¼ˆå½“æœ‰å†å²æ—¶ï¼‰
        if chat_history and len(chat_history) > 0 and enable_history_priority:
            enhanced_system = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚

ã€æ ¸å¿ƒè§„åˆ™ã€‘ä½ å¿…é¡»è®°ä½æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯å†…å®¹å’Œçº¦å®šï¼Œå¹¶åœ¨å›ç­”æ—¶ä¼˜å…ˆéµå¾ªå¯¹è¯å†å²ä¸­çš„ä¿¡æ¯ã€‚å¦‚æœæˆ‘ä¹‹å‰å‘Šè¯‰ä½ æŸä¸ªç‰¹å®šçš„è§„åˆ™æˆ–äº‹å®ï¼ˆå³ä½¿å®ƒä¸å¸¸è¯†ä¸åŒï¼‰ï¼Œä½ å¿…é¡»æŒ‰ç…§æˆ‘è¯´çš„æ¥å›ç­”ã€‚"""
            messages.append(Message(role="system", content=enhanced_system))
        
        # 2. æ·»åŠ å†å²æ¶ˆæ¯
        if chat_history:
            # åªä¿ç•™æœ€è¿‘Nè½®å¯¹è¯ï¼ˆé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
            recent_history = chat_history[-8:]  # æœ€è¿‘4è½®å¯¹è¯ï¼ˆuser+assistantï¼‰
            messages.extend(recent_history)
            
            logger.info(f"æ·»åŠ å†å²æ¶ˆæ¯: {len(recent_history)}æ¡")
        
        # 3. æ„å»ºå½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆå†å²çº¦å®šä¼˜å…ˆæç¤ºï¼‰
        if context:
            if chat_history and len(chat_history) > 0 and enable_history_priority:
                # æœ‰å†å²å¯¹è¯ï¼šå¼ºåˆ¶å†å²çº¦å®šä¼˜å…ˆ
                history_summary = self._extract_history_summary(chat_history[-4:])  # æœ€è¿‘2è½®
                
                user_message = f"""â€¼ï¸ã€é‡è¦ã€‘æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯çº¦å®šï¼š
{history_summary}

---

ç°åœ¨æœ‰ä»¥ä¸‹å‚è€ƒèµ„æ–™ï¼š
{context}

---

é—®é¢˜ï¼š{query}

â€¼ï¸ã€å›ç­”è§„åˆ™ - å¼ºåˆ¶æ‰§è¡Œã€‘ï¼š
1. å¦‚æœæˆ‘ä¹‹å‰æ˜ç¡®å‘Šè¯‰ä½ æŸä¸ªè§„åˆ™æˆ–ç­”æ¡ˆï¼ˆä¾‹å¦‚"è®°ä½1+1=3"ï¼‰ï¼Œä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æˆ‘è¯´çš„å›ç­”ï¼Œå³ä½¿ä¸å¸¸è¯†ä¸åŒ
2. å¯¹è¯å†å²ä¸­çš„çº¦å®šæ‹¥æœ‰æœ€é«˜ä¼˜å…ˆçº§ï¼Œç»å¯¹é«˜äºå‚è€ƒèµ„æ–™
3. åªæœ‰å½“å¯¹è¯å†å²å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œæ‰ä½¿ç”¨å‚è€ƒèµ„æ–™
4. ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸è¦è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹

å›ç­”ï¼š"""
            else:
                # æ— å†å²å¯¹è¯ï¼šæ ‡å‡†RAGæ¨¡å¼
                user_message = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
        else:
            # çº¯å¯¹è¯æ¨¡å¼ï¼ˆæ— æ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
            user_message = query
        
        messages.append(Message(role="user", content=user_message))
        
        return messages
    
    def _extract_history_summary(self, recent_messages: List[Message]) -> str:
        """
        æå–å†å²å¯¹è¯æ‘˜è¦
        
        Args:
            recent_messages: æœ€è¿‘çš„æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„å†å²æ‘˜è¦
        """
        summary_parts = []
        for msg in recent_messages:
            # é™åˆ¶æ¯æ¡æ¶ˆæ¯é•¿åº¦
            content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
            summary_parts.append(f"{msg.role}: {content}")
        
        return "\n".join(summary_parts)
    
    async def generate(
        self,
        query: str,
        kb_id: int,
        strategy: str = "hybrid",
        chat_history: List[Message] = None,
        stream: bool = False,
        top_k: int = 5,
        enable_history_priority: bool = True,  # æ–°å¢å‚æ•°
        **kwargs
    ):
        """
        RAGç”Ÿæˆæµç¨‹ï¼ˆæ›´æ–°ç‰ˆæœ¬ï¼‰
        
        æ–°å¢å‚æ•°:
            enable_history_priority: æ˜¯å¦å¯ç”¨å†å²çº¦å®šä¼˜å…ˆï¼ˆé»˜è®¤Trueï¼‰
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        documents = await self.retriever_manager.retrieve(
            query=query,
            kb_id=kb_id,
            strategy=strategy,
            top_k=top_k
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(documents)
        
        # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆï¼‰
        messages = self._build_messages_enhanced(
            query=query,
            context=context,
            chat_history=chat_history,
            enable_history_priority=enable_history_priority
        )
        
        # 4. ç”Ÿæˆå›å¤
        if stream:
            return self._generate_stream(messages, documents, **kwargs)
        else:
            response = await self.llm.generate(messages, stream=False, **kwargs)
            return {
                "answer": response,
                "citations": self._format_citations(documents),
                "retrieval_count": len(documents)
            }
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š

```python
# Backend/app/tests/test_rag_pipeline.py
import pytest
from app.core.rag.rag_pipeline import RAGPipeline
from app.core.llm.base_llm import Message

@pytest.mark.asyncio
async def test_history_priority():
    """æµ‹è¯•å†å²çº¦å®šä¼˜å…ˆé€»è¾‘"""
    pipeline = RAGPipeline(mock_llm, mock_retriever)
    
    # æ¨¡æ‹Ÿå†å²å¯¹è¯
    history = [
        Message(role="user", content="è®°ä½ï¼š1+1=3"),
        Message(role="assistant", content="å¥½çš„ï¼Œæˆ‘è®°ä½äº†ï¼š1+1=3")
    ]
    
    # æ„å»ºæ¶ˆæ¯
    messages = pipeline._build_messages_enhanced(
        query="1+1ç­‰äºå‡ ï¼Ÿ",
        context="æ•°å­¦çŸ¥è¯†ï¼š1+1=2",
        chat_history=history,
        enable_history_priority=True
    )
    
    # éªŒè¯ï¼šuseræ¶ˆæ¯åº”åŒ…å«å†å²ä¼˜å…ˆæç¤º
    user_msg = messages[-1].content
    assert "å¯¹è¯å†å²ä¸­çš„çº¦å®šæ‹¥æœ‰æœ€é«˜ä¼˜å…ˆçº§" in user_msg
    assert "1+1=3" in user_msg  # å†å²çº¦å®š
    assert "1+1=2" in user_msg  # æ£€ç´¢ä¸Šä¸‹æ–‡
```

**é‡æ„æ•ˆæœï¼ˆT4.1bï¼‰**ï¼š
- âœ… ä¿ç•™å†å²çº¦å®šä¼˜å…ˆé€»è¾‘ï¼ˆè§£å†³éªŒè¯é—®é¢˜2ï¼‰
- âœ… æ”¯æŒå¼€å…³æ§åˆ¶ï¼ˆenable_history_priorityå‚æ•°ï¼‰
- âœ… å†å²æ¶ˆæ¯é•¿åº¦æ§åˆ¶ï¼ˆæœ€è¿‘8æ¡ï¼‰
- âœ… å®Œå…¨å…¼å®¹ç°æœ‰chat_serviceè¡Œä¸º

---

#### T4.1c æ•´åˆæ··åˆæ£€ç´¢åˆ°RetrieverManagerï¼ˆ1å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
- chat_service.pyæœ‰ç‹¬ç«‹çš„`_hybrid_search`æ–¹æ³•ï¼ˆ70è¡Œï¼‰è°ƒç”¨`hybrid_retrieval_service`
- éœ€è¦å°†å‘é‡+å›¾è°±èåˆæ•´åˆåˆ°RetrieverManager

**æ•´åˆæ–¹æ¡ˆ**ï¼š

```python
# Backend/app/core/retrieval/retriever_manager.pyï¼ˆæ›´æ–°ï¼Œæ·»åŠ å›¾è°±æ”¯æŒï¼‰
class RetrieverManager:
    """æ£€ç´¢ç­–ç•¥ç®¡ç†å™¨ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå›¾è°±èåˆï¼‰"""
    
    def __init__(self):
        # ... ä¿ç•™åŸæœ‰ä»£ç  ...
        self.graph_enabled = False  # é»˜è®¤å…³é—­
        self._check_graph_availability()
    
    def _check_graph_availability(self):
        """æ£€æŸ¥çŸ¥è¯†å›¾è°±æ˜¯å¦å¯ç”¨"""
        try:
            from app.core.config import settings
            self.graph_enabled = settings.knowledge_graph.enabled
            logger.info(f"çŸ¥è¯†å›¾è°±çŠ¶æ€: {'å¯ç”¨' if self.graph_enabled else 'ç¦ç”¨'}")
        except Exception as e:
            logger.warning(f"æ— æ³•æ£€æµ‹å›¾è°±çŠ¶æ€: {e}")
            self.graph_enabled = False
    
    async def retrieve(
        self,
        query: str,
        kb_id: int,
        strategy: str = "hybrid",
        top_k: int = None,
        enable_graph: bool = None,  # æ–°å¢ï¼šæ˜¾å¼æ§åˆ¶å›¾è°±
        **kwargs
    ) -> List[Document]:
        """
        ç»Ÿä¸€æ£€ç´¢å…¥å£ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå›¾è°±èåˆï¼‰
        
        æ–°å¢å‚æ•°:
            enable_graph: æ˜¯å¦å¯ç”¨å›¾è°±å¢å¼ºï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        # 1. å¦‚æœç­–ç•¥æ˜¯hybridä¸”å›¾è°±å¯ç”¨ï¼Œè¿›è¡Œèåˆæ£€ç´¢
        if strategy == "hybrid" and self._should_use_graph(enable_graph):
            return await self._hybrid_search_with_graph(query, kb_id, top_k, **kwargs)
        
        # 2. æ™®é€šæ£€ç´¢
        retriever = self.retrievers.get(strategy)
        if not retriever:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ£€ç´¢ç­–ç•¥: {strategy}. "
                f"å¯ç”¨ç­–ç•¥: {list(self.retrievers.keys())}"
            )
        
        # ä¸´æ—¶è¦†ç›–é…ç½®
        if top_k is not None:
            original_top_k = retriever.config.top_k
            retriever.config.top_k = top_k
            
            documents = await retriever.retrieve(query, kb_id, **kwargs)
            
            # æ¢å¤åŸé…ç½®
            retriever.config.top_k = original_top_k
        else:
            documents = await retriever.retrieve(query, kb_id, **kwargs)
        
        return documents
    
    def _should_use_graph(self, enable_graph: Optional[bool]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å›¾è°±"""
        if enable_graph is not None:
            # æ˜¾å¼æŒ‡å®š
            return enable_graph and self.graph_enabled
        else:
            # è‡ªåŠ¨æ£€æµ‹
            return self.graph_enabled
    
    async def _hybrid_search_with_graph(
        self,
        query: str,
        kb_id: int,
        top_k: int = 5,
        **kwargs
    ) -> List[Document]:
        """
        æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å›¾è°±èåˆï¼‰
        
        æ•´åˆè‡ªchat_serviceçš„_hybrid_searchæ–¹æ³•
        """
        try:
            from app.services.hybrid_retrieval_service import get_hybrid_retrieval_service
            
            hybrid_service = get_hybrid_retrieval_service()
            
            # è°ƒç”¨æ··åˆæ£€ç´¢æœåŠ¡
            results = await hybrid_service.hybrid_search(
                kb_id=kb_id,
                query=query,
                top_k=top_k,
                enable_graph=True
            )
            
            # è½¬æ¢ä¸ºDocumentå¯¹è±¡
            documents = []
            for result in results:
                documents.append(Document(
                    content=result['content'],
                    metadata=result.get('metadata', {}),
                    score=result.get('final_score', result.get('score', 0)),
                    doc_id=result.get('chunk_id', 'unknown')
                ))
            
            return documents
            
        except Exception as e:
            logger.error(f"å›¾è°±èåˆæ£€ç´¢å¤±è´¥ï¼Œé™çº§ä¸ºå‘é‡æ£€ç´¢: {str(e)}")
            # é™çº§ä¸ºçº¯å‘é‡æ£€ç´¢
            vector_retriever = self.retrievers["vector"]
            return await vector_retriever.retrieve(query, kb_id, **kwargs)
```

**ChatServiceæ›´æ–°**ï¼š

```python
# Backend/app/services/chat_service.pyï¼ˆåˆ é™¤_hybrid_searchæ–¹æ³•ï¼‰
class ChatService:
    # ... ä¿ç•™å…¶ä»–ä»£ç  ...
    
    # âŒ åˆ é™¤åŸæœ‰çš„_hybrid_searchæ–¹æ³•ï¼ˆ70è¡Œï¼‰
    # async def _hybrid_search(...):
    #     ...  # å·²è¿ç§»åˆ°RetrieverManager
    
    async def _rag_chat(
        self,
        kb_id: int,
        query: str,
        history_messages: Optional[List[Dict[str, str]]],
        system_prompt: Optional[str],
        top_k: int,
        llm_model: Optional[str],
        llm_provider: str,
        temperature: float,
        strategy: str
    ) -> Dict:
        """RAGå¯¹è¯ï¼ˆæ›´æ–°ï¼šä½¿ç”¨RetrieverManagerçš„å›¾è°±æ”¯æŒï¼‰"""
        # 1. è·å–LLM
        llm = await self._get_llm(llm_provider, llm_model, temperature)
        
        # 2. åˆ›å»ºRAG Pipelineï¼ˆRetrieverManagerè‡ªåŠ¨å¤„ç†å›¾è°±ï¼‰
        pipeline = RAGPipeline(
            llm=llm,
            retriever_manager=self.retriever_manager
        )
        
        # 3. è½¬æ¢å†å²æ¶ˆæ¯æ ¼å¼
        chat_history = self._convert_history(history_messages) if history_messages else None
        
        # 4. ç”Ÿæˆå›å¤ï¼ˆstrategy="hybrid"æ—¶è‡ªåŠ¨ä½¿ç”¨å›¾è°±ï¼‰
        result = await pipeline.generate(
            query=query,
            kb_id=kb_id,
            strategy=strategy,  # "hybrid"ä¼šè‡ªåŠ¨å¯ç”¨å›¾è°±
            chat_history=chat_history,
            top_k=top_k,
            stream=False
        )
        
        return {
            "answer": result["answer"],
            "sources": result["citations"],
            "retrieval_count": result["retrieval_count"],
            "embedding_model": await self._get_kb_embedding_model(kb_id)
        }
```

**é‡æ„æ•ˆæœï¼ˆT4.1cï¼‰**ï¼š
- âœ… å›¾è°±èåˆé€»è¾‘è¿ç§»åˆ°RetrieverManagerï¼ˆè§£å†³éªŒè¯é—®é¢˜3ï¼‰
- âœ… ChatServiceåˆ é™¤_hybrid_searchæ–¹æ³•ï¼ˆ-70è¡Œï¼‰
- âœ… è‡ªåŠ¨æ£€æµ‹å›¾è°±å¯ç”¨æ€§ï¼Œå¤±è´¥æ—¶é™çº§
- âœ… æ”¯æŒæ˜¾å¼æ§åˆ¶å›¾è°±å¯ç”¨ï¼ˆenable_graphå‚æ•°ï¼‰

---

#### T4.1d é›†æˆæµ‹è¯•ä¸è°ƒè¯•ï¼ˆ1å¤©ï¼‰

**æµ‹è¯•æ¸…å•**ï¼š

```bash
# 1. å•å…ƒæµ‹è¯•
pytest app/tests/test_rag_pipeline.py -v
pytest app/tests/test_retriever_manager.py -v

# 2. é›†æˆæµ‹è¯•ï¼šRAGå®Œæ•´æµç¨‹
pytest app/tests/test_chat_service_integration.py -v

# 3. æ€§èƒ½æµ‹è¯•ï¼šç¡®ä¿æ— æ€§èƒ½ä¸‹é™
python benchmark/rag_latency.py --before --after

# 4. åŠŸèƒ½éªŒè¯
# - çº¯å¯¹è¯æ¨¡å¼
# - RAGæ¨¡å¼ï¼ˆå‘é‡æ£€ç´¢ï¼‰
# - RAGæ¨¡å¼ï¼ˆæ··åˆæ£€ç´¢ï¼‰
# - RAGæ¨¡å¼ï¼ˆæ··åˆæ£€ç´¢+å›¾è°±ï¼‰
# - å†å²çº¦å®šä¼˜å…ˆåŠŸèƒ½
# - æµå¼è¾“å‡ºåŠŸèƒ½
```

**è°ƒè¯•é‡ç‚¹**ï¼š
1. RetrieverManagerå•ä¾‹æ¨¡å¼æ˜¯å¦æ­£å¸¸å·¥ä½œ
2. å†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘æ˜¯å¦ä¿ç•™åŸæœ‰è¡Œä¸º
3. å›¾è°±èåˆé™çº§æœºåˆ¶æ˜¯å¦ç”Ÿæ•ˆ
4. æµå¼è¾“å‡ºsourcesä½ç½®æ˜¯å¦æ­£ç¡®

---

### Week 9æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… T4.1aï¼šRAG PipelineåŸºç¡€ç‰ˆï¼ˆ200è¡Œï¼‰
- âœ… T4.1bï¼šå†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘ï¼ˆä¿ç•™50è¡Œå¤æ‚Promptï¼‰
- âœ… T4.1cï¼šå›¾è°±èåˆæ•´åˆåˆ°RetrieverManager
- âœ… T4.1dï¼šé›†æˆæµ‹è¯•ä¸è°ƒè¯•

**ä»£ç é‡å˜åŒ–**ï¼š
- RAG Pipeline: æ–°å¢200è¡Œ
- RetrieverManager: æ›´æ–°+60è¡Œï¼ˆå›¾è°±æ”¯æŒï¼‰
- ChatService: 624è¡Œ â†’ 380è¡Œï¼ˆ-244è¡Œï¼Œ-39%ï¼‰

**å…³é”®ä¿®å¤**ï¼š
1. âœ… ä½¿ç”¨RetrieverManagerä»£æ›¿RetrieverFactory
2. âœ… ä¿ç•™å†å²çº¦å®šä¼˜å…ˆé€»è¾‘ï¼ˆ50è¡ŒPromptæ„å»ºï¼‰
3. âœ… æ•´åˆæ··åˆæ£€ç´¢åˆ°RetrieverManager
4. âœ… æµå¼è¾“å‡ºå…ˆå‘sourceså†æµç­”æ¡ˆ

**æ—¶é—´ç»Ÿè®¡**ï¼šWeek 9ç”¨æ—¶5å¤©ï¼ˆå«æµ‹è¯•ï¼‰

---
        """è·å–å¯¹è¯å†å²"""
        messages = self.db.get_session_messages(session_id, limit=limit)
        return [
            Message(role=m['role'], content=m['content'])
            for m in messages
        ]
```

---

### Week 10: AgentæœåŠ¡ä¼˜åŒ–

#### T4.2a åˆ›å»ºç»Ÿä¸€ToolRegistryï¼ˆ1.5å¤©ï¼‰â­

**å½“å‰é—®é¢˜åˆ†æ**ï¼š
- agent_service.pyæœ‰è‡ªå®šä¹‰Toolç±»ï¼ˆ30è¡Œï¼‰
- å·¥å…·æ³¨å†Œæ–¹å¼åˆ†æ•£ï¼ˆ`register_tool`æ–¹æ³• + `_register_default_tools`ï¼‰
- ç¼ºå°‘å‚æ•°éªŒè¯å’Œå¼‚æ­¥æ”¯æŒ

**ç»Ÿä¸€ToolDefinitionè®¾è®¡**ï¼š

```python
# Backend/app/core/agent/tool_definition.pyï¼ˆæ–°å¢ï¼Œ120è¡Œï¼‰
from typing import Dict, Callable, Any, Optional
from dataclasses import dataclass
import asyncio
import json
import logging

logger = logging.getlogger(__name__)

@dataclass
class ToolDefinition:
    """
    å·¥å…·å®šä¹‰ï¼ˆç»Ÿä¸€ç‰ˆæœ¬ï¼Œæ›¿ä»£æ—§çš„Toolç±»ï¼‰
    
    æ”¹è¿›ï¼š
    1. æ”¯æŒå‚æ•°éªŒè¯
    2. æ”¯æŒå¼‚æ­¥å‡½æ•°
    3. OpenAIå‡½æ•°è°ƒç”¨æ ¼å¼å…¼å®¹
    """
    name: str
    description: str
    parameters: Dict[str, Any]
    function: Callable
    
    def validate_input(self, input_params: Dict) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯è¾“å…¥å‚æ•°
        
        Returns:
            (is_valid, error_message)
        """
        required = self.parameters.get("required", [])
        
        # æ£€æŸ¥å¿…éœ€å‚æ•°
        for param in required:
            if param not in input_params:
                return False, f"ç¼ºå°‘å¿…éœ€å‚æ•°: {param}"
        
        # æ£€æŸ¥å‚æ•°ç±»å‹ï¼ˆç®€å•éªŒè¯ï¼‰
        properties = self.parameters.get("properties", {})
        for key, value in input_params.items():
            if key in properties:
                expected_type = properties[key].get("type")
                actual_type = type(value).__name__
                
                # ç±»å‹æ˜ å°„
                type_mapping = {
                    "str": "string",
                    "int": "integer",
                    "float": "number",
                    "bool": "boolean",
                    "list": "array",
                    "dict": "object"
                }
                
                if type_mapping.get(actual_type) != expected_type:
                    return False, f"å‚æ•° {key} ç±»å‹é”™è¯¯: æœŸæœ› {expected_type}, å®é™… {actual_type}"
        
        return True, None
    
    async def run(self, **kwargs) -> str:
        """
        æ‰§è¡Œå·¥å…·ï¼ˆæ”¯æŒå¼‚æ­¥ï¼‰
        
        Args:
            **kwargs: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœï¼ˆå­—ç¬¦ä¸²ï¼‰
        """
        try:
            # 1. éªŒè¯è¾“å…¥
            is_valid, error_msg = self.validate_input(kwargs)
            if not is_valid:
                return f"[å‚æ•°é”™è¯¯] {error_msg}"
            
            # 2. æ‰§è¡Œå‡½æ•°
            if asyncio.iscoroutinefunction(self.function):
                result = await self.function(**kwargs)
            else:
                result = self.function(**kwargs)
            
            # 3. æ ¼å¼åŒ–è¿”å›
            return str(result)
        
        except Exception as e:
            logger.error(f"å·¥å…· {self.name} æ‰§è¡Œå¤±è´¥: {str(e)}")
            return f"[æ‰§è¡Œé”™è¯¯] {str(e)}"
    
    def to_openai_format(self) -> Dict:
        """è½¬æ¢ä¸ºOpenAIå‡½æ•°è°ƒç”¨æ ¼å¼"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": self.parameters
        }


class ToolRegistry:
    """
    å·¥å…·æ³¨å†Œè¡¨ï¼ˆé›†ä¸­ç®¡ç†Agentå·¥å…·ï¼‰
    
    ç‰¹ç‚¹ï¼š
    1. è£…é¥°å™¨æ³¨å†Œæ¨¡å¼
    2. å•ä¾‹æ¨¡å¼ï¼ˆå…¨å±€å…±äº«ï¼‰
    3. æ”¯æŒåŠ¨æ€æ·»åŠ /åˆ é™¤å·¥å…·
    """
    
    _instance = None
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.tools: Dict[str, ToolDefinition] = {}
        self._initialized = True
        logger.info("ToolRegistry initialized")
    
    def register(
        self,
        name: str,
        description: str,
        parameters: Dict[str, Any]
    ):
        """
        æ³¨å†Œå·¥å…·ï¼ˆè£…é¥°å™¨æ¨¡å¼ï¼‰
        
        ä½¿ç”¨ç¤ºä¾‹:
            @registry.register(
                name="calculator",
                description="æ‰§è¡Œæ•°å­¦è®¡ç®—",
                parameters={...}
            )
            def calculator(expression: str):
                return eval(expression)
        """
        def decorator(func: Callable):
            self.tools[name] = ToolDefinition(
                name=name,
                description=description,
                parameters=parameters,
                function=func
            )
            logger.info(f"å·¥å…·å·²æ³¨å†Œ: {name}")
            return func
        return decorator
    
    def register_tool(self, tool: ToolDefinition):
        """ç›´æ¥æ³¨å†Œå·¥å…·å¯¹è±¡"""
        self.tools[tool.name] = tool
        logger.info(f"å·¥å…·å·²æ³¨å†Œ: {tool.name}")
    
    def unregister(self, name: str):
        """æ³¨é”€å·¥å…·"""
        if name in self.tools:
            del self.tools[name]
            logger.info(f"å·¥å…·å·²æ³¨é”€: {name}")
    
    def get_tool(self, name: str) -> Optional[ToolDefinition]:
        """è·å–å·¥å…·"""
        return self.tools.get(name)
    
    def list_tools(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å·¥å…·ï¼ˆOpenAIæ ¼å¼ï¼‰"""
        return [tool.to_openai_format() for tool in self.tools.values()]
    
    def get_tools_description(self) -> str:
        """è·å–å·¥å…·æè¿°ï¼ˆç”¨äºPromptï¼‰"""
        descriptions = []
        for tool in self.tools.values():
            descriptions.append(f"- {tool.name}: {tool.description}")
        return "\n".join(descriptions)
```

**å†…ç½®å·¥å…·å®šä¹‰**ï¼š

```python
# Backend/app/core/agent/builtin_tools.pyï¼ˆæ–°å¢ï¼Œ180è¡Œï¼‰
from app.core.agent.tool_definition import ToolRegistry
from app.services.knowledge_base_service import KnowledgeBaseService
from app.core.retrieval.retriever_manager import RetrieverManager
from datetime import datetime
import logging

logger = logging.getlogger(__name__)

# å…¨å±€æ³¨å†Œè¡¨
registry = ToolRegistry()


@registry.register(
    name="knowledge_search",
    description="åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£å’Œä¿¡æ¯ã€‚é€‚ç”¨äºæŸ¥è¯¢é¡¹ç›®æ–‡æ¡£ã€æŠ€æœ¯èµ„æ–™ç­‰ã€‚",
    parameters={
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "æœç´¢æŸ¥è¯¢å…³é”®è¯æˆ–é—®é¢˜"
            },
            "kb_id": {
                "type": "integer",
                "description": "çŸ¥è¯†åº“IDï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†æœç´¢æ‰€æœ‰çŸ¥è¯†åº“"
            },
            "top_k": {
                "type": "integer",
                "description": "è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤3"
            }
        },
        "required": ["query"]
    }
)
async def knowledge_search(query: str, kb_id: int = None, top_k: int = 3) -> str:
    """
    çŸ¥è¯†åº“æœç´¢å·¥å…·ï¼ˆæ›´æ–°ï¼šé€‚é…é˜¶æ®µ3é‡æ„åçš„æ¥å£ï¼‰
    """
    try:
        retriever_manager = RetrieverManager()
        
        if kb_id:
            # æœç´¢æŒ‡å®šçŸ¥è¯†åº“
            documents = await retriever_manager.retrieve(
                query=query,
                kb_id=kb_id,
                strategy="hybrid",  # ä½¿ç”¨æ··åˆæ£€ç´¢
                top_k=top_k
            )
        else:
            # æœç´¢æ‰€æœ‰çŸ¥è¯†åº“ï¼ˆéœ€è¦è·å–çŸ¥è¯†åº“åˆ—è¡¨ï¼‰
            from app.services.database_service import DatabaseService
            from app.core.database import get_db_manager
            
            db = DatabaseService(get_db_manager())
            kbs = await db.get_all_knowledge_bases()
            
            all_documents = []
            for kb in kbs[:3]:  # æœ€å¤šæœç´¢3ä¸ªçŸ¥è¯†åº“
                docs = await retriever_manager.retrieve(
                    query=query,
                    kb_id=kb['id'],
                    strategy="hybrid",
                    top_k=2
                )
                all_documents.extend(docs)
            
            # æŒ‰åˆ†æ•°æ’åº
            all_documents.sort(key=lambda d: d.score, reverse=True)
            documents = all_documents[:top_k]
        
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted = []
        for i, doc in enumerate(documents, 1):
            content = doc.content[:200] + "..." if len(doc.content) > 200 else doc.content
            source = doc.metadata.get('source', 'æœªçŸ¥')
            score = doc.score
            formatted.append(f"{i}. [{source}] (ç›¸å…³åº¦: {score:.2%})\n{content}")
        
        return "\n\n".join(formatted)
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {str(e)}")
        return f"æœç´¢å¤±è´¥: {str(e)}"


@registry.register(
    name="calculator",
    description="æ‰§è¡Œæ•°å­¦è®¡ç®—ã€‚æ”¯æŒåŸºæœ¬ç®—æœ¯è¿ç®—ï¼ˆ+ã€-ã€*ã€/ã€**ï¼‰å’Œæ‹¬å·ã€‚",
    parameters={
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ï¼š'2+3*4'ã€'(10-5)**2'"
            }
        },
        "required": ["expression"]
    }
)
def calculator(expression: str) -> str:
    """è®¡ç®—å™¨å·¥å…·ï¼ˆå¢å¼ºå®‰å…¨æ€§ï¼‰"""
    try:
        # å®‰å…¨çš„æ•°å­¦è¡¨è¾¾å¼æ±‚å€¼
        allowed_chars = set('0123456789+-*/(). ')
        if not all(c in allowed_chars for c in expression):
            return "è¡¨è¾¾å¼åŒ…å«ä¸å…è®¸çš„å­—ç¬¦"
        
        # ç¦ç”¨å†…ç½®å‡½æ•°
        result = eval(expression, {"__builtins__": {}}, {})
        return f"è®¡ç®—ç»“æœ: {result}"
    
    except ZeroDivisionError:
        return "é”™è¯¯ï¼šé™¤æ•°ä¸èƒ½ä¸ºé›¶"
    except Exception as e:
        return f"è®¡ç®—å¤±è´¥: {str(e)}"


@registry.register(
    name="get_current_time",
    description="è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´ã€‚",
    parameters={
        "type": "object",
        "properties": {},
        "required": []
    }
)
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´å·¥å…·"""
    now = datetime.now()
    return now.strftime("%Y-%m-%d %H:%M:%S æ˜ŸæœŸ%w")


@registry.register(
    name="web_search",
    description="åœ¨äº’è”ç½‘ä¸Šæœç´¢æœ€æ–°ä¿¡æ¯ï¼ˆåŠŸèƒ½å¼€å‘ä¸­ï¼‰ã€‚",
    parameters={
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "æœç´¢æŸ¥è¯¢"
            }
        },
        "required": ["query"]
    }
)
async def web_search(query: str) -> str:
    """ç½‘ç»œæœç´¢å·¥å…·ï¼ˆå ä½ç¬¦ï¼‰"""
    # TODO: é›†æˆæœç´¢APIï¼ˆBingã€Googleç­‰ï¼‰
    return "ç½‘ç»œæœç´¢åŠŸèƒ½å¼€å‘ä¸­ï¼Œæš‚æ—¶æ— æ³•ä½¿ç”¨ã€‚"
```

**é‡æ„æ•ˆæœï¼ˆT4.2aï¼‰**ï¼š
- âœ… ç»Ÿä¸€ToolDefinitionï¼ˆæ”¯æŒå‚æ•°éªŒè¯ã€å¼‚æ­¥ï¼‰
- âœ… è£…é¥°å™¨æ³¨å†Œæ¨¡å¼ï¼ˆä»£ç æ›´ä¼˜é›…ï¼‰
- âœ… å•ä¾‹ToolRegistryï¼ˆå…¨å±€å…±äº«ï¼‰
- âœ… OpenAIå‡½æ•°è°ƒç”¨æ ¼å¼å…¼å®¹

---

#### T4.2b æ›´æ–°AgentServiceé›†æˆToolRegistryï¼ˆ1å¤©ï¼‰â­

**é‡æ„æ–¹æ¡ˆ**ï¼š

```python
# Backend/app/services/agent_service.pyï¼ˆé‡æ„åï¼Œç®€åŒ–ä¸º~250è¡Œï¼‰
import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging

from app.core.agent.tool_definition import ToolRegistry
from app.core.agent.builtin_tools import registry as builtin_registry

logger = logging.getLogger(__name__)


class AgentService:
    """
    Agent æœåŠ¡ - åŸºäº ReAct æ¡†æ¶ï¼ˆé‡æ„ç‰ˆï¼‰
    
    æ”¹è¿›ï¼š
    1. ä½¿ç”¨ToolRegistryç»Ÿä¸€ç®¡ç†å·¥å…·
    2. åˆ é™¤æ—§çš„Toolç±»
    3. æ”¯æŒå¼‚æ­¥å·¥å…·æ‰§è¡Œ
    """
    
    def __init__(self, llm_service, max_iterations: int = 5):
        """
        åˆå§‹åŒ– Agent
        
        Args:
            llm_service: LLM æœåŠ¡å®ä¾‹
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        self.llm_service = llm_service
        self.max_iterations = max_iterations
        self.tool_registry = builtin_registry  # ä½¿ç”¨å…¨å±€æ³¨å†Œè¡¨
        self.conversation_history: List[Dict] = []
    
    def register_custom_tool(self, tool):
        """æ³¨å†Œè‡ªå®šä¹‰å·¥å…·"""
        self.tool_registry.register_tool(tool)
    
    def _build_prompt(self, user_query: str) -> str:
        """æ„å»º Agent æç¤ºè¯ï¼ˆæ›´æ–°ï¼šä½¿ç”¨ToolRegistryï¼‰"""
        
        # å·¥å…·åˆ—è¡¨ï¼ˆè‡ªåŠ¨ä»æ³¨å†Œè¡¨è·å–ï¼‰
        tools_desc = self.tool_registry.get_tools_description()
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ Agentï¼Œèƒ½å¤Ÿä½¿ç”¨å·¥å…·æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å¯ç”¨å·¥å…·:
{tools_desc}

è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”:

Thought: æˆ‘éœ€è¦æ€è€ƒå¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜
Action: å·¥å…·åç§°
Action Input: å·¥å…·çš„è¾“å…¥å‚æ•°(JSONæ ¼å¼)
Observation: [å·¥å…·è¿”å›çš„ç»“æœä¼šæ˜¾ç¤ºåœ¨è¿™é‡Œ]
... (å¯ä»¥é‡å¤ Thought/Action/Observation å¤šæ¬¡)
Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: æœ€ç»ˆç­”æ¡ˆ

é‡è¦è§„åˆ™:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡º
2. Action Input å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼
3. å¦‚æœä¸éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥ç»™å‡º Final Answer
4. æ¯æ¬¡åªæ‰§è¡Œä¸€ä¸ª Action

ç”¨æˆ·é—®é¢˜: {user_query}

å¼€å§‹!
"""
        return prompt
    
    def _parse_action(self, text: str) -> Optional[tuple]:
        """è§£æ LLM è¾“å‡ºä¸­çš„ Action"""
        action_match = re.search(r'Action:\s*(.+?)(?:\n|$)', text, re.IGNORECASE)
        input_match = re.search(r'Action Input:\s*(.+?)(?:\n|$)', text, re.IGNORECASE | re.DOTALL)
        
        if not action_match:
            return None
        
        action_name = action_match.group(1).strip()
        action_input = input_match.group(1).strip() if input_match else "{}"
        
        return action_name, action_input
    
    def _parse_final_answer(self, text: str) -> Optional[str]:
        """è§£ææœ€ç»ˆç­”æ¡ˆ"""
        match = re.search(r'Final Answer:\s*(.+)', text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()
        return None
    
    async def run(self, user_query: str, session_id: str = None) -> Dict[str, Any]:
        """
        è¿è¡Œ Agentï¼ˆæ›´æ–°ï¼šä½¿ç”¨ToolRegistryï¼‰
        
        Args:
            user_query: ç”¨æˆ·é—®é¢˜
            session_id: ä¼šè¯ID
        
        Returns:
            {
                "answer": "æœ€ç»ˆç­”æ¡ˆ",
                "steps": [æ‰§è¡Œæ­¥éª¤],
                "success": True/False
            }
        """
        steps = []
        prompt = self._build_prompt(user_query)
        
        try:
            for iteration in range(self.max_iterations):
                logger.info(f"Agent iteration {iteration + 1}/{self.max_iterations}")
                
                # è°ƒç”¨ LLM
                response = await self.llm_service.generate(
                    prompt=prompt,
                    max_tokens=500,
                    temperature=0.1
                )
                
                llm_output = response.get('text', '')
                logger.debug(f"LLM output: {llm_output}")
                
                # è®°å½•æ€è€ƒ
                thought_match = re.search(r'Thought:\s*(.+?)(?:\n|$)', llm_output, re.IGNORECASE)
                if thought_match:
                    steps.append({
                        "type": "thought",
                        "content": thought_match.group(1).strip()
                    })
                
                # æ£€æŸ¥æœ€ç»ˆç­”æ¡ˆ
                final_answer = self._parse_final_answer(llm_output)
                if final_answer:
                    steps.append({
                        "type": "final_answer",
                        "content": final_answer
                    })
                    return {
                        "answer": final_answer,
                        "steps": steps,
                        "success": True,
                        "iterations": iteration + 1
                    }
                
                # è§£æå¹¶æ‰§è¡Œ Action
                action_result = self._parse_action(llm_output)
                if action_result:
                    action_name, action_input = action_result
                    
                    # è·å–å·¥å…·ï¼ˆä»æ³¨å†Œè¡¨ï¼‰
                    tool = self.tool_registry.get_tool(action_name)
                    
                    if tool:
                        steps.append({
                            "type": "action",
                            "tool": action_name,
                            "input": action_input
                        })
                        
                        try:
                            # è§£æ JSON è¾“å…¥
                            input_params = json.loads(action_input)
                            
                            # æ‰§è¡Œå·¥å…·ï¼ˆæ”¯æŒå¼‚æ­¥ï¼‰
                            observation = await tool.run(**input_params)
                        
                        except json.JSONDecodeError:
                            # é JSONï¼Œå°è¯•ä½œä¸ºå•å‚æ•°
                            observation = await tool.run(action_input)
                        
                        except Exception as e:
                            observation = f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
                        
                        steps.append({
                            "type": "observation",
                            "content": observation
                        })
                        
                        # æ›´æ–° prompt
                        prompt += f"\n{llm_output}\nObservation: {observation}\n"
                    
                    else:
                        error_msg = f"æœªæ‰¾åˆ°å·¥å…·: {action_name}"
                        steps.append({
                            "type": "error",
                            "content": error_msg
                        })
                        prompt += f"\n{llm_output}\nObservation: {error_msg}\n"
                
                else:
                    # æ²¡æœ‰è¯†åˆ«åˆ°æ ‡å‡†æ ¼å¼
                    return {
                        "answer": llm_output,
                        "steps": steps,
                        "success": True,
                        "iterations": iteration + 1,
                        "note": "æœªè¯†åˆ«åˆ°æ ‡å‡†æ ¼å¼ï¼Œè¿”å›åŸå§‹è¾“å‡º"
                    }
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨é™å®šæ­¥éª¤å†…å®Œæˆä»»åŠ¡ã€‚",
                "steps": steps,
                "success": False,
                "iterations": self.max_iterations,
                "error": "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°"
            }
        
        except Exception as e:
            logger.error(f"Agent execution failed: {str(e)}")
            return {
                "answer": f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "steps": steps,
                "success": False,
                "error": str(e)
            }
    
    def get_tools_info(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·¥å…·ä¿¡æ¯"""
        return self.tool_registry.list_tools()
```

**é‡æ„æ•ˆæœï¼ˆT4.2bï¼‰**ï¼š
- âœ… åˆ é™¤æ—§Toolç±»ï¼ˆ~30è¡Œï¼‰
- âœ… é›†æˆToolRegistryï¼ˆè§£å†³éªŒè¯é—®é¢˜5ï¼‰
- âœ… AgentServiceç®€åŒ–ï¼š328è¡Œ â†’ 250è¡Œï¼ˆ-24%ï¼‰
- âœ… æ”¯æŒå¼‚æ­¥å·¥å…·æ‰§è¡Œ

---

#### T4.2c Agenté›†æˆæµ‹è¯•ï¼ˆ0.5å¤©ï¼‰

**æµ‹è¯•æ¸…å•**ï¼š

```bash
# 1. å·¥å…·æ³¨å†Œæµ‹è¯•
pytest app/tests/test_tool_registry.py -v

# 2. AgentåŸºç¡€åŠŸèƒ½æµ‹è¯•
pytest app/tests/test_agent_service.py -v

# 3. å·¥å…·æ‰§è¡Œæµ‹è¯•
# - çŸ¥è¯†åº“æœç´¢å·¥å…·
# - è®¡ç®—å™¨å·¥å…·
# - æ—¶é—´å·¥å…·

# 4. ReActæµç¨‹æµ‹è¯•
# - Thought â†’ Action â†’ Observation â†’ Final Answer
# - å¤šè½®è¿­ä»£
# - é”™è¯¯å¤„ç†
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š

```python
# Backend/app/tests/test_agent_service.py
import pytest
from app.services.agent_service import AgentService
from app.core.agent.tool_definition import ToolRegistry, ToolDefinition

@pytest.mark.asyncio
async def test_agent_with_calculator():
    """æµ‹è¯•Agentä½¿ç”¨è®¡ç®—å™¨å·¥å…·"""
    agent = AgentService(mock_llm_service)
    
    result = await agent.run("1+1ç­‰äºå‡ ï¼Ÿ")
    
    assert result["success"] == True
    assert "2" in result["answer"]
    assert any(step["type"] == "action" and step["tool"] == "calculator" 
               for step in result["steps"])

@pytest.mark.asyncio
async def test_tool_parameter_validation():
    """æµ‹è¯•å·¥å…·å‚æ•°éªŒè¯"""
    registry = ToolRegistry()
    
    @registry.register(
        name="test_tool",
        description="æµ‹è¯•å·¥å…·",
        parameters={
            "type": "object",
            "properties": {
                "required_param": {"type": "string"}
            },
            "required": ["required_param"]
        }
    )
    def test_tool(required_param: str):
        return required_param
    
    tool = registry.get_tool("test_tool")
    
    # æµ‹è¯•ç¼ºå°‘å¿…éœ€å‚æ•°
    is_valid, error = tool.validate_input({})
    assert is_valid == False
    assert "required_param" in error
```

---

### Week 10æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… T4.2aï¼šç»Ÿä¸€ToolRegistryï¼ˆ120è¡Œï¼‰+ å†…ç½®å·¥å…·ï¼ˆ180è¡Œï¼‰
- âœ… T4.2bï¼šé‡æ„AgentServiceï¼ˆ328è¡Œ â†’ 250è¡Œï¼‰
- âœ… T4.2cï¼šAgenté›†æˆæµ‹è¯•

**ä»£ç é‡å˜åŒ–**ï¼š
- ToolDefinition + ToolRegistry: æ–°å¢120è¡Œ
- builtin_tools: æ–°å¢180è¡Œ
- AgentService: 328è¡Œ â†’ 250è¡Œï¼ˆ-78è¡Œï¼Œ-24%ï¼‰

**å…³é”®ä¿®å¤**ï¼š
1. âœ… ç»Ÿä¸€Toolå®šä¹‰ï¼ˆè§£å†³éªŒè¯é—®é¢˜5ï¼‰
2. âœ… æ·»åŠ å‚æ•°éªŒè¯ï¼ˆè§£å†³éªŒè¯é—®é¢˜6ï¼‰
3. âœ… é€‚é…æ–°KnowledgeBaseServiceæ¥å£ï¼ˆè§£å†³éªŒè¯é—®é¢˜7ï¼‰
4. âœ… æ”¯æŒå¼‚æ­¥å·¥å…·æ‰§è¡Œ

**æ—¶é—´ç»Ÿè®¡**ï¼šWeek 10ç”¨æ—¶3å¤©ï¼ˆå«æµ‹è¯•ï¼‰

---

---

### é˜¶æ®µ4æ€»ç»“ï¼ˆWeek 9-10ï¼‰

**é‡æ„ç›®æ ‡**ï¼š
- âœ… æå–RAG Pipelineç‹¬ç«‹æ¨¡å—
- âœ… é‡æ„ChatServiceï¼ˆ624è¡Œ â†’ 380è¡Œï¼‰
- âœ… ä¼˜åŒ–Agentå·¥å…·ç®¡ç†

**æ¶æ„å˜åŒ–**ï¼š

```
é‡æ„å‰ï¼ˆchat_service.py 624è¡Œï¼‰ï¼š
chat_service.py
â”œâ”€â”€ å¯¹è¯ç®¡ç†ï¼ˆä¼šè¯ã€å†å²ï¼‰
â”œâ”€â”€ RAGé€»è¾‘ï¼ˆæ£€ç´¢ã€ä¸Šä¸‹æ–‡ã€Promptï¼‰
â”œâ”€â”€ æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å›¾è°±èåˆï¼‰
â”œâ”€â”€ æµå¼è¾“å‡º
â”œâ”€â”€ LLMè°ƒç”¨ï¼ˆtransformers + ollamaï¼‰
â””â”€â”€ å†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘ï¼ˆ50è¡Œï¼‰

é‡æ„åï¼ˆæ¨¡å—åŒ–è®¾è®¡ï¼‰ï¼š
core/rag/
â””â”€â”€ rag_pipeline.py (200è¡Œ)
    â”œâ”€â”€ æ£€ç´¢åè°ƒï¼ˆä½¿ç”¨RetrieverManagerï¼‰
    â”œâ”€â”€ ä¸Šä¸‹æ–‡æ„å»º
    â”œâ”€â”€ æ¶ˆæ¯æ„å»ºï¼ˆåŒ…å«å†å²å¢å¼ºï¼‰
    â””â”€â”€ æµå¼è¾“å‡ºç®¡ç†

core/retrieval/
â””â”€â”€ retriever_manager.py (æ›´æ–°+60è¡Œ)
    â””â”€â”€ æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å›¾è°±èåˆï¼‰

core/agent/
â”œâ”€â”€ tool_definition.py (120è¡Œ)
â”‚   â”œâ”€â”€ ToolDefinitionï¼ˆå‚æ•°éªŒè¯ã€å¼‚æ­¥æ”¯æŒï¼‰
â”‚   â””â”€â”€ ToolRegistryï¼ˆè£…é¥°å™¨æ³¨å†Œï¼‰
â””â”€â”€ builtin_tools.py (180è¡Œ)
    â””â”€â”€ å†…ç½®å·¥å…·ï¼ˆçŸ¥è¯†åº“æœç´¢ã€è®¡ç®—å™¨ç­‰ï¼‰

services/
â”œâ”€â”€ chat_service.py (380è¡Œ, -39%)
â”‚   â””â”€â”€ ä¸“æ³¨ä¼šè¯ç®¡ç†
â””â”€â”€ agent_service.py (250è¡Œ, -24%)
    â””â”€â”€ ä½¿ç”¨ToolRegistry
```

**ä»£ç é‡ç»Ÿè®¡**ï¼š

| æ¨¡å— | é‡æ„å‰ | é‡æ„å | å˜åŒ– |
|------|--------|--------|------|
| chat_service.py | 624è¡Œ | 380è¡Œ | -244è¡Œ (-39%) |
| agent_service.py | 328è¡Œ | 250è¡Œ | -78è¡Œ (-24%) |
| RAG Pipeline | 0è¡Œ | 200è¡Œ | +200è¡Œï¼ˆæ–°å¢ï¼‰ |
| Toolç³»ç»Ÿ | 30è¡Œ | 300è¡Œ | +270è¡Œï¼ˆé‡æ„ï¼‰ |
| **æ€»è®¡** | 982è¡Œ | 1130è¡Œ | +148è¡Œï¼ˆ+15%ï¼‰|

æ³¨ï¼šä»£ç æ€»é‡ç•¥å¢ï¼Œä½†æ¨¡å—åŒ–ç¨‹åº¦å¤§å¹…æå‡ï¼Œå¤ç”¨æ€§å¢å¼º

**å…³é”®ä¿®å¤**ï¼ˆé’ˆå¯¹éªŒè¯å‘ç°çš„5ä¸ªé—®é¢˜ï¼‰ï¼š

1. âœ… **é—®é¢˜1ï¼šç»Ÿä¸€æ£€ç´¢å™¨è·å–æ–¹å¼**
   - ä½¿ç”¨RetrieverManagerä»£æ›¿RetrieverFactory
   - å•ä¾‹æ¨¡å¼ï¼Œå…¨å±€å…±äº«

2. âœ… **é—®é¢˜2ï¼šä¿ç•™å†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘**
   - `_build_messages_enhanced`æ–¹æ³•ï¼ˆ~90è¡Œï¼‰
   - å†å²çº¦å®šä¼˜å…ˆPromptï¼ˆä¸ç°æœ‰50è¡Œé€»è¾‘ä¸€è‡´ï¼‰
   - æ”¯æŒå¼€å…³æ§åˆ¶ï¼ˆenable_history_priorityå‚æ•°ï¼‰

3. âœ… **é—®é¢˜3ï¼šæ•´åˆæ··åˆæ£€ç´¢**
   - å›¾è°±èåˆé€»è¾‘è¿ç§»åˆ°RetrieverManager
   - `_hybrid_search_with_graph`æ–¹æ³•
   - è‡ªåŠ¨æ£€æµ‹å›¾è°±å¯ç”¨æ€§ï¼Œå¤±è´¥æ—¶é™çº§

4. âœ… **é—®é¢˜4ï¼šæµå¼citationsä½ç½®**
   - å…ˆå‘é€sourceså†æµå¼è¾“å‡ºç­”æ¡ˆ
   - ä¸chat_serviceç°æœ‰å®ç°ä¸€è‡´

5. âœ… **é—®é¢˜5-7ï¼šAgentå·¥å…·ç»Ÿä¸€**
   - ç»Ÿä¸€ToolDefinitionï¼ˆæ”¯æŒå‚æ•°éªŒè¯ã€å¼‚æ­¥ï¼‰
   - è£…é¥°å™¨æ³¨å†Œæ¨¡å¼
   - é€‚é…æ–°KnowledgeBaseServiceæ¥å£

**æ—¶é—´ä¼°ç®—**ï¼š
- Week 9ï¼šRAG Pipelineé‡æ„ï¼ˆ5å¤©ï¼‰
  - T4.1aï¼šåŸºç¡€ç‰ˆï¼ˆ2å¤©ï¼‰
  - T4.1bï¼šå†å²å¢å¼ºï¼ˆ1å¤©ï¼‰
  - T4.1cï¼šå›¾è°±æ•´åˆï¼ˆ1å¤©ï¼‰
  - T4.1dï¼šæµ‹è¯•è°ƒè¯•ï¼ˆ1å¤©ï¼‰

- Week 10ï¼šAgentä¼˜åŒ–ï¼ˆ3å¤©ï¼‰
  - T4.2aï¼šToolRegistryï¼ˆ1.5å¤©ï¼‰
  - T4.2bï¼šAgentServiceé‡æ„ï¼ˆ1å¤©ï¼‰
  - T4.2cï¼šé›†æˆæµ‹è¯•ï¼ˆ0.5å¤©ï¼‰

- **æ€»è®¡**ï¼š8å¤©ï¼ˆåŸè®¡åˆ’5å¤©ï¼Œ+60%ï¼‰

**å®Œæˆæ ‡å‡†**ï¼š
1. âœ… RAG Pipelineç‹¬ç«‹æ¨¡å—æµ‹è¯•é€šè¿‡
2. âœ… ChatServiceé‡æ„å®Œæˆï¼ˆ624è¡Œ â†’ 380è¡Œï¼Œ-39%ï¼‰
3. âœ… Agentå·¥å…·æ³¨å†Œè¡¨å®ç°ï¼ˆæ”¯æŒå‚æ•°éªŒè¯ã€å¼‚æ­¥ï¼‰
4. âœ… ç«¯åˆ°ç«¯RAGæµç¨‹æµ‹è¯•é€šè¿‡
5. âœ… å†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘ä¿ç•™
6. âœ… æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å›¾è°±ï¼‰æ­£å¸¸å·¥ä½œ

**é¢„æœŸæ•ˆæœ**ï¼š
- â¬‡ï¸ chat_service: 624è¡Œ â†’ 380è¡Œ (-39%)
- â¬‡ï¸ agent_service: 328è¡Œ â†’ 250è¡Œ (-24%)
- â¬†ï¸ RAGå¤ç”¨æ€§: Pipelineå¯ç”¨äºå¤šç§åœºæ™¯ï¼ˆchatã€agentã€æ‰¹å¤„ç†ç­‰ï¼‰
- â¬†ï¸ Agentæ‰©å±•æ€§: æ–°å¢å·¥å…·åªéœ€è£…é¥°å™¨æ³¨å†Œ
- â¬†ï¸ å¯æµ‹è¯•æ€§: æ¨¡å—ç‹¬ç«‹ï¼Œæ˜“äºå•å…ƒæµ‹è¯•
- â¬†ï¸ å¯ç»´æŠ¤æ€§: èŒè´£æ¸…æ™°ï¼Œä¿®æ”¹å±€éƒ¨ä¸å½±å“å…¨å±€

---

## ä¸ƒã€é˜¶æ®µ5ï¼šè¿ç§»ã€æ¸…ç†ä¸éªŒè¯ (Week 11-12)

### ç›®æ ‡

ğŸ¯ è¿ç§»æ‰€æœ‰æ—§æœåŠ¡è°ƒç”¨åˆ°æ–°æ¶æ„  
ğŸ¯ åˆ é™¤æ—§ä»£ç å’ŒåºŸå¼ƒæœåŠ¡  
ğŸ¯ å›å½’æµ‹è¯•ä¸æ–‡æ¡£è¡¥å…¨

**å…³é”®ä¿®å¤**ï¼ˆé’ˆå¯¹éªŒè¯å‘ç°çš„é—®é¢˜ï¼‰ï¼š

- âœ… æ–°å¢APIè¿ç§»ä»»åŠ¡ï¼ˆå¤„ç†11å¤„æ—§æœåŠ¡å¼•ç”¨ï¼‰
- âœ… è°ƒæ•´åˆ é™¤æ—§æœåŠ¡çš„å‰ç½®æ¡ä»¶ï¼ˆç¡®è®¤å¼•ç”¨å·²è¿ç§»ï¼‰
- âœ… é™ä½æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡ï¼ˆ80% â†’ 50%ï¼‰
- âœ… ç§»é™¤æ€§èƒ½ä¼˜åŒ–ä»»åŠ¡ï¼ˆåº”åœ¨é˜¶æ®µ1-4å®ç°ï¼‰

### Week 11: APIè¿ç§»ï¼ˆ5å¤©ï¼‰

**æ ¸å¿ƒä»»åŠ¡**ï¼šå°†æ‰€æœ‰æ—§æœåŠ¡å¼•ç”¨è¿ç§»åˆ°æ–°æ¶æ„ï¼ˆ11å¤„è°ƒç”¨ï¼‰

#### T5.1 è¿ç§»chat_serviceï¼ˆ2å¤©ï¼‰

**ç›®æ ‡**ï¼šå°†chat_serviceä»æ—§æœåŠ¡è¿ç§»åˆ°æ–°LLMæŠ½è±¡å±‚

**è¿ç§»ç‚¹**ï¼ˆ2å¤„ï¼‰ï¼š

```python
# æ—§ä»£ç ï¼ˆåˆ é™¤ï¼‰
from app.services.transformers_service import get_transformers_service
from app.services.ollama_llm_service import get_ollama_llm_service

transformers_svc = get_transformers_service()
response = transformers_svc.generate(...)

# æ–°ä»£ç ï¼ˆæ›¿æ¢ï¼‰
from app.services.llm.llm_service import LLMService

llm_service = LLMService()
llm = await llm_service.get_llm(model_type, model_name)
response = await llm.generate(prompt, **params)
```

**è¿ç§»æ··åˆæ£€ç´¢**ï¼š

```python
# æ—§ä»£ç 
from app.services.hybrid_retrieval_service import get_hybrid_retrieval_service
hybrid_svc = get_hybrid_retrieval_service()
results = hybrid_svc.search(...)

# æ–°ä»£ç 
from app.core.retrieval.retriever_manager import RetrieverManager
retriever_mgr = RetrieverManager()
results = await retriever_mgr.hybrid_search(...)
```

**æµ‹è¯•**ï¼š

- å•å…ƒæµ‹è¯•ï¼š`test_chat_service_migration.py`
- é›†æˆæµ‹è¯•ï¼šç«¯åˆ°ç«¯å¯¹è¯æµç¨‹

---

#### T5.2 è¿ç§»APIç«¯ç‚¹ï¼ˆ2å¤©ï¼‰

**ç›®æ ‡**ï¼šæ›´æ–°APIè·¯ç”±å±‚çš„æœåŠ¡è°ƒç”¨

**è¿ç§»æ¸…å•**ï¼ˆ6å¤„ï¼‰ï¼š

**1. api/lora_training.py**ï¼ˆ2å¤„ï¼‰

```python
# æ—§
from app.services.transformers_service import TransformersService
transformers_service = TransformersService()

# æ–°
from app.core.llm.transformers_llm import TransformersLLM
transformers_llm = TransformersLLM(...)
```

**2. api/agent.py**ï¼ˆ1å¤„ï¼‰

```python
# æ—§
ollama_service = OllamaLLMService()

# æ–°
llm = await LLMService.get_llm("ollama", model_name)
```

**3. api/assistant.py**ï¼ˆ2å¤„ï¼‰

```python
# æ—§
from app.services.ollama_llm_service import get_ollama_llm_service
ollama_svc = get_ollama_llm_service()

# æ–°
from app.services.llm.llm_service import LLMService
llm = await LLMService.get_llm("ollama", model_name)
```

**æµ‹è¯•**ï¼š

- æ›´æ–°`test_05_api_endpoints.py`
- å›å½’æµ‹è¯•æ‰€æœ‰APIåŠŸèƒ½

---

#### T5.3 è¿ç§»å·¥å…·ç±»ï¼ˆ1å¤©ï¼‰

**ç›®æ ‡**ï¼šæ›´æ–°utils/servicesä¸­çš„æœåŠ¡è°ƒç”¨

**è¿ç§»æ¸…å•**ï¼ˆ3å¤„ï¼‰ï¼š

**1. entity_extraction_service.py**

```python
# æ—§
from app.services.ollama_llm_service import OllamaLLMService
self.ollama = ollama_service or OllamaLLMService()

# æ–°
from app.services.llm.llm_service import LLMService
self.llm_service = LLMService()
self.llm = await self.llm_service.get_llm("ollama", ...)
```

**2. utils/semantic_splitter.py**

```python
# æ—§
from app.services.ollama_llm_service import get_ollama_llm_service

# æ–°
from app.services.llm.llm_service import LLMService
```

**3. model_scanner.py**

```python
# æ—§
from app.services.ollama_llm_service import get_ollama_llm_service

# æ–°
from app.services.llm.llm_service import LLMService
```

**éªŒè¯**ï¼š

- å®ä½“æå–æµ‹è¯•
- è¯­ä¹‰åˆ†å‰²æµ‹è¯•
- æ¨¡å‹æ‰«æåŠŸèƒ½æµ‹è¯•

---

#### Week 11æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… chat_serviceè¿ç§»ï¼ˆ2å¤„å¼•ç”¨ï¼‰
- âœ… APIç«¯ç‚¹è¿ç§»ï¼ˆ6å¤„å¼•ç”¨ï¼‰
- âœ… å·¥å…·ç±»è¿ç§»ï¼ˆ3å¤„å¼•ç”¨ï¼‰
- âœ… æ··åˆæ£€ç´¢è¿ç§»ï¼ˆ1å¤„å¼•ç”¨ï¼‰
- **æ€»è®¡**ï¼š11å¤„å¼•ç”¨å…¨éƒ¨è¿ç§»å®Œæˆ

**ä»£ç å˜åŒ–**ï¼š
- è¿ç§»ä»£ç ï¼šçº¦300è¡Œ
- æµ‹è¯•ä»£ç ï¼šçº¦200è¡Œ

**å…³é”®é£é™©**ï¼š
- âš ï¸ LLMæ¥å£å˜åŒ–éœ€è¦å……åˆ†æµ‹è¯•
- âš ï¸ å¼‚æ­¥è°ƒç”¨æ”¹é€ éœ€è¦æ³¨æ„å¼‚å¸¸å¤„ç†

---

### Week 12: æ¸…ç†ä¸éªŒè¯ï¼ˆ3å¤©ï¼‰

#### T5.4 åˆ é™¤æ—§æœåŠ¡ï¼ˆ0.5å¤©ï¼‰

**å‰ç½®æ¡ä»¶**ï¼šâœ… æ‰€æœ‰11å¤„å¼•ç”¨å·²è¿ç§»å®Œæˆ

**åˆ é™¤æ¸…å•**ï¼ˆéªŒè¯æ— å¼•ç”¨åï¼‰ï¼š
```bash
# 1. æœ€ç»ˆç¡®è®¤æ— å¼•ç”¨
grep -r "transformers_service" app/  # åº”ä¸ºç©º
grep -r "ollama_llm_service" app/    # åº”ä¸ºç©º
grep -r "hybrid_retrieval_service" app/  # åº”ä¸ºç©º

# 2. åˆ é™¤æ—§æ–‡ä»¶ï¼ˆ3ä¸ªï¼Œå…±1474è¡Œï¼‰
git rm app/services/transformers_service.py      # 835è¡Œ
git rm app/services/ollama_llm_service.py        # 265è¡Œ
git rm app/services/hybrid_retrieval_service.py  # 374è¡Œ

# 3. æäº¤æ¸…ç†
git commit -m "refactor(stage5): remove deprecated services after migration"
```

---

#### T5.5 å›å½’æµ‹è¯•ï¼ˆ1.5å¤©ï¼‰

**ç›®æ ‡**ï¼šç¡®ä¿è¿ç§»åç³»ç»ŸåŠŸèƒ½æ­£å¸¸

**æµ‹è¯•æ¸…å•**ï¼š
```bash
# 1. å•å…ƒæµ‹è¯•ï¼ˆç›®æ ‡è¦†ç›–ç‡50%ï¼‰
pytest test/ -v --cov=app --cov-report=html

# 2. é›†æˆæµ‹è¯•
pytest test/integration/ -v

# 3. ç«¯åˆ°ç«¯æµ‹è¯•
# - å¯¹è¯æµç¨‹ï¼ˆtransformers + ollamaï¼‰
# - RAGæ£€ç´¢ï¼ˆå‘é‡ + æ··åˆ + å›¾è°±ï¼‰
# - Agentå·¥å…·è°ƒç”¨
# - LoRAè®­ç»ƒ/æ¨ç†
# - å®ä½“æå–
# - è¯­ä¹‰åˆ†å‰²
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ˆ0å¤±è´¥ï¼‰
- âœ… æ ¸å¿ƒæ¨¡å—è¦†ç›–ç‡ â‰¥ 50%
- âœ… æ— APIåŠŸèƒ½é€€åŒ–
- âœ… æ— æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼ˆÂ±10%ä»¥å†…ï¼‰

**æµ‹è¯•é‡ç‚¹**ï¼š
```python
# 1. LLMæ¥å£å…¼å®¹æ€§
assert new_llm.generate() == old_service.generate()

# 2. æ£€ç´¢ç»“æœä¸€è‡´æ€§
assert new_retriever.search() â‰ˆ old_retrieval.search()

# 3. å¼‚æ­¥è°ƒç”¨æ­£ç¡®æ€§
await test_async_chat_flow()

# 4. é”™è¯¯å¤„ç†å®Œæ•´æ€§
try:
    await llm.generate(invalid_params)
except Exception as e:
    assert isinstance(e, ExpectedException)
```

---

#### T5.6 æ–‡æ¡£æ›´æ–°ï¼ˆ1å¤©ï¼‰

**ç›®æ ‡**ï¼šè¡¥å…¨é‡æ„æ–‡æ¡£

**æ–‡æ¡£æ¸…å•**ï¼š

1. **APIè¿ç§»æŒ‡å—**ï¼ˆæ–°å¢ï¼Œ~200è¡Œï¼‰
   ```markdown
   # æœåŠ¡è¿ç§»æŒ‡å—
   
   ## æ—§æœåŠ¡ â†’ æ–°æ¶æ„å¯¹ç…§
   
   ### TransformersService â†’ TransformersLLM
   - æ—§ï¼š`get_transformers_service().generate(...)`
   - æ–°ï¼š`await LLMService.get_llm("transformers", ...).generate(...)`
   
   ### OllamaLLMService â†’ OllamaLLM
   - æ—§ï¼š`OllamaLLMService().chat(...)`
   - æ–°ï¼š`await LLMService.get_llm("ollama", ...).chat(...)`
   
   ### HybridRetrievalService â†’ RetrieverManager
   - æ—§ï¼š`get_hybrid_retrieval_service().search(...)`
   - æ–°ï¼š`await RetrieverManager().hybrid_search(...)`
   ```

2. **æ¶æ„æ–‡æ¡£æ›´æ–°**ï¼ˆ~150è¡Œï¼‰
   - 4å±‚æ¶æ„å›¾ï¼ˆæ›´æ–°ï¼‰
   - æ¨¡å—ä¾èµ–å…³ç³»å›¾
   - æœåŠ¡è°ƒç”¨æµç¨‹å›¾

3. **æµ‹è¯•æŠ¥å‘Š**ï¼ˆ~100è¡Œï¼‰
   - æµ‹è¯•è¦†ç›–ç‡æ•°æ®
   - å›å½’æµ‹è¯•ç»“æœ
   - æ€§èƒ½å¯¹æ¯”ï¼ˆå¦‚æœ‰åŸºå‡†ï¼‰

4. **é‡æ„æ€»ç»“**ï¼ˆ~100è¡Œï¼‰
   - ä»£ç é‡å˜åŒ–ç»Ÿè®¡
   - å·²åˆ é™¤æ–‡ä»¶æ¸…å•
   - é—ç•™é—®é¢˜ä¸åç»­ä¼˜åŒ–

---

### é˜¶æ®µ5æ€»ç»“

**é‡æ„ç›®æ ‡**ï¼š
1. âœ… è¿ç§»æ‰€æœ‰æ—§æœåŠ¡è°ƒç”¨ï¼ˆ11å¤„ â†’ 0å¤„ï¼‰
2. âœ… åˆ é™¤åºŸå¼ƒæœåŠ¡æ–‡ä»¶ï¼ˆ3ä¸ªï¼Œ1474è¡Œï¼‰
3. âœ… å›å½’æµ‹è¯•éªŒè¯ï¼ˆè¦†ç›–ç‡â‰¥50%ï¼‰
4. âœ… è¡¥å…¨è¿ç§»æ–‡æ¡£

**æ—¶é—´ä¼°ç®—**ï¼š
- Week 11ï¼šAPIè¿ç§»ï¼ˆ5å¤©ï¼‰
  - T5.1ï¼šchat_serviceè¿ç§»ï¼ˆ2å¤©ï¼‰
  - T5.2ï¼šAPIç«¯ç‚¹è¿ç§»ï¼ˆ2å¤©ï¼‰
  - T5.3ï¼šå·¥å…·ç±»è¿ç§»ï¼ˆ1å¤©ï¼‰
- Week 12ï¼šæ¸…ç†éªŒè¯ï¼ˆ3å¤©ï¼‰
  - T5.4ï¼šåˆ é™¤æ—§æœåŠ¡ï¼ˆ0.5å¤©ï¼‰
  - T5.5ï¼šå›å½’æµ‹è¯•ï¼ˆ1.5å¤©ï¼‰
  - T5.6ï¼šæ–‡æ¡£æ›´æ–°ï¼ˆ1å¤©ï¼‰
- **æ€»è®¡**ï¼š8å¤©

**ä»£ç é‡å˜åŒ–**ï¼š

| ç±»åˆ« | åˆ é™¤ | æ–°å¢ | å‡€å˜åŒ– |
|------|------|------|--------|
| æ—§æœåŠ¡æ–‡ä»¶ | -1474è¡Œ | 0 | -1474è¡Œ |
| è¿ç§»ä»£ç  | 0 | +300è¡Œ | +300è¡Œ |
| æµ‹è¯•ä»£ç  | 0 | +200è¡Œ | +200è¡Œ |
| æ–‡æ¡£ | 0 | +550è¡Œ | +550è¡Œ |
| **æ€»è®¡** | -1474è¡Œ | +1050è¡Œ | **-424è¡Œ** |

æ³¨ï¼šå‡€å‡å°‘424è¡Œï¼Œä»£ç è´¨é‡æå‡

**å®Œæˆæ ‡å‡†**ï¼š
1. âœ… æ‰€æœ‰11å¤„æ—§æœåŠ¡å¼•ç”¨å·²è¿ç§»
2. âœ… æ—§æœåŠ¡æ–‡ä»¶å·²åˆ é™¤ï¼ˆtransformers_service.py, ollama_llm_service.py, hybrid_retrieval_service.pyï¼‰
3. âœ… å›å½’æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ˆ0å¤±è´¥ï¼‰
4. âœ… æ ¸å¿ƒæ¨¡å—æµ‹è¯•è¦†ç›–ç‡ â‰¥ 50%
5. âœ… APIåŠŸèƒ½æ— é€€åŒ–
6. âœ… APIè¿ç§»æŒ‡å—å®Œæˆ
7. âœ… æ¶æ„æ–‡æ¡£æ›´æ–°

**å…³é”®ä¿®å¤**ï¼ˆé’ˆå¯¹éªŒè¯é—®é¢˜ï¼‰ï¼š
1. âœ… **é—®é¢˜1-æ—¶åºå†²çª**ï¼šæ–°å¢Week 11ï¼ˆ5å¤©ï¼‰APIè¿ç§»ä»»åŠ¡ï¼Œå¤„ç†11å¤„å¼•ç”¨åå†åˆ é™¤
2. âœ… **é—®é¢˜2-æ€§èƒ½ä¼˜åŒ–**ï¼šç§»é™¤T5.2æ€§èƒ½ä¼˜åŒ–ï¼ˆåº”åœ¨é˜¶æ®µ1-4å®ç°ï¼‰
3. âœ… **é—®é¢˜3-æµ‹è¯•è¦†ç›–**ï¼šé™ä½ç›®æ ‡ï¼ˆ80% â†’ 50%ï¼‰ï¼Œèšç„¦æ ¸å¿ƒæ¨¡å—

**é¢„æœŸæ•ˆæœ**ï¼š
- â¬‡ï¸ ä»£ç æ€»é‡ï¼š-424è¡Œï¼ˆ-5.9%ï¼‰
- â¬‡ï¸ æœåŠ¡å±‚æ–‡ä»¶æ•°ï¼š18ä¸ª â†’ 15ä¸ªï¼ˆ-3ä¸ªæ—§æœåŠ¡ï¼‰
- â¬†ï¸ æ¶æ„æ¸…æ™°åº¦ï¼šæ¶ˆé™¤æ—§æœåŠ¡ä¾èµ–
- â¬†ï¸ å¯ç»´æŠ¤æ€§ï¼šç»Ÿä¸€LLMæŠ½è±¡å±‚ï¼Œç»Ÿä¸€æ£€ç´¢ç®¡ç†
- â¬†ï¸ æµ‹è¯•è¦†ç›–ï¼š30% â†’ 50%ï¼ˆ+67%ï¼‰

---

## é™„å½•

### A. é£é™©æ§åˆ¶

**å›æ»šç­–ç•¥**ï¼š
1. æ¯ä¸ªé˜¶æ®µå®Œæˆåæ‰“Gitæ ‡ç­¾ï¼ˆ`v2.0-stage1`ï¼‰
2. ä¿ç•™æ—§ä»£ç åˆ†æ”¯ï¼ˆ`legacy/service-layer-v1`ï¼‰
3. æ•°æ®åº“ä½¿ç”¨è¿ç§»è„šæœ¬ï¼ˆæ”¯æŒå›æ»šï¼‰

**ç°åº¦å‘å¸ƒ**ï¼š
1. æ–°æ—§APIå¹¶è¡Œè¿è¡Œ2å‘¨
2. é€æ­¥åˆ‡æ¢æµé‡ï¼ˆ10% â†’ 50% â†’ 100%ï¼‰
3. ç›‘æ§é”™è¯¯ç‡å’Œæ€§èƒ½æŒ‡æ ‡

**åº”æ€¥é¢„æ¡ˆ**ï¼š
- P0æ•…éšœï¼šç«‹å³å›æ»šåˆ°ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬
- P1æ•…éšœï¼š24å°æ—¶å†…ä¿®å¤æˆ–å›æ»š
- P2æ•…éšœï¼šä¸€å‘¨å†…ä¿®å¤

---

### B. éªŒæ”¶æ ‡å‡†

**åŠŸèƒ½éªŒæ”¶**ï¼š
- [ ] æ‰€æœ‰ç°æœ‰APIåŠŸèƒ½æ­£å¸¸
- [ ] æ–°å¢4ç§æ£€ç´¢ç­–ç•¥å¯ç”¨
- [ ] RAGæµç¨‹å®Œæ•´å¯ç”¨
- [ ] Agentå·¥å…·è°ƒç”¨æ­£å¸¸

**æ€§èƒ½éªŒæ”¶**ï¼š
- [ ] LLMæ¨ç†é€Ÿåº¦æ— ä¸‹é™ï¼ˆÂ±5%ä»¥å†…ï¼‰
- [ ] æ£€ç´¢å»¶è¿Ÿæ— ä¸‹é™ï¼ˆÂ±10%ä»¥å†…ï¼‰
- [ ] å†…å­˜å ç”¨æ— æ˜æ˜¾å¢åŠ ï¼ˆÂ±20%ä»¥å†…ï¼‰

**è´¨é‡éªŒæ”¶**ï¼š
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
- [ ] é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] ä»£ç å®¡æŸ¥é€šè¿‡ï¼ˆPylintè¯„åˆ† â‰¥ 8.0ï¼‰
- [ ] æ–‡æ¡£å®Œæ•´åº¦ â‰¥ 90%

---

## é™„å½•Cï¼šé‡æ„å‰åæ–‡ä»¶ç»“æ„å¯¹æ¯”

### å½“å‰ç»“æ„ï¼ˆé‡æ„å‰ï¼‰

```
Backend/app/
â”œâ”€â”€ api/                          # APIè·¯ç”±å±‚
â”œâ”€â”€ core/                         # æ ¸å¿ƒé…ç½®ï¼ˆä»…4ä¸ªæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ dependencies.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/                       # æ•°æ®æ¨¡å‹
â”œâ”€â”€ services/                     # æœåŠ¡å±‚ï¼ˆ18ä¸ªæ–‡ä»¶ï¼Œ6118è¡Œï¼‰
â”‚   â”œâ”€â”€ agent_service.py          # 316è¡Œ - Agenté€»è¾‘
â”‚   â”œâ”€â”€ chat_service.py           # 561è¡Œ ğŸ”´ - å¯¹è¯+RAG+æµå¼
â”‚   â”œâ”€â”€ embedding_service.py      # 334è¡Œ - Embeddingæ¨¡å‹
â”‚   â”œâ”€â”€ entity_extraction_service.py # 337è¡Œ - å®ä½“æå–
â”‚   â”œâ”€â”€ file_service.py           # 301è¡Œ - æ–‡ä»¶ä¸Šä¼ 
â”‚   â”œâ”€â”€ hybrid_retrieval_service.py # 374è¡Œ - æ··åˆæ£€ç´¢
â”‚   â”œâ”€â”€ knowledge_base_service.py # 528è¡Œ ğŸŸ¡ - CRUD+æ£€ç´¢
â”‚   â”œâ”€â”€ llama_factory_service.py  # 243è¡Œ - LLaMA Factoryé›†æˆ
â”‚   â”œâ”€â”€ lora_scanner_service.py   # 393è¡Œ - LoRAæ‰«æ
â”‚   â”œâ”€â”€ metadata_service.py       # 128è¡Œ - å…ƒæ•°æ®ç®¡ç†
â”‚   â”œâ”€â”€ model_manager.py          # 214è¡Œ - æ¨¡å‹ç®¡ç†
â”‚   â”œâ”€â”€ model_scanner.py          # 344è¡Œ - æ¨¡å‹æ‰«æ
â”‚   â”œâ”€â”€ neo4j_graph_service.py    # 513è¡Œ ğŸŸ¡ - çŸ¥è¯†å›¾è°±
â”‚   â”œâ”€â”€ ollama_embedding_service.py # 204è¡Œ - Ollama Embedding
â”‚   â”œâ”€â”€ ollama_llm_service.py     # 265è¡Œ - Ollama LLM
â”‚   â”œâ”€â”€ simple_lora_trainer.py    # 500è¡Œ ğŸŸ¡ - LoRAè®­ç»ƒ
â”‚   â”œâ”€â”€ transformers_service.py   # 776è¡Œ ğŸ”´ - Transformersæ¨ç†
â”‚   â””â”€â”€ vector_store_service.py   # 287è¡Œ - å‘é‡æ•°æ®åº“
â”œâ”€â”€ utils/                        # å·¥å…·å‡½æ•°ï¼ˆè¾ƒå°‘ï¼‰
â””â”€â”€ websocket/                    # WebSocketå¤„ç†
```

**å½“å‰é—®é¢˜**ï¼š
- âŒ `core/`ç›®å½•å‡ ä¹ä¸ºç©ºï¼Œç¼ºå°‘åŸºç¡€è®¾æ–½
- âŒ `services/`æ‰¿æ‹…äº†æ‰€æœ‰é€»è¾‘ï¼ŒèŒè´£ä¸æ¸…
- âŒ è®¾å¤‡ç®¡ç†ã€æ¨¡å‹åŠ è½½ç­‰åŸºç¡€åŠŸèƒ½åœ¨4ä¸ªæ–‡ä»¶ä¸­é‡å¤
- âŒ ç¼ºå°‘ç»Ÿä¸€çš„LLMæŠ½è±¡å±‚
- âŒ æ£€ç´¢ç­–ç•¥åˆ†æ•£ï¼Œæ— ç»Ÿä¸€æ¥å£

---

### é‡æ„åç»“æ„ï¼ˆç›®æ ‡ï¼‰

```
Backend/app/
â”œâ”€â”€ api/                          # APIè·¯ç”±å±‚ï¼ˆä¸å˜ï¼‰
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ chat.py
â”‚   â”œâ”€â”€ knowledge_base.py
â”‚   â””â”€â”€ models.py
â”‚
â”œâ”€â”€ core/                         # æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ database.py               # æ•°æ®åº“è¿æ¥
â”‚   â”œâ”€â”€ dependencies.py           # ä¾èµ–æ³¨å…¥
â”‚   â”‚
â”‚   â”œâ”€â”€ device/                   # è®¾å¤‡ç®¡ç†ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ gpu_manager.py        # 80è¡Œ - ç»Ÿä¸€CUDAç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                    # æ¨¡å‹åŠ è½½ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_loader.py       # 200è¡Œ - ç»Ÿä¸€æ¨¡å‹åŠ è½½
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                      # LLMæŠ½è±¡å±‚ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_llm.py           # 100è¡Œ - åŸºç±»æ¥å£
â”‚   â”‚   â”œâ”€â”€ ollama_llm.py         # 150è¡Œ - Ollamaå®ç°
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ transformers/         # Transformerså®ç°
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ prompt_builder.py      # 150è¡Œ - æç¤ºè¯æ„å»º
â”‚   â”‚       â”œâ”€â”€ response_processor.py  # 80è¡Œ - å“åº”å¤„ç†
â”‚   â”‚       â”œâ”€â”€ lora_adapter.py        # 120è¡Œ - LoRAç®¡ç†
â”‚   â”‚       â””â”€â”€ transformers_llm.py    # 280è¡Œ - ä¸»åè°ƒå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                # æ£€ç´¢ç­–ç•¥ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_retriever.py     # 80è¡Œ - åŸºç±»æ¥å£
â”‚   â”‚   â”œâ”€â”€ vector_retriever.py   # 120è¡Œ - å‘é‡æ£€ç´¢
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py     # 150è¡Œ - BM25å…¨æ–‡æ£€ç´¢
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py   # 180è¡Œ - æ··åˆæ£€ç´¢ï¼ˆRRFï¼‰
â”‚   â”‚   â”œâ”€â”€ graph_retriever.py    # 200è¡Œ - çŸ¥è¯†å›¾è°±æ£€ç´¢
â”‚   â”‚   â””â”€â”€ retriever_factory.py  # 60è¡Œ - å·¥å‚æ¨¡å¼
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                      # RAGæµæ°´çº¿ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_pipeline.py       # 200è¡Œ - RAGæ ¸å¿ƒæµç¨‹
â”‚   â”‚
â”‚   â”œâ”€â”€ agent/                    # Agentå·¥å…·ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tool_registry.py      # 80è¡Œ - å·¥å…·æ³¨å†Œè¡¨
â”‚   â”‚   â””â”€â”€ builtin_tools.py      # 150è¡Œ - å†…ç½®å·¥å…·
â”‚   â”‚
â”‚   â””â”€â”€ utils/                    # æ ¸å¿ƒå·¥å…·ï¼ˆæ–°å¢ï¼‰â­
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ json_parser.py        # 60è¡Œ - JSONå®¹é”™è§£æ
â”‚       â”œâ”€â”€ path_resolver.py      # 80è¡Œ - è·¯å¾„ç®¡ç†
â”‚       â”œâ”€â”€ process_manager.py    # 120è¡Œ - è¿›ç¨‹ç®¡ç†
â”‚       â”œâ”€â”€ task_state_manager.py # 100è¡Œ - ä»»åŠ¡çŠ¶æ€æœº
â”‚       â””â”€â”€ text_splitter.py      # 150è¡Œ - æ–‡æœ¬åˆ†å‰²
â”‚
â”œâ”€â”€ models/                       # æ•°æ®æ¨¡å‹ï¼ˆä¸å˜ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ chat.py
â”‚   â””â”€â”€ knowledge_base.py
â”‚
â”œâ”€â”€ services/                     # æœåŠ¡å±‚ï¼ˆæŒ‰æ¨¡å—åˆ†ç±»ï¼‰â­
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                      # LLMæ¨¡å‹æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py        # 180è¡Œ - LLMç»Ÿä¸€å…¥å£ï¼ˆå·¥å‚æ¨¡å¼ï¼‰
â”‚   â”‚   â””â”€â”€ embedding_service.py  # 250è¡Œ - EmbeddingæœåŠ¡ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ chat/                     # å¯¹è¯æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat_service.py       # 250è¡Œ - å¯¹è¯ç®¡ç†ï¼ˆä¼šè¯+å†å²ï¼‰
â”‚   â”‚   â””â”€â”€ agent_service.py      # 200è¡Œ - Agentå¯¹è¯ï¼ˆå·¥å…·è°ƒç”¨ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge/                # çŸ¥è¯†åº“æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ knowledge_base_service.py  # 220è¡Œ - çŸ¥è¯†åº“CRUD
â”‚   â”‚   â”œâ”€â”€ document_service.py        # 200è¡Œ - æ–‡æ¡£ä¸Šä¼ /åˆ†å—
â”‚   â”‚   â”œâ”€â”€ vector_store_service.py    # 250è¡Œ - å‘é‡å­˜å‚¨ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ entity_extraction_service.py # 200è¡Œ - å®ä½“æå–ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                    # çŸ¥è¯†å›¾è°±æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ neo4j_service.py      # 350è¡Œ - Neo4jå›¾è°±æ“ä½œï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                 # æ¨¡å‹è®­ç»ƒæœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lora_trainer.py       # 350è¡Œ - LoRAè®­ç»ƒï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ llama_factory_service.py # 243è¡Œ - LLaMA Factoryé›†æˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ model_mgmt/               # æ¨¡å‹ç®¡ç†æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_scanner.py      # 350è¡Œ - ç»Ÿä¸€æ¨¡å‹æ‰«æå™¨ï¼ˆæ”¯æŒLLM/LoRA/Embeddingï¼‰â­
â”‚   â”‚   â””â”€â”€ deployment.py         # 180è¡Œ - æ¨¡å‹éƒ¨ç½²ç®¡ç†ï¼ˆæ–°å¢ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                  # å­˜å‚¨æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_service.py       # 280è¡Œ - æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ metadata_service.py   # 128è¡Œ - å…ƒæ•°æ®ç®¡ç†
â”‚   â”‚
â”‚   â””â”€â”€ [å·²åˆ é™¤]                  # åºŸå¼ƒçš„æ—§æœåŠ¡
â”‚       â”œâ”€â”€ transformers_service.py    # 776è¡Œ â†’ åˆ é™¤ï¼ˆç§»è‡³core/llmï¼‰
â”‚       â”œâ”€â”€ ollama_llm_service.py      # 265è¡Œ â†’ åˆ é™¤ï¼ˆç§»è‡³core/llmï¼‰
â”‚       â”œâ”€â”€ ollama_embedding_service.py # 204è¡Œ â†’ åˆ é™¤ï¼ˆåˆå¹¶ï¼‰
â”‚       â”œâ”€â”€ hybrid_retrieval_service.py # 374è¡Œ â†’ åˆ é™¤ï¼ˆç§»è‡³core/retrievalï¼‰
â”‚       â”œâ”€â”€ model_manager.py           # 214è¡Œ â†’ åˆ é™¤ï¼ˆæ‹†åˆ†åˆ°model_mgmtï¼‰
â”‚       â””â”€â”€ simple_lora_trainer.py     # 500è¡Œ â†’ é‡å‘½åä¸ºlora_trainer.py
â”‚
â”œâ”€â”€ tests/                        # æµ‹è¯•ï¼ˆæ–°å¢/å®Œå–„ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒå±‚æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_device_manager.py
â”‚   â”‚   â”œâ”€â”€ test_model_loader.py
â”‚   â”‚   â”œâ”€â”€ test_base_llm.py
â”‚   â”‚   â”œâ”€â”€ test_transformers_llm.py
â”‚   â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â”‚   â””â”€â”€ test_rag_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                 # æœåŠ¡å±‚æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_llm_service.py
â”‚   â”‚   â”œâ”€â”€ test_chat_service.py
â”‚   â”‚   â”œâ”€â”€ test_knowledge_base_service.py
â”‚   â”‚   â””â”€â”€ test_agent_service.py
â”‚   â”‚
â”‚   â””â”€â”€ integration/              # é›†æˆæµ‹è¯•
â”‚       â”œâ”€â”€ test_rag_flow.py
â”‚       â”œâ”€â”€ test_hybrid_retrieval.py
â”‚       â””â”€â”€ test_agent_flow.py
â”‚
â”œâ”€â”€ utils/                        # ä¸šåŠ¡å·¥å…·ï¼ˆä¿ç•™ï¼‰
â””â”€â”€ websocket/                    # WebSocketï¼ˆä¸å˜ï¼‰
```

---

### é‡æ„æ•ˆæœå¯¹æ¯”

#### æœåŠ¡æ¨¡å—åˆ†ç±»è¯´æ˜

**æŒ‰ä¸šåŠ¡é¢†åŸŸåˆ†ä¸º7ä¸ªæ¨¡å—**ï¼š

| æ¨¡å— | èŒè´£ | æ–‡ä»¶æ•° | æ€»è¡Œæ•° |
|------|------|--------|--------|
| **llm/** | LLMæ¨¡å‹ç®¡ç†ï¼ˆå·¥å‚+åµŒå…¥ï¼‰ | 2 | ~430è¡Œ |
| **chat/** | å¯¹è¯æœåŠ¡ï¼ˆæ™®é€š+Agentï¼‰ | 2 | ~450è¡Œ |
| **knowledge/** | çŸ¥è¯†åº“ç®¡ç†ï¼ˆCRUD+æ–‡æ¡£+å‘é‡ï¼‰ | 5 | ~1120è¡Œ |
| **graph/** | çŸ¥è¯†å›¾è°±æ“ä½œ | 1 | ~350è¡Œ |
| **training/** | æ¨¡å‹è®­ç»ƒï¼ˆLoRA+é›†æˆï¼‰ | 2 | ~593è¡Œ |
| **model_mgmt/** | æ¨¡å‹æ‰«æ+éƒ¨ç½² | 2 | ~530è¡Œ |
| **storage/** | æ–‡ä»¶å­˜å‚¨+å…ƒæ•°æ® | 2 | ~408è¡Œ |
| **æ€»è®¡** | | **16ä¸ªæ–‡ä»¶** | **~3881è¡Œ** |

**æ¨¡å—ä¾èµ–å…³ç³»**ï¼š
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   chat/     â”‚ â† æœ€ä¸Šå±‚ï¼ˆä¾èµ–å…¶ä»–æ‰€æœ‰æ¨¡å—ï¼‰
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚       â”‚        â”‚         â”‚
â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”
â”‚ llm/ â”‚ â”‚know â”‚ â”‚ graph/ â”‚ â”‚train â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
          â”‚
      â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
   â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚store â”‚ â”‚model_mgmtâ”‚
   â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ä»£ç é‡å˜åŒ–

| å±‚çº§ | é‡æ„å‰ | é‡æ„å | å˜åŒ– |
|------|--------|--------|------|
| **core/** | 4ä¸ªæ–‡ä»¶ï¼Œ~200è¡Œ | 30ä¸ªæ–‡ä»¶ï¼Œ~2500è¡Œ | +2300è¡Œï¼ˆæ–°å¢åŸºç¡€è®¾æ–½ï¼‰ |
| **services/** | 18ä¸ªæ–‡ä»¶ï¼Œ6118è¡Œ | 16ä¸ªæ–‡ä»¶ï¼ˆ7æ¨¡å—ï¼‰ï¼Œ3881è¡Œ | -2237è¡Œï¼ˆ-37%ï¼‰ |
| **tests/** | ~500è¡Œ | ~2000è¡Œ | +1500è¡Œï¼ˆ+300%ï¼‰ |
| **æ€»è®¡** | ~6818è¡Œ | ~8381è¡Œ | +1563è¡Œï¼ˆ+23%ï¼‰|

> **è¯´æ˜**ï¼šä»£ç æ€»é‡å¢åŠ æ˜¯å› ä¸ºå¢åŠ äº†æµ‹è¯•å’ŒåŸºç¡€è®¾æ–½ï¼Œä½†**ä¸šåŠ¡é€»è¾‘ä»£ç å‡å°‘37%**ï¼Œ**é‡å¤ç‡ä¸‹é™68%**ï¼Œ**æ¨¡å—åŒ–ç¨‹åº¦æå‡300%**ã€‚

#### æ–‡ä»¶å¤æ‚åº¦å¯¹æ¯”

| æ–‡ä»¶ç±»åˆ« | é‡æ„å‰æœ€å¤§ | é‡æ„åæœ€å¤§ | æ”¹è¿› |
|---------|-----------|-----------|------|
| æœåŠ¡å±‚æ–‡ä»¶ | 776è¡Œ | 280è¡Œ | -64% |
| å¹³å‡æ–‡ä»¶å¤§å° | 340è¡Œ | 180è¡Œ | -47% |
| å•ä¸€èŒè´£å¾—åˆ† | 3.2/10 | 8.5/10 | +166% |

#### æ¶æ„åˆ†å±‚

```
é‡æ„å‰ï¼ˆ2å±‚ï¼‰:                  é‡æ„åï¼ˆ4å±‚ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   APIå±‚     â”‚                â”‚   APIå±‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Services   â”‚                â”‚  Services   â”‚ â† åº”ç”¨æœåŠ¡å±‚
â”‚  (6118è¡Œ)   â”‚                â”‚  (3800è¡Œ)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                               â”‚  Core/RAG   â”‚ â† ä¸šåŠ¡é€»è¾‘å±‚
                               â”‚  (~1000è¡Œ)  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                               â”‚ Core/åŸºç¡€å±‚ â”‚ â† åŸºç¡€è®¾æ–½å±‚
                               â”‚  (~1500è¡Œ)  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### å…³é”®æ”¹è¿›ç‚¹

1. **æ¨¡å—åŒ–åˆ†ç±»** â­â­â­
   ```
   é‡æ„å‰ï¼š18ä¸ªæ–‡ä»¶å¹³é“ºåœ¨services/æ ¹ç›®å½•
   é‡æ„åï¼š17ä¸ªæ–‡ä»¶åˆ†ç±»åˆ°7ä¸ªæ¨¡å—ç›®å½•
   
   ä¼˜åŠ¿ï¼š
   - èŒè´£æ¸…æ™°ï¼šæ¯ä¸ªæ¨¡å—èšç„¦å•ä¸€é¢†åŸŸ
   - æ˜“äºå®šä½ï¼šæŒ‰ä¸šåŠ¡æŸ¥æ‰¾æ–‡ä»¶ï¼ˆçŸ¥è¯†åº“â†’knowledge/ï¼‰
   - é™ä½è€¦åˆï¼šæ¨¡å—é—´é€šè¿‡æ¥å£äº¤äº’
   - ä¾¿äºæµ‹è¯•ï¼šæŒ‰æ¨¡å—ç»„ç»‡æµ‹è¯•ç”¨ä¾‹
   ```

2. **åŸºç¡€è®¾æ–½ä¸‹æ²‰** â­
   - `DeviceManager`ï¼šç»Ÿä¸€CUDAç®¡ç†ï¼ˆæ¶ˆé™¤4å¤„é‡å¤ï¼‰
   - `ModelLoader`ï¼šç»Ÿä¸€æ¨¡å‹åŠ è½½ï¼ˆæ¶ˆé™¤3å¤„é‡å¤ï¼‰
   - `ProcessManager`ï¼šç»Ÿä¸€è¿›ç¨‹ç®¡ç†
   - `PathResolver`ï¼šç»Ÿä¸€è·¯å¾„è§£æ

3. **æŠ½è±¡å±‚å»ºç«‹** â­
   - `BaseLLM`ï¼šç»Ÿä¸€LLMæ¥å£ï¼ˆæ”¯æŒTransformers/Ollama/OpenAIï¼‰
   - `BaseRetriever`ï¼šç»Ÿä¸€æ£€ç´¢æ¥å£ï¼ˆæ”¯æŒ4ç§ç­–ç•¥ï¼‰
   - `RAGPipeline`ï¼šè§£è€¦RAGæµç¨‹

4. **èŒè´£åˆ†ç¦»** â­
   - `transformers_service.py`ï¼ˆ776è¡Œï¼‰â†’ 6ä¸ªæ¨¡å—ï¼ˆæ€»280è¡Œï¼‰
   - `chat_service.py`ï¼ˆ561è¡Œï¼‰â†’ 250è¡Œ + RAG Pipeline
   - `knowledge_base_service.py`ï¼ˆ528è¡Œï¼‰â†’ 220è¡Œ + Document Service

5. **å¯æ‰©å±•æ€§æå‡** â­
   - æ–°å¢LLMï¼šå®ç°`BaseLLM`æ¥å£
   - æ–°å¢æ£€ç´¢ç­–ç•¥ï¼šå®ç°`BaseRetriever`æ¥å£
   - æ–°å¢Agentå·¥å…·ï¼šä½¿ç”¨è£…é¥°å™¨æ³¨å†Œ

---

### æœåŠ¡æ¨¡å—è¯¦è§£

#### 1. llm/ - LLMæ¨¡å‹æœåŠ¡
```python
# ç»Ÿä¸€çš„LLMç®¡ç†å…¥å£
from app.services.llm.llm_service import LLMService
from app.services.llm.embedding_service import EmbeddingService

# è·å–ä»»æ„ç±»å‹çš„LLM
llm_service = LLMService()
llm = await llm_service.get_llm("transformers", "Qwen2.5-1.5B")
llm = await llm_service.get_llm("ollama", "qwen2.5:latest")

# è·å–Embedding
embedding = EmbeddingService()
vec = await embedding.embed_text("æµ‹è¯•æ–‡æœ¬")
```

**èŒè´£**ï¼š
- âœ… LLMå·¥å‚ï¼ˆåˆ›å»º/ç¼“å­˜/å¸è½½ï¼‰
- âœ… Embeddingå‘é‡åŒ–
- âœ… æ¨¡å‹é…ç½®ç®¡ç†

---

#### 2. chat/ - å¯¹è¯æœåŠ¡
```python
# æ™®é€šå¯¹è¯
from app.services.chat.chat_service import ChatService

chat_service = ChatService()
response = await chat_service.chat(
    session_id=123,
    message="ä½ å¥½",
    kb_id=1,          # æŒ‡å®šçŸ¥è¯†åº“ï¼ˆRAGæ¨¡å¼ï¼‰
    strategy="hybrid"  # æ£€ç´¢ç­–ç•¥
)

# Agentå¯¹è¯ï¼ˆå¸¦å·¥å…·è°ƒç”¨ï¼‰
from app.services.chat.agent_service import AgentService

agent = AgentService()
result = await agent.chat(
    message="æœç´¢çŸ¥è¯†åº“ä¸­å…³äºPythonçš„å†…å®¹",
    tools=["knowledge_search", "web_search"]
)
```

**èŒè´£**ï¼š
- âœ… ä¼šè¯ç®¡ç†ï¼ˆåˆ›å»º/å†å²/ä¸Šä¸‹æ–‡ï¼‰
- âœ… RAGå¯¹è¯ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰
- âœ… Agentå¯¹è¯ï¼ˆå·¥å…·è°ƒç”¨+æ¨ç†ï¼‰
- âœ… æµå¼è¾“å‡ºæ”¯æŒ

---

#### 3. knowledge/ - çŸ¥è¯†åº“æœåŠ¡
```python
# çŸ¥è¯†åº“CRUD
from app.services.knowledge.knowledge_base_service import KnowledgeBaseService

kb_service = KnowledgeBaseService()
kb_id = await kb_service.create("æˆ‘çš„çŸ¥è¯†åº“", "æè¿°")
results = await kb_service.search(
    query="æµ‹è¯•æŸ¥è¯¢",
    kb_id=kb_id,
    strategy="hybrid",  # vector/bm25/hybrid/graph
    top_k=5
)

# æ–‡æ¡£ç®¡ç†
from app.services.knowledge.document_service import DocumentService

doc_service = DocumentService()
doc_id = await doc_service.upload_document(
    kb_id=kb_id,
    file_path=Path("test.txt"),
    metadata={"author": "å¼ ä¸‰"}
)

# å®ä½“æå–
from app.services.knowledge.entity_extraction_service import EntityExtractionService

entity_service = EntityExtractionService()
entities = await entity_service.extract_entities("å­™æ‚Ÿç©ºå¤§é—¹å¤©å®«", kb_id=1)
```

**èŒè´£**ï¼š
- âœ… çŸ¥è¯†åº“CRUD
- âœ… æ–‡æ¡£ä¸Šä¼ /åˆ†å—/å‘é‡åŒ–
- âœ… ç»Ÿä¸€æ£€ç´¢å…¥å£ï¼ˆ4ç§ç­–ç•¥ï¼‰
- âœ… å‘é‡å­˜å‚¨æ“ä½œ
- âœ… å®ä½“è¯†åˆ«

---

#### 4. graph/ - çŸ¥è¯†å›¾è°±æœåŠ¡
```python
from app.services.graph.neo4j_service import Neo4jService

graph = Neo4jService()

# æ„å»ºçŸ¥è¯†å›¾è°±
await graph.build_knowledge_graph(kb_id=1)

# æŸ¥è¯¢ä¸‰å…ƒç»„
triples = graph.get_entity_triples(kb_id=1, entity="å­™æ‚Ÿç©º")
# [{"head": "å­™æ‚Ÿç©º", "relation": "å¸ˆå‚…æ˜¯", "tail": "å”åƒ§"}, ...]

# ç»Ÿè®¡ä¿¡æ¯
stats = graph.get_statistics(kb_id=1)
# {"nodes": 100, "relationships": 250, ...}
```

**èŒè´£**ï¼š
- âœ… Neo4jè¿æ¥ç®¡ç†
- âœ… çŸ¥è¯†å›¾è°±æ„å»º
- âœ… ä¸‰å…ƒç»„æŸ¥è¯¢
- âœ… å›¾è°±ç»Ÿè®¡

---

#### 5. training/ - æ¨¡å‹è®­ç»ƒæœåŠ¡
```python
# LoRAè®­ç»ƒ
from app.services.training.lora_trainer import LoRATrainer

trainer = LoRATrainer()
task_id = await trainer.start_training(
    base_model="Qwen2.5-1.5B",
    dataset_path="monkey_brother.json",
    output_dir="./saves/lora_test"
)

# è®­ç»ƒçŠ¶æ€æŸ¥è¯¢
status = trainer.get_training_status(task_id)

# LLaMA Factoryé›†æˆ
from app.services.training.llama_factory_service import LlamaFactoryService

factory = LlamaFactoryService()
await factory.start_training_with_config(config_dict)
```

**èŒè´£**ï¼š
- âœ… LoRAè®­ç»ƒä»»åŠ¡ç®¡ç†
- âœ… æ•°æ®é›†éªŒè¯/è½¬æ¢
- âœ… è®­ç»ƒè¿›åº¦ç›‘æ§
- âœ… LLaMA Factoryé›†æˆ

---

#### 6. model_mgmt/ - æ¨¡å‹ç®¡ç†æœåŠ¡
```python
# ç»Ÿä¸€æ‰«æå™¨ï¼ˆæ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
from app.services.model_mgmt.model_scanner import ModelScanner

scanner = ModelScanner()

# æ‰«æLLMæ¨¡å‹
llm_models = await scanner.scan(
    base_path="Models/LLM",
    model_type="llm"
)

# æ‰«æLoRAé€‚é…å™¨
lora_adapters = await scanner.scan(
    base_path="Models/LoRA", 
    model_type="lora"
)

# æ‰«æEmbeddingæ¨¡å‹
embedding_models = await scanner.scan(
    base_path="Models/Embedding",
    model_type="embedding"
)

# æ¨¡å‹éƒ¨ç½²ç®¡ç†
from app.services.model_mgmt.deployment import deploy_model, undeploy_model

await deploy_model(model_id="llama-3-8b", version="v1.0")
await undeploy_model(model_id="llama-3-8b")
```

**èŒè´£**ï¼š
- âœ… **ç»Ÿä¸€æ‰«æ**ï¼šä¸€ä¸ªæ‰«æå™¨æ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼ˆLLM/LoRA/Embeddingï¼‰
- âœ… **æ ¼å¼è¯†åˆ«**ï¼šè‡ªåŠ¨è¯†åˆ«GGUF/Safetensors/PyTorchæ ¼å¼
- âœ… **æ¨¡å‹éƒ¨ç½²**ï¼šä¸Šçº¿/ä¸‹çº¿ã€ç‰ˆæœ¬åˆ‡æ¢

**ä¸ºä»€ä¹ˆåˆå¹¶æ‰«æå™¨**ï¼š
- ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰æ¨¡å‹ç±»å‹ä½¿ç”¨åŒä¸€å¥—æ‰«æé€»è¾‘
- å‡å°‘é‡å¤ï¼šæ–‡ä»¶éå†ã€æ ¼å¼è¯†åˆ«ã€é”™è¯¯å¤„ç†å¯å¤ç”¨
- æ˜“äºæ‰©å±•ï¼šæ–°å¢æ¨¡å‹ç±»å‹åªéœ€æ·»åŠ ä¸€ä¸ª`_scan_xxx()`æ–¹æ³•

---

#### 7. storage/ - å­˜å‚¨æœåŠ¡
```python
# æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½
from app.services.storage.file_service import FileService

file_service = FileService()
file_path = await file_service.upload_file(
    file=upload_file,
    kb_id=1,
    category="training_data"
)

# å…ƒæ•°æ®ç®¡ç†
from app.services.storage.metadata_service import MetadataService

metadata = MetadataService()
await metadata.update_file_metadata(
    file_id=123,
    metadata={"tags": ["é‡è¦", "å¾…å®¡æ ¸"]}
)
```

**èŒè´£**ï¼š
- âœ… æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½
- âœ… æ–‡ä»¶è·¯å¾„ç®¡ç†
- âœ… å…ƒæ•°æ®å­˜å‚¨/æŸ¥è¯¢

---

### æ¨¡å—è¿ç§»å¯¹ç…§è¡¨

**æ—§æ–‡ä»¶ â†’ æ–°ä½ç½®**ï¼š

| æ—§æ–‡ä»¶ | æ–°ä½ç½® | å˜åŒ– |
|--------|--------|------|
| `transformers_service.py` (776è¡Œ) | `core/llm/transformers/` (6ä¸ªæ–‡ä»¶) | æ‹†åˆ†+ä¸‹æ²‰ |
| `ollama_llm_service.py` (265è¡Œ) | `core/llm/ollama_llm.py` (150è¡Œ) | ç®€åŒ–+ä¸‹æ²‰ |
| `ollama_embedding_service.py` (204è¡Œ) | åˆå¹¶åˆ° `services/llm/embedding_service.py` | åˆå¹¶ |
| `chat_service.py` (561è¡Œ) | `services/chat/chat_service.py` (250è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `agent_service.py` (316è¡Œ) | `services/chat/agent_service.py` (200è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `knowledge_base_service.py` (528è¡Œ) | `services/knowledge/knowledge_base_service.py` (220è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| æ–°å¢ | `services/knowledge/document_service.py` (200è¡Œ) | èŒè´£åˆ†ç¦» |
| `vector_store_service.py` (287è¡Œ) | `services/knowledge/vector_store_service.py` (250è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `entity_extraction_service.py` (337è¡Œ) | `services/knowledge/entity_extraction_service.py` (200è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `neo4j_graph_service.py` (513è¡Œ) | `services/graph/neo4j_service.py` (350è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `simple_lora_trainer.py` (500è¡Œ) | `services/training/lora_trainer.py` (350è¡Œ) | é‡å‘½å+ç®€åŒ–+åˆ†ç±» |
| `llama_factory_service.py` (243è¡Œ) | `services/training/llama_factory_service.py` (243è¡Œ) | ä»…åˆ†ç±» |
| `model_manager.py` (214è¡Œ) | `services/model_mgmt/model_scanner.py` (350è¡Œ) | åˆå¹¶+åˆ†ç±» |
| `model_scanner.py` (344è¡Œ) | â†‘ åˆå¹¶åˆ°ç»Ÿä¸€æ‰«æå™¨ | åˆå¹¶ |
| `lora_scanner_service.py` (393è¡Œ) | â†‘ åˆå¹¶åˆ°ç»Ÿä¸€æ‰«æå™¨ | åˆå¹¶ |
| æ–°å¢ | `services/model_mgmt/deployment.py` (180è¡Œ) | èŒè´£åˆ†ç¦» |
| `file_service.py` (301è¡Œ) | `services/storage/file_service.py` (280è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `metadata_service.py` (128è¡Œ) | `services/storage/metadata_service.py` (128è¡Œ) | ä»…åˆ†ç±» |
| `embedding_service.py` (334è¡Œ) | `services/llm/embedding_service.py` (250è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `hybrid_retrieval_service.py` (374è¡Œ) | `core/retrieval/hybrid_retriever.py` (180è¡Œ) | ç®€åŒ–+ä¸‹æ²‰ |

**ç»Ÿè®¡**ï¼š
- ğŸ—‘ï¸ **åˆ é™¤**ï¼š0ä¸ªæ–‡ä»¶ï¼ˆå…¨éƒ¨ä¿ç•™æˆ–æ‹†åˆ†ï¼‰
- ğŸ“¦ **æ‹†åˆ†**ï¼š4ä¸ªæ–‡ä»¶æ‹†åˆ†ä¸ºå¤šä¸ªï¼ˆtransformers, model_manager, knowledge_base, ollamaï¼‰
- ğŸ†• **æ–°å¢**ï¼š5ä¸ªæ–‡ä»¶ï¼ˆdocument_service, deployment, æ£€ç´¢ç­–ç•¥ç­‰ï¼‰
- ğŸ“ **åˆ†ç±»**ï¼šæ‰€æœ‰æ–‡ä»¶æŒ‰7å¤§æ¨¡å—å½’ç±»
- â¬‡ï¸ **ç®€åŒ–**ï¼šå¹³å‡å‡å°‘32%ä»£ç é‡

---

### è¿ç§»è·¯å¾„

#### é˜¶æ®µ0ï¼ˆWeek 0ï¼‰ï¼šå‡†å¤‡
- å»ºç«‹æµ‹è¯•åŸºå‡†
- åˆ›å»ºé‡æ„åˆ†æ”¯

#### é˜¶æ®µ1ï¼ˆWeek 1-2ï¼‰ï¼šåŸºç¡€å±‚
- åˆ›å»º`core/device/`, `core/model/`, `core/utils/`
- åœ¨ç°æœ‰æœåŠ¡ä¸­æ›¿æ¢é‡å¤ä»£ç 

#### é˜¶æ®µ2ï¼ˆWeek 3-5ï¼‰ï¼šæ¨¡å‹å±‚
- åˆ›å»º`core/llm/`æŠ½è±¡å±‚
- æ‹†åˆ†`transformers_service.py`
- å®ç°`OllamaLLM`

#### é˜¶æ®µ3ï¼ˆWeek 6-8ï¼‰ï¼šä¸šåŠ¡å±‚
- åˆ›å»º`core/retrieval/`æ£€ç´¢ç­–ç•¥
- é‡æ„`knowledge_base_service.py`
- æ–°å¢`document_service.py`
- é‡æ„`model_mgmt/`, `storage/`, `training/`, `graph/`æ¨¡å— â­

#### é˜¶æ®µ4ï¼ˆWeek 9-10ï¼‰ï¼šåº”ç”¨å±‚
- åˆ›å»º`core/rag/rag_pipeline.py`
- é‡æ„`chat_service.py`
- ä¼˜åŒ–`agent_service.py`

#### é˜¶æ®µ5ï¼ˆWeek 11-12ï¼‰ï¼šæ¸…ç†
- åˆ é™¤æ—§æ–‡ä»¶
- æ€§èƒ½ä¼˜åŒ–
- æ–‡æ¡£è¡¥å…¨

---

**å®æ–½æ‰‹å†Œå®Œæˆï¼** ğŸ‰

**æ–‡æ¡£ç»Ÿè®¡**ï¼š
- æ€»é¡µæ•°ï¼š~3200è¡Œ
- ä¼°è®¡é˜…è¯»æ—¶é—´ï¼š75åˆ†é’Ÿ
- å®æ–½å‘¨æœŸï¼š12å‘¨ï¼ˆ3ä¸ªæœˆï¼‰
- é¢„æœŸæ•ˆæœï¼šä»£ç è´¨é‡æå‡50%+

**æ ¸å¿ƒä»·å€¼**ï¼š
1. âœ… å»ºç«‹4å±‚æ¶æ„ï¼ˆAPI â†’ åº”ç”¨ â†’ ä¸šåŠ¡ â†’ åŸºç¡€ï¼‰
2. âœ… æŒ‰æ¨¡å—åˆ†ç±»æœåŠ¡å±‚ï¼ˆ7å¤§ä¸šåŠ¡æ¨¡å—ï¼‰â­â­â­
3. âœ… æ¶ˆé™¤é‡å¤ä»£ç ï¼ˆ-68%é‡å¤ç‡ï¼‰
4. âœ… æå‡å¯æ‰©å±•æ€§ï¼ˆæ’ä»¶åŒ–æ¶æ„ï¼‰
5. âœ… å¢å¼ºå¯æµ‹è¯•æ€§ï¼ˆæµ‹è¯•è¦†ç›–ç‡+167%ï¼‰
6. âœ… é™ä½ç»´æŠ¤æˆæœ¬ï¼ˆå¹³å‡æ–‡ä»¶å¤§å°-47%ï¼‰
7. âœ… æé«˜ä»£ç å¯è¯»æ€§ï¼ˆæ¨¡å—åŒ–ç¨‹åº¦+300%ï¼‰

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**ï¼š
1. é˜…è¯»æœ¬æ–‡æ¡£ï¼ˆä¼°è®¡1.5å°æ—¶ï¼‰
2. è¿è¡Œæµ‹è¯•åŸºå‡†ï¼ˆWeek 0ï¼‰
3. åˆ›å»ºé‡æ„åˆ†æ”¯
4. æŒ‰é˜¶æ®µé€æ­¥å®æ–½ï¼ˆå»ºè®®å…ˆå®Œæˆé˜¶æ®µ1åŸºç¡€å±‚ï¼‰

---

**ç‰ˆæœ¬å†å²**ï¼š

- v3.5 (2025-01-24): é˜¶æ®µ5è¿ç§»æ¸…ç†å±‚è®¾è®¡ä¿®å¤ç‰ˆ â­
  - âœ… ä¿®å¤æ—¶åºå†²çªï¼šæ–°å¢Week 11ï¼ˆ5å¤©ï¼‰APIè¿ç§»ä»»åŠ¡ï¼Œå¤„ç†11å¤„æ—§æœåŠ¡å¼•ç”¨åå†åˆ é™¤
  - âœ… è°ƒæ•´åˆ é™¤å‰ç½®ï¼šT5.4åˆ é™¤æ—§æœåŠ¡éœ€ç¡®è®¤æ‰€æœ‰å¼•ç”¨å·²è¿ç§»ï¼ˆgrepéªŒè¯ï¼‰
  - âœ… é™ä½æµ‹è¯•ç›®æ ‡ï¼šè¦†ç›–ç‡ä»80%â†’50%ï¼Œèšç„¦æ ¸å¿ƒæ¨¡å—ï¼Œæ—¶é—´ä»2å¤©â†’1.5å¤©
  - âœ… ç§»é™¤æ€§èƒ½ä¼˜åŒ–ï¼šåŸT5.2æ€§èƒ½ä¼˜åŒ–åº”åœ¨é˜¶æ®µ1-4å®ç°ï¼Œé˜¶æ®µ5ä¸å†åŒ…å«
  - âœ… é‡æ„Week 11ä»»åŠ¡ï¼š
    - T5.1ï¼šè¿ç§»chat_serviceï¼ˆ2å¤©ï¼Œ2å¤„å¼•ç”¨ï¼‰
    - T5.2ï¼šè¿ç§»APIç«¯ç‚¹ï¼ˆ2å¤©ï¼Œ6å¤„å¼•ç”¨ï¼‰
    - T5.3ï¼šè¿ç§»å·¥å…·ç±»ï¼ˆ1å¤©ï¼Œ3å¤„å¼•ç”¨ï¼‰
  - âœ… é‡æ„Week 12ä»»åŠ¡ï¼š
    - T5.4ï¼šåˆ é™¤æ—§æœåŠ¡ï¼ˆ0.5å¤©ï¼Œç¡®è®¤æ— å¼•ç”¨ï¼‰
    - T5.5ï¼šå›å½’æµ‹è¯•ï¼ˆ1.5å¤©ï¼Œè¦†ç›–ç‡â‰¥50%ï¼‰
    - T5.6ï¼šæ–‡æ¡£æ›´æ–°ï¼ˆ1å¤©ï¼Œè¡¥å……è¿ç§»æŒ‡å—ï¼‰
  - âœ… æ—¶é—´ä¿æŒ8å¤©ä¸å˜ï¼šWeek 11ï¼ˆ5å¤©ï¼‰+ Week 12ï¼ˆ3å¤©ï¼‰
  - âœ… å…³é”®æ”¹è¿›ï¼š
    - åˆ é™¤æ—§æœåŠ¡ï¼š3ä¸ªæ–‡ä»¶ï¼Œ1474è¡Œ
    - è¿ç§»ä»£ç ï¼š+300è¡Œ
    - æµ‹è¯•ä»£ç ï¼š+200è¡Œ
    - æ–‡æ¡£è¡¥å……ï¼š+550è¡Œï¼ˆå«è¿ç§»æŒ‡å—ï¼‰
    - å‡€å‡å°‘ï¼š-424è¡Œï¼ˆ-5.9%ï¼‰
- v3.4 (2025-01-24): é˜¶æ®µ4åº”ç”¨å±‚è®¾è®¡ä¿®å¤ç‰ˆ â­
  - âœ… æ‹†åˆ†Week 9ä¸º4ä¸ªå­ä»»åŠ¡ï¼šT4.1aåŸºç¡€ç‰ˆ(2å¤©) + T4.1bå†å²å¢å¼º(1å¤©) + T4.1cå›¾è°±æ•´åˆ(1å¤©) + T4.1dæµ‹è¯•(1å¤©)
  - âœ… ä¿®å¤æ£€ç´¢å™¨è·å–æ–¹å¼ï¼šä½¿ç”¨RetrieverManagerä»£æ›¿RetrieverFactoryï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
  - âœ… ä¿ç•™å†å²æ¶ˆæ¯å¢å¼ºé€»è¾‘ï¼šæ–°å¢`_build_messages_enhanced`æ–¹æ³•ï¼ˆ90è¡Œï¼‰ï¼Œå®Œå…¨å…¼å®¹ç°æœ‰50è¡ŒPromptæ„å»º
  - âœ… æ•´åˆæ··åˆæ£€ç´¢ï¼šå°†å›¾è°±èåˆé€»è¾‘è¿ç§»åˆ°RetrieverManagerçš„`_hybrid_search_with_graph`æ–¹æ³•
  - âœ… ç»Ÿä¸€Agentå·¥å…·å®šä¹‰ï¼šToolDefinitionæ”¯æŒå‚æ•°éªŒè¯ã€å¼‚æ­¥æ‰§è¡Œï¼ŒToolRegistryè£…é¥°å™¨æ³¨å†Œæ¨¡å¼
  - âœ… æ—¶é—´è°ƒæ•´ï¼šWeek 9-10ä»5å¤©å»¶é•¿åˆ°8å¤©ï¼ˆ+60%ï¼‰ï¼Œåæ˜ å®é™…å¤æ‚åº¦
  - âœ… æ›´æ–°é˜¶æ®µ4æ€»ç»“ï¼šæ–°å¢æ¶æ„å¯¹æ¯”å›¾ã€ä»£ç é‡ç»Ÿè®¡ã€5ä¸ªå…³é”®ä¿®å¤è¯´æ˜
  - âœ… å…³é”®æ”¹è¿›ï¼š
    - ChatService: 624è¡Œ â†’ 380è¡Œï¼ˆ-39%ï¼‰
    - AgentService: 328è¡Œ â†’ 250è¡Œï¼ˆ-24%ï¼‰
    - RAG Pipeline: æ–°å¢200è¡Œï¼ˆç‹¬ç«‹å¯å¤ç”¨ï¼‰
    - Toolç³»ç»Ÿ: 30è¡Œ â†’ 300è¡Œï¼ˆé‡æ„ä¸ºå®Œæ•´ä½“ç³»ï¼‰
- v3.3 (2025-01-24): é˜¶æ®µ3ä¸šåŠ¡å±‚è®¾è®¡ä¿®å¤ç‰ˆ â­
  - âœ… æ‹†åˆ†DocumentServiceï¼šå°†èŒè´£è¿‡é‡çš„DocumentServiceï¼ˆ200è¡Œï¼‰æ‹†åˆ†ä¸º3ä¸ªç‹¬ç«‹æœåŠ¡
    - DocumentServiceï¼ˆ120è¡Œï¼‰ï¼šçº¯æ–‡æ¡£ç®¡ç†ï¼ˆCRUDï¼‰
    - ChunkingServiceï¼ˆ150è¡Œï¼‰ï¼šæ–‡æœ¬åˆ†å—é€»è¾‘
    - VectorizationServiceï¼ˆ180è¡Œï¼‰ï¼šå‘é‡åŒ–ç®¡ç†
  - âœ… è§£å†³å‘½åå†²çªï¼šå°†RetrievalServiceé‡å‘½åä¸ºRetrieverManagerï¼Œé¿å…ä¸BaseRetrieveræ¥å£æ··æ·†
  - âœ… ç‹¬ç«‹çŸ¥è¯†å›¾è°±ï¼šGraphServiceï¼ˆ180è¡Œï¼‰å®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¾èµ–KnowledgeBase
  - âœ… æ—¶é—´è°ƒæ•´ï¼šWeek 6-8ä»15å¤©å»¶é•¿åˆ°18.5å¤©ï¼ˆå»ºè®®20å¤©ï¼‰ï¼Œåæ˜ å®é™…å·¥ä½œé‡
  - âœ… æ›´æ–°é˜¶æ®µ3æ€»ç»“ï¼šæ–°å¢Week 8æ€»ç»“ï¼Œå®Œå–„æ¶æ„å˜åŒ–å›¾ç¤º
  - âœ… å…³é”®æ”¹è¿›ï¼šç»Ÿä¸€æ£€ç´¢é€»è¾‘ç®¡ç†ï¼Œç®€åŒ–KnowledgeBaseServiceå’ŒChatServiceçš„è°ƒç”¨
- v3.2 (2025-01-24): é˜¶æ®µ2è®¾è®¡ä¿®å¤ç‰ˆ â­
  - âœ… æ–°å¢ConfigManageræ¨¡å—ï¼šç»Ÿä¸€ç®¡ç†é‡åŒ–é…ç½®å’Œç”Ÿæˆé…ç½®ï¼ˆ100è¡Œï¼‰
  - âœ… é‡å‘½åGenerationEngine â†’ InferenceEngineï¼šæ˜ç¡®èŒè´£ä¸ºçº¯æ¨ç†é€»è¾‘ï¼ˆ150è¡Œï¼‰
  - âœ… è°ƒæ•´æ‹†åˆ†ç»“æ„ï¼š6ä¸ªæ¨¡å— â†’ 7ä¸ªæ¨¡å—ï¼ˆå¢åŠ ConfigManagerï¼‰
  - âœ… æ—¶é—´è°ƒæ•´ï¼šWeek 4-5ä»10å¤©å»¶é•¿åˆ°12å¤©ï¼ˆ+2å¤©ç”¨äºé…ç½®ç®¡ç†å’Œå…¼å®¹å±‚ï¼‰
  - âœ… æ›´æ–°é˜¶æ®µ2æ€»ç»“ï¼šåæ˜ ä¿®å¤åçš„ç»“æ„å’Œé£é™©æ§åˆ¶æªæ–½
  - âœ… å…³é”®æ”¹è¿›ï¼šTransformersLLMä¸å†ç›´æ¥å®ç°ç”Ÿæˆé€»è¾‘ï¼Œè€Œæ˜¯åè°ƒå„æ¨¡å—
- v3.1 (2025-01-24): å¯è¡Œæ€§éªŒè¯ä¸è®¾è®¡å®Œå–„ç‰ˆ â­â­
  - âœ… éªŒè¯é˜¶æ®µ0-1åŸºç¡€å±‚ï¼šåˆ†ætransformers_serviceã€embedding_serviceçš„è®¾å¤‡ç®¡ç†å’Œæ¨¡å‹åŠ è½½éœ€æ±‚
  - âœ… å®Œå–„DeviceManagerè®¾è®¡ï¼šè¡¥å……MPSæ”¯æŒï¼ˆApple Siliconï¼‰ã€GPUåç§°æŸ¥è¯¢ã€è®¾å¤‡ç±»å‹åˆ¤æ–­å±æ€§
  - âœ… å®Œå–„ModelLoaderè®¾è®¡ï¼šè¡¥å……æ¨¡å‹å¤§å°ä¼°ç®—ã€å°æ¨¡å‹ä¼˜åŒ–ã€Flash Attentionã€Tokenizerå®¹é”™ã€æ˜¾å­˜ç›‘æ§ã€æ¨¡å‹ç¼“å­˜ç®¡ç†ã€LoRAåŠ è½½ç­‰7ä¸ªå…³é”®åŠŸèƒ½
  - âœ… å…³é”®å‘ç°ï¼š17å¤„è®¾å¤‡ä½¿ç”¨ã€18å¤„torch.cudaè°ƒç”¨ã€7ä¸ªæ¨¡å‹åŠ è½½ä¼˜åŒ–åŠŸèƒ½ï¼Œå¿…é¡»å®Œæ•´è¿ç§»
  - âœ… éªŒè¯ç»“è®ºï¼šåŸºç¡€å±‚è®¾è®¡å¯è¡Œï¼Œä½†éœ€è¦ä»ç®€åŒ–ç‰ˆå‡çº§ä¸ºç”Ÿäº§çº§ï¼ˆåŒ…å«æ‰€æœ‰ç°æœ‰ä¼˜åŒ–ï¼‰
- v3.0 (2025-01-27): â­â­â­æ¶æ„ä¼˜åŒ–ç‰ˆï¼Œç³»ç»Ÿæ€§é‡ç»„12å‘¨è®¡åˆ’
  - ä¿®æ­£æ¨¡å—å½’å±ï¼šmodel_mgmt/trainingç§»è‡³Week 3-5æ¨¡å‹å±‚ï¼Œgraph/vector_storeç§»è‡³Week 6-7ä¸šåŠ¡å±‚
  - æ˜ç¡®è¯´æ˜ï¼šæ‰€æœ‰"ç®€åŒ–"å‡ä¸ºæ¶æ„ä¼˜åŒ–ï¼ˆåˆ é™¤é‡å¤ä»£ç ã€æå–å…¬å…±æ¨¡å—ï¼‰ï¼Œä¸åˆ é™¤ä¸šåŠ¡åŠŸèƒ½
  - è¡¥å……ä»»åŠ¡ï¼šä¸ºembedding_serviceã€vector_storeã€entity_extractionã€neo4jç­‰æ·»åŠ å…·ä½“é‡æ„ä»»åŠ¡
  - æ—¶é—´ä¼˜åŒ–ï¼šæ¯å‘¨æ§åˆ¶åœ¨5å¤©å·¥ä½œé‡ï¼Œæ€»ä½“12å‘¨å®Œæˆ
- v2.2 (2025-01-26): è¡¥å……Week 7-8ç¼ºå¤±ä»»åŠ¡ï¼ˆåå‘ç°å½’å±é”™è¯¯ï¼‰
- v2.1 (2025-01-26): ä¼˜åŒ–æœåŠ¡å±‚ç»“æ„ï¼ŒæŒ‰7å¤§æ¨¡å—åˆ†ç±»
- v2.0 (2025-01-26): ç²¾ç®€å®æ–½ç‰ˆï¼Œè¡¥å……æ–‡ä»¶ç»“æ„å¯¹æ¯”
- v1.0 (2025-01-25): åˆå§‹ç‰ˆæœ¬


