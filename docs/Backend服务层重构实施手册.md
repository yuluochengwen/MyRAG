# BackendæœåŠ¡å±‚é‡æ„å®æ–½æ‰‹å†Œ

> **ç‰ˆæœ¬**: v3.0 æ¶æ„ä¼˜åŒ–ç‰ˆ  
> **åˆ›å»ºæ—¶é—´**: 2025-01-26  
> **é€‚ç”¨é¡¹ç›®**: MyRAGçŸ¥è¯†åº“ç³»ç»Ÿ  
> **é‡æ„å‘¨æœŸ**: 12å‘¨  
> âš ï¸ **é‡è¦è¯´æ˜**: æ‰€æœ‰"ç®€åŒ–"å‡ä¸ºæ¶æ„ä¼˜åŒ–ï¼Œ**ä¸åˆ é™¤ä»»ä½•ä¸šåŠ¡åŠŸèƒ½**ï¼Œä»…é€šè¿‡æ¶ˆé™¤é‡å¤ä»£ç ã€åˆ†å±‚è§£è€¦å®ç°ä»£ç å‡å°‘  

---

## ğŸ“‹ å¿«é€Ÿå¯¼èˆª

| ç« èŠ‚ | å†…å®¹ | æ—¶é—´ |
|------|------|------|
| [ä¸€ã€ç°çŠ¶è¯„ä¼°](#ä¸€ç°çŠ¶è¯„ä¼°) | ä»£ç ç»Ÿè®¡ã€é—®é¢˜è¯†åˆ«ã€ä¼˜å…ˆçº§ | - |
| [äºŒã€é˜¶æ®µ0ï¼šå‡†å¤‡](#äºŒé˜¶æ®µ0å‡†å¤‡week-0) | æµ‹è¯•åŸºå‡†ã€ç¯å¢ƒå‡†å¤‡ | Week 0 |
| [ä¸‰ã€é˜¶æ®µ1ï¼šåŸºç¡€å±‚](#ä¸‰é˜¶æ®µ1åŸºç¡€å±‚week-1-2) | DeviceManager, ModelLoader | Week 1-2 |
| [å››ã€é˜¶æ®µ2ï¼šæ¨¡å‹å±‚](#å››é˜¶æ®µ2æ¨¡å‹å±‚week-3-5) | LLMæŠ½è±¡ã€transformersæ‹†åˆ† | Week 3-5 |
| [äº”ã€é˜¶æ®µ3ï¼šä¸šåŠ¡å±‚](#äº”é˜¶æ®µ3ä¸šåŠ¡å±‚week-6-8) | æ£€ç´¢ç­–ç•¥ã€çŸ¥è¯†åº“é‡æ„ | Week 6-8 |
| [å…­ã€é˜¶æ®µ4ï¼šåº”ç”¨å±‚](#å…­é˜¶æ®µ4åº”ç”¨å±‚week-9-10) | RAG Pipelineã€Agent | Week 9-10 |
| [ä¸ƒã€é˜¶æ®µ5ï¼šæ¸…ç†](#ä¸ƒé˜¶æ®µ5æ¸…ç†week-11-12) | åˆ é™¤æ—§ä»£ç ã€ä¼˜åŒ– | Week 11-12 |

---

## ä¸€ã€ç°çŠ¶è¯„ä¼°

### 1.1 ä»£ç è§„æ¨¡

**18ä¸ªæœåŠ¡æ–‡ä»¶ï¼Œæ€»è®¡6118è¡Œ**ï¼ˆå®é™…ç»Ÿè®¡ï¼‰

| é£é™©çº§åˆ« | æ–‡ä»¶ | è¡Œæ•° | æ ¸å¿ƒé—®é¢˜ |
|---------|------|------|---------|
| ğŸ”´ æé«˜ | transformers_service.py | 776 | 7ä¸ªèŒè´£æ··åˆï¼ˆè®¾å¤‡ç®¡ç†/æ¨¡å‹åŠ è½½/LoRA/æç¤ºè¯/ç”Ÿæˆï¼‰ |
| ğŸ”´ é«˜ | chat_service.py | 561 | 5ä¸ªèŒè´£æ··åˆï¼ˆä¼šè¯ç®¡ç†/RAG/æµå¼è¾“å‡º/å†å²ï¼‰ |
| ğŸŸ¡ ä¸­ | knowledge_base_service.py | 528 | CRUD+æ£€ç´¢+å‘é‡åŒ– |
| ğŸŸ¡ ä¸­ | neo4j_graph_service.py | 513 | å›¾è°±æ„å»º+æŸ¥è¯¢ |
| ğŸŸ¡ ä¸­ | simple_lora_trainer.py | 500 | æ•°æ®é›†å¤„ç†+è®­ç»ƒç®¡ç† |
| ğŸŸ¢ ä½ | å…¶ä»–13ä¸ªæ–‡ä»¶ | 3240 | ç›¸å¯¹å¯æ¥å— |

### 1.2 æ ¸å¿ƒé—®é¢˜ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

| ä¼˜å…ˆçº§ | é—®é¢˜ | å½±å“æ–‡ä»¶ | è§£å†³æ–¹æ¡ˆ | é˜¶æ®µ |
|--------|------|---------|---------|------|
| **P0** | è®¾å¤‡ç®¡ç†é‡å¤4æ¬¡ | transformers, embedding, lora_trainer, ollama | åˆ›å»ºDeviceManager | 1 |
| **P0** | æ¨¡å‹åŠ è½½é‡å¤3æ¬¡ | transformers, embedding, lora_trainer | åˆ›å»ºModelLoader | 1 |
| **P1** | transformers_serviceè¿‡å¤§ | transformers_service (835è¡Œ) | æ‹†åˆ†ä¸º6ä¸ªæ¨¡å— | 2 |
| **P1** | ç¼ºå°‘LLMæŠ½è±¡å±‚ | transformers, ollama | å®šä¹‰BaseLLMæ¥å£ | 2 |
| **P2** | chat_serviceèŒè´£ä¸æ¸… | chat_service (624è¡Œ) | æå–RAG Pipeline | 3 |
| **P2** | æ£€ç´¢ç­–ç•¥åˆ†æ•£ | 3ä¸ªæ–‡ä»¶ | ç­–ç•¥æ¨¡å¼é‡æ„ | 3 |
| **P3** | å·¥å…·ç±»ç¼ºå¤± | entity_extractionç­‰ | åˆ›å»ºJSONParserç­‰ | 4 |

### 1.3 é‡æ„ç›®æ ‡

**æ¶æ„ç›®æ ‡**ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰ï¼š
- â¬‡ï¸ ä»£ç é‡å¤ç‡: >25% â†’ <10% (-60%) - é€šè¿‡æå–å…¬å…±æ¨¡å—
- â¬‡ï¸ æœ€å¤§æ–‡ä»¶: 835è¡Œ â†’ 280è¡Œ (-66%) - é€šè¿‡èŒè´£åˆ†ç¦»
- â¬†ï¸ æ¨¡å—åŒ–ç¨‹åº¦: 18ä¸ªå¹³é“ºæ–‡ä»¶ â†’ 7å¤§æ¨¡å—åˆ†ç±»
- â¬†ï¸ æµ‹è¯•è¦†ç›–ç‡: 30% â†’ 80% (+167%)
- â¬†ï¸ å¯æ‰©å±•æ€§: æ’ä»¶åŒ–æ¶æ„ï¼Œæ˜“äºæ·»åŠ æ–°æ¨¡å‹/æ£€ç´¢ç­–ç•¥

**ä»£ç é‡å˜åŒ–**ï¼ˆè‡ªç„¶ç»“æœï¼‰ï¼š
- serviceså±‚: 6118è¡Œ â†’ 3881è¡Œ (-37%ï¼Œå› ä¸ºé‡å¤ä»£ç ç§»è‡³coreå±‚ï¼‰
- coreå±‚: 200è¡Œ â†’ 2500è¡Œ (+2300è¡Œï¼Œæ–°å¢åŸºç¡€è®¾æ–½ï¼‰
- å‡€æ•ˆæœ: ä»£ç æ€»é‡ç•¥å¢ï¼Œä½†**è´¨é‡å¤§å¹…æå‡**

---

## äºŒã€é˜¶æ®µ0ï¼šå‡†å¤‡ (Week 0)

### ç›®æ ‡
âœ… å»ºç«‹æµ‹è¯•åŸºå‡†  
âœ… å‡†å¤‡é‡æ„ç¯å¢ƒ  
âœ… è¯†åˆ«å›æ»šç‚¹

### ä»»åŠ¡æ¸…å•

**T0.1 æµ‹è¯•åŸºå‡†å»ºç«‹**
```bash
# 1. è¿è¡Œç°æœ‰æµ‹è¯•
cd Backend
pytest app/tests/ --cov=app/services --cov-report=html

# 2. è®°å½•æ€§èƒ½åŸºå‡†
python benchmark/llm_latency.py  # è®°å½•æ¨ç†é€Ÿåº¦
python benchmark/memory_usage.py  # è®°å½•å†…å­˜å ç”¨

# 3. ä¿å­˜ç»“æœ
cp htmlcov docs/test_baseline_before.html
```

**T0.2 ä»£ç åº¦é‡**
```bash
# è®¡ç®—åœˆå¤æ‚åº¦
radon cc app/services -a > docs/complexity_before.txt

# ä»£ç é‡å¤åˆ†æ
pylint app/services --disable=all --enable=duplicate-code > docs/duplication_before.txt
```

**T0.3 ç¯å¢ƒå‡†å¤‡**
```bash
# åˆ›å»ºé‡æ„åˆ†æ”¯
git checkout -b refactor/service-layer-v2

# é…ç½®è‡ªåŠ¨åŒ–æµ‹è¯•
# æ¯æ¬¡æäº¤è‡ªåŠ¨è¿è¡Œæµ‹è¯•
```

### äº¤ä»˜ç‰©
- âœ… `docs/test_baseline.md` - æµ‹è¯•åŸºå‡†æŠ¥å‘Š
- âœ… `docs/code_metrics.md` - ä»£ç åº¦é‡æŠ¥å‘Š
- âœ… Featureåˆ†æ”¯åˆ›å»ºå®Œæˆ

---

## ä¸‰ã€é˜¶æ®µ1ï¼šåŸºç¡€å±‚ (Week 1-2)

### ç›®æ ‡
ğŸ¯ å»ºç«‹åŸºç¡€è®¾æ–½  
ğŸ¯ æ¶ˆé™¤P0çº§åˆ«çš„é‡å¤ä»£ç   
ğŸ¯ ä¸ºåç»­é‡æ„é“ºè·¯

### Week 1: è®¾å¤‡ç®¡ç†ä¸å·¥å…·ç±»

#### T1.1 åˆ›å»ºDeviceManagerï¼ˆ2å¤©ï¼‰

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# Backend/app/core/device/gpu_manager.py
from dataclasses import dataclass
import torch
from transformers import BitsAndBytesConfig

@dataclass
class DeviceInfo:
    device_type: str  # cuda/cpu
    device_name: str
    total_memory_gb: float
    allocated_memory_gb: float
    reserved_memory_gb: float

class DeviceManager:
    """ç»Ÿä¸€çš„CUDAè®¾å¤‡ç®¡ç†"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def get_device_info(self) -> DeviceInfo:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        if self.device == "cpu":
            return DeviceInfo("cpu", "CPU", 0, 0, 0)
        return DeviceInfo(
            device_type="cuda",
            device_name=torch.cuda.get_device_name(0),
            total_memory_gb=torch.cuda.get_device_properties(0).total_memory / 1024**3,
            allocated_memory_gb=torch.cuda.memory_allocated(0) / 1024**3,
            reserved_memory_gb=torch.cuda.memory_reserved(0) / 1024**3
        )
    
    def get_quantization_config(self) -> BitsAndBytesConfig:
        """è·å–INT4é‡åŒ–é…ç½®"""
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
    
    def clear_cache(self):
        """æ¸…ç†æ˜¾å­˜ç¼“å­˜"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
```

**æµ‹è¯•ä»£ç **ï¼š
```python
# Backend/app/tests/test_device_manager.py
def test_device_manager_init():
    dm = DeviceManager()
    assert dm.device in ["cuda", "cpu"]

def test_get_device_info():
    dm = DeviceManager()
    info = dm.get_device_info()
    assert info.device_type in ["cuda", "cpu"]
    assert info.total_memory_gb >= 0
```

**æ›¿æ¢æ—§ä»£ç **ï¼š
```python
# åœ¨transformers_service.py, embedding_service.pyç­‰4ä¸ªæ–‡ä»¶ä¸­æ›¿æ¢ï¼š
# æ—§ä»£ç ï¼š
# self.device = "cuda" if torch.cuda.is_available() else "cpu"

# æ–°ä»£ç ï¼š
from app.core.device.gpu_manager import DeviceManager
self.device_manager = DeviceManager()
self.device = self.device_manager.device
```

---

#### T1.2 åˆ›å»ºåŸºç¡€å·¥å…·ç±»ï¼ˆ3å¤©ï¼‰

**JSONParser - ç»Ÿä¸€JSONè§£æå®¹é”™**ï¼š
```python
# Backend/app/core/utils/json_parser.py
import json
import re
from typing import Optional, Dict, Any

class JSONParser:
    """ç»Ÿä¸€çš„JSONè§£æå®¹é”™å·¥å…·"""
    
    @staticmethod
    def extract_json(text: str, fallback: Optional[Dict] = None) -> Dict[str, Any]:
        """
        3å±‚é™çº§ç­–ç•¥æå–JSON
        
        Args:
            text: å¯èƒ½åŒ…å«JSONçš„æ–‡æœ¬
            fallback: è§£æå¤±è´¥æ—¶è¿”å›çš„é»˜è®¤å€¼
            
        Returns:
            è§£æåçš„å­—å…¸
        """
        # ç¬¬1å±‚ï¼šç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass
        
        # ç¬¬2å±‚ï¼šæå–ä»£ç å—
        try:
            if '```json' in text:
                start = text.find('```json') + 7
                end = text.find('```', start)
                json_str = text[start:end].strip()
            elif '```' in text:
                start = text.find('```') + 3
                end = text.find('```', start)
                json_str = text[start:end].strip()
            else:
                raise ValueError("No code block found")
            
            return json.loads(json_str)
        except:
            pass
        
        # ç¬¬3å±‚ï¼šæŸ¥æ‰¾{}
        try:
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > 0:
                json_str = text[start:end]
                return json.loads(json_str)
        except:
            pass
        
        # å…¨éƒ¨å¤±è´¥ï¼Œè¿”å›fallback
        return fallback or {}
```

**PathResolver - ç»Ÿä¸€è·¯å¾„ç®¡ç†**ï¼š
```python
# Backend/app/core/utils/path_resolver.py
from pathlib import Path
from app.core.config import settings

class PathResolver:
    """ç»Ÿä¸€çš„è·¯å¾„è§£ææœåŠ¡"""
    
    def __init__(self):
        self.base_dir = Path(settings.file.upload_dir).parent
    
    def get_model_path(self, model_type: str, model_name: str) -> Path:
        """
        è·å–æ¨¡å‹è·¯å¾„
        
        Args:
            model_type: llm / embedding / lora
            model_name: æ¨¡å‹åç§°
            
        Returns:
            å®Œæ•´è·¯å¾„
        """
        type_map = {
            "llm": "Models/LLM",
            "embedding": "Models/Embedding",
            "lora": "Models/LoRA"
        }
        return self.base_dir / type_map[model_type] / model_name
    
    def get_kb_path(self, kb_id: int) -> Path:
        """è·å–çŸ¥è¯†åº“è·¯å¾„"""
        return self.base_dir / "KnowledgeBase" / f"kb_{kb_id}"
    
    def get_training_data_path(self, filename: str) -> Path:
        """è·å–è®­ç»ƒæ•°æ®è·¯å¾„"""
        return self.base_dir / "TrainingData" / filename
```

**ProcessManager - è¿›ç¨‹ç®¡ç†**ï¼š
```python
# Backend/app/core/utils/process_manager.py
import subprocess
import psutil
import time
from typing import List, Optional, Dict, Any

class ProcessManager:
    """ç»Ÿä¸€çš„è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    
    def start_process(
        self,
        cmd: List[str],
        wait_time: int = 5,
        log_file: Optional[str] = None
    ) -> int:
        """
        å¯åŠ¨è¿›ç¨‹å¹¶ç­‰å¾…å°±ç»ª
        
        Args:
            cmd: å‘½ä»¤åˆ—è¡¨
            wait_time: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            
        Returns:
            è¿›ç¨‹PID
        """
        # Windowsç‰¹æ®Šå¤„ç†
        creation_flags = (
            subprocess.CREATE_NEW_PROCESS_GROUP 
            if hasattr(subprocess, 'CREATE_NEW_PROCESS_GROUP') 
            else 0
        )
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
        log_handle = open(log_file, 'w') if log_file else subprocess.PIPE
        
        process = subprocess.Popen(
            cmd,
            stdout=log_handle,
            stderr=subprocess.STDOUT,
            creationflags=creation_flags
        )
        
        # ç­‰å¾…è¿›ç¨‹å¯åŠ¨
        time.sleep(wait_time)
        
        # éªŒè¯è¿›ç¨‹å­˜åœ¨
        if not psutil.pid_exists(process.pid):
            raise RuntimeError(f"è¿›ç¨‹å¯åŠ¨å¤±è´¥: PID {process.pid}")
        
        return process.pid
    
    def stop_process(self, pid: int, timeout: int = 10) -> bool:
        """
        ä¼˜é›…åœæ­¢è¿›ç¨‹ï¼ˆterminate â†’ wait â†’ killï¼‰
        
        Args:
            pid: è¿›ç¨‹ID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸåœæ­¢
        """
        try:
            process = psutil.Process(pid)
            
            # 1. å°è¯•ä¼˜é›…ç»ˆæ­¢
            process.terminate()
            
            # 2. ç­‰å¾…è¿›ç¨‹ç»“æŸ
            try:
                process.wait(timeout=timeout)
                return True
            except psutil.TimeoutExpired:
                # 3. è¶…æ—¶åå¼ºåˆ¶æ€æ­»
                process.kill()
                return True
                
        except psutil.NoSuchProcess:
            return True  # è¿›ç¨‹å·²ä¸å­˜åœ¨
        except Exception as e:
            return False
    
    def get_process_status(self, pid: int) -> Dict[str, Any]:
        """è·å–è¿›ç¨‹çŠ¶æ€"""
        try:
            process = psutil.Process(pid)
            return {
                "pid": pid,
                "status": process.status(),
                "running": process.is_running(),
                "cpu_percent": process.cpu_percent(),
                "memory_mb": process.memory_info().rss / 1024 / 1024
            }
        except psutil.NoSuchProcess:
            return {"pid": pid, "running": False}
```

**TaskStateManager - ä»»åŠ¡çŠ¶æ€ç®¡ç†**ï¼š
```python
# Backend/app/core/utils/task_state_manager.py
from enum import Enum
from typing import Dict, List, Optional

class TaskState(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class TaskStateManager:
    """ç»Ÿä¸€çš„ä»»åŠ¡çŠ¶æ€ç®¡ç†ï¼ˆå¸¦çŠ¶æ€æœºéªŒè¯ï¼‰"""
    
    # åˆæ³•çš„çŠ¶æ€è½¬æ¢
    TRANSITIONS = {
        TaskState.PENDING: [TaskState.RUNNING, TaskState.FAILED],
        TaskState.RUNNING: [TaskState.COMPLETED, TaskState.FAILED],
        TaskState.COMPLETED: [],  # ç»ˆæ€
        TaskState.FAILED: []       # ç»ˆæ€
    }
    
    def can_transition(
        self,
        from_state: TaskState,
        to_state: TaskState
    ) -> bool:
        """éªŒè¯çŠ¶æ€è½¬æ¢æ˜¯å¦åˆæ³•"""
        return to_state in self.TRANSITIONS.get(from_state, [])
    
    def update_task_status(
        self,
        db_connection,
        table_name: str,
        task_id: int,
        new_state: TaskState,
        **extra_fields
    ) -> bool:
        """
        æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆè‡ªåŠ¨éªŒè¯åˆæ³•æ€§ï¼‰
        
        Args:
            db_connection: æ•°æ®åº“è¿æ¥
            table_name: è¡¨å
            task_id: ä»»åŠ¡ID
            new_state: æ–°çŠ¶æ€
            **extra_fields: é¢å¤–å­—æ®µï¼ˆprogress, messageç­‰ï¼‰
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        # 1. è·å–å½“å‰çŠ¶æ€
        with db_connection.cursor() as cursor:
            cursor.execute(
                f"SELECT status FROM {table_name} WHERE id = %s",
                (task_id,)
            )
            result = cursor.fetchone()
            if not result:
                return False
            
            current_state = TaskState(result['status'])
        
        # 2. éªŒè¯è½¬æ¢åˆæ³•æ€§
        if not self.can_transition(current_state, new_state):
            raise ValueError(
                f"éæ³•çŠ¶æ€è½¬æ¢: {current_state.value} â†’ {new_state.value}"
            )
        
        # 3. æ‰§è¡Œæ›´æ–°
        update_fields = ["status = %s"]
        params = [new_state.value]
        
        for field, value in extra_fields.items():
            update_fields.append(f"{field} = %s")
            params.append(value)
        
        # æ·»åŠ æ—¶é—´æˆ³
        if new_state == TaskState.COMPLETED:
            update_fields.append("completed_at = NOW()")
        
        params.append(task_id)
        
        with db_connection.cursor() as cursor:
            sql = f"UPDATE {table_name} SET {', '.join(update_fields)} WHERE id = %s"
            cursor.execute(sql, params)
            db_connection.commit()
        
        return True
```

---

### Week 2: æ¨¡å‹åŠ è½½å™¨

#### T1.3 åˆ›å»ºModelLoaderï¼ˆ3å¤©ï¼‰

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# Backend/app/core/model/model_loader.py
from pathlib import Path
from typing import Optional, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from app.core.device.gpu_manager import DeviceManager

class ModelLoader:
    """ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½å™¨ï¼ˆæ”¯æŒæ™®é€š/é‡åŒ–/LoRAï¼‰"""
    
    def __init__(self, device_manager: DeviceManager):
        self.device_manager = device_manager
        self.model_cache = {}  # æ¨¡å‹ç¼“å­˜
    
    async def load(
        self,
        model_path: Path,
        quantize: bool = True,
        lora_path: Optional[Path] = None
    ) -> Tuple:
        """
        ç»Ÿä¸€çš„åŠ è½½å…¥å£
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            quantize: æ˜¯å¦é‡åŒ–
            lora_path: LoRAè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (model, tokenizer)
        """
        # 1. åŠ è½½tokenizer
        tokenizer = self._load_tokenizer(model_path)
        
        # 2. åŠ è½½åŸºåº§æ¨¡å‹
        model = self._load_base_model(model_path, quantize)
        
        # 3. åº”ç”¨LoRAï¼ˆå¦‚æœæœ‰ï¼‰
        if lora_path:
            model = self._apply_lora(model, lora_path)
        
        return model, tokenizer
    
    def _load_tokenizer(self, model_path: Path):
        """åŠ è½½tokenizerï¼ˆå¸¦é™çº§ï¼‰"""
        try:
            return AutoTokenizer.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                use_fast=True
            )
        except Exception:
            return AutoTokenizer.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                use_fast=False
            )
    
    def _load_base_model(self, model_path: Path, quantize: bool):
        """åŠ è½½åŸºåº§æ¨¡å‹"""
        load_kwargs = {
            "pretrained_model_name_or_path": str(model_path),
            "trust_remote_code": True,
            "dtype": torch.float16,
            "low_cpu_mem_usage": True,
        }
        
        if quantize and self.device_manager.device == "cuda":
            load_kwargs["quantization_config"] = (
                self.device_manager.get_quantization_config()
            )
            load_kwargs["device_map"] = "auto"
            load_kwargs["max_memory"] = {0: "5.5GiB", "cpu": "0GiB"}
        
        model = AutoModelForCausalLM.from_pretrained(**load_kwargs)
        model.eval()
        
        return model
    
    def _apply_lora(self, base_model, lora_path: Path):
        """åº”ç”¨LoRAé€‚é…å™¨"""
        return PeftModel.from_pretrained(
            base_model,
            str(lora_path),
            dtype=torch.float16
        )
```

---

### é˜¶æ®µ1æ€»ç»“

**å®Œæˆæ ‡å‡†**ï¼š
- âœ… æ‰€æœ‰æ–°æ¨¡å—å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆè¦†ç›–ç‡>80%ï¼‰
- âœ… åœ¨4ä¸ªæœåŠ¡ä¸­æˆåŠŸæ›¿æ¢è®¾å¤‡ç®¡ç†ä»£ç 
- âœ… åœ¨3ä¸ªæœåŠ¡ä¸­æˆåŠŸæ›¿æ¢æ¨¡å‹åŠ è½½ä»£ç 
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼ˆæ— ä¸‹é™ï¼‰

**é¢„æœŸæ•ˆæœ**ï¼š
- â¬‡ï¸ ä»£ç é‡å¤ç‡: 25% â†’ 18% (-28%)
- â¬‡ï¸ è®¾å¤‡ç®¡ç†é‡å¤: 4å¤„ â†’ 1å¤„ (-75%)
- â¬‡ï¸ æ¨¡å‹åŠ è½½é‡å¤: 3å¤„ â†’ 1å¤„ (-66%)

**ä¸‹ä¸€æ­¥**ï¼šè¿›å…¥é˜¶æ®µ2 - æ¨¡å‹æœåŠ¡å±‚é‡æ„

---

## å››ã€é˜¶æ®µ2ï¼šæ¨¡å‹å±‚ (Week 3-5)

### ç›®æ ‡
ğŸ¯ å»ºç«‹ç»Ÿä¸€çš„LLMæŠ½è±¡å±‚  
ğŸ¯ æ‹†åˆ†transformers_serviceï¼ˆ835è¡Œ â†’ 280è¡Œï¼‰  
ğŸ¯ å®ç°æ¨¡å‹æœåŠ¡çš„å¯æ’æ‹”æ¶æ„

### Week 3: LLMæŠ½è±¡æ¥å£

#### T2.1 å®šä¹‰BaseLLMæ¥å£ï¼ˆ2å¤©ï¼‰

**æ ¸å¿ƒæ¥å£è®¾è®¡**ï¼š
```python
# Backend/app/core/llm/base_llm.py
from abc import ABC, abstractmethod
from typing import AsyncGenerator, Dict, Any, Optional, List
from dataclasses import dataclass

@dataclass
class LLMConfig:
    """LLMé€šç”¨é…ç½®"""
    model_name: str
    temperature: float = 0.7
    max_new_tokens: int = 512
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.1

@dataclass
class Message:
    """æ¶ˆæ¯ç»“æ„"""
    role: str  # system/user/assistant
    content: str

class BaseLLM(ABC):
    """æ‰€æœ‰LLMæœåŠ¡çš„åŸºç±»"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.model_name = config.model_name
    
    @abstractmethod
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¼‚æ­¥åŠ è½½ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        pass
    
    @abstractmethod
    async def generate(
        self,
        messages: List[Message],
        stream: bool = False,
        **kwargs
    ) -> str | AsyncGenerator[str, None]:
        """
        ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒæµå¼/éæµå¼ï¼‰
        
        Args:
            messages: å¯¹è¯å†å²
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: é¢å¤–å‚æ•°ï¼ˆè¦†ç›–configï¼‰
            
        Returns:
            å®Œæ•´å›å¤ æˆ– æµå¼ç”Ÿæˆå™¨
        """
        pass
    
    @abstractmethod
    async def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            {
                "name": "æ¨¡å‹åç§°",
                "type": "transformers/ollama/openai",
                "status": "loaded/loading/error",
                "device": "cuda/cpu",
                "memory_usage_mb": 1234,
                "loaded_adapters": ["lora1", "lora2"]  # ä»…LoRAæ¨¡å‹
            }
        """
        pass
    
    @abstractmethod
    async def cleanup(self):
        """æ¸…ç†èµ„æºï¼ˆå¸è½½æ¨¡å‹ï¼‰"""
        pass
    
    def _merge_config(self, **kwargs) -> Dict[str, Any]:
        """åˆå¹¶é…ç½®å‚æ•°ï¼ˆkwargsä¼˜å…ˆï¼‰"""
        base = {
            "temperature": self.config.temperature,
            "max_new_tokens": self.config.max_new_tokens,
            "top_p": self.config.top_p,
            "top_k": self.config.top_k,
            "repetition_penalty": self.config.repetition_penalty
        }
        base.update(kwargs)
        return base
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š
```python
# Backend/app/tests/test_base_llm.py
import pytest
from app.core.llm.base_llm import BaseLLM, LLMConfig, Message

class MockLLM(BaseLLM):
    """æµ‹è¯•ç”¨çš„Mockå®ç°"""
    
    async def initialize(self) -> bool:
        return True
    
    async def generate(self, messages, stream=False, **kwargs):
        if stream:
            async def gen():
                yield "Hello"
                yield " World"
            return gen()
        return "Hello World"
    
    async def get_model_info(self):
        return {
            "name": self.model_name,
            "type": "mock",
            "status": "loaded"
        }
    
    async def cleanup(self):
        pass

@pytest.mark.asyncio
async def test_llm_config_merge():
    config = LLMConfig(model_name="test", temperature=0.5)
    llm = MockLLM(config)
    
    merged = llm._merge_config(temperature=0.9, max_new_tokens=1024)
    assert merged["temperature"] == 0.9  # kwargsä¼˜å…ˆ
    assert merged["max_new_tokens"] == 1024
    assert merged["top_p"] == 0.9  # ä½¿ç”¨é»˜è®¤å€¼

@pytest.mark.asyncio
async def test_llm_generate():
    config = LLMConfig(model_name="test")
    llm = MockLLM(config)
    
    messages = [Message(role="user", content="Hi")]
    response = await llm.generate(messages)
    assert response == "Hello World"
```

---

#### T2.2 å®ç°OllamaLLMï¼ˆ1å¤©ï¼‰

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# Backend/app/core/llm/ollama_llm.py
import httpx
from typing import AsyncGenerator, List
from app.core.llm.base_llm import BaseLLM, Message

class OllamaLLM(BaseLLM):
    """Ollama LLMå®ç°"""
    
    def __init__(self, config, base_url: str = "http://localhost:11434"):
        super().__init__(config)
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=300.0)
    
    async def initialize(self) -> bool:
        """æ£€æŸ¥Ollamaæ˜¯å¦åœ¨çº¿"""
        try:
            response = await self.client.get(f"{self.base_url}/api/tags")
            models = response.json().get("models", [])
            return any(m["name"] == self.model_name for m in models)
        except:
            return False
    
    async def generate(
        self,
        messages: List[Message],
        stream: bool = False,
        **kwargs
    ):
        """è°ƒç”¨Ollama API"""
        gen_config = self._merge_config(**kwargs)
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": m.role, "content": m.content} for m in messages],
            "stream": stream,
            "options": {
                "temperature": gen_config["temperature"],
                "num_predict": gen_config["max_new_tokens"],
                "top_p": gen_config["top_p"],
                "top_k": gen_config["top_k"],
                "repeat_penalty": gen_config["repetition_penalty"]
            }
        }
        
        if stream:
            return self._stream_generate(payload)
        else:
            return await self._sync_generate(payload)
    
    async def _sync_generate(self, payload: dict) -> str:
        """éæµå¼ç”Ÿæˆ"""
        response = await self.client.post(
            f"{self.base_url}/api/chat",
            json=payload
        )
        return response.json()["message"]["content"]
    
    async def _stream_generate(self, payload: dict) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆ"""
        async with self.client.stream(
            "POST",
            f"{self.base_url}/api/chat",
            json=payload
        ) as response:
            async for line in response.aiter_lines():
                if line:
                    data = json.loads(line)
                    if "message" in data:
                        yield data["message"]["content"]
    
    async def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        try:
            response = await self.client.post(
                f"{self.base_url}/api/show",
                json={"name": self.model_name}
            )
            info = response.json()
            return {
                "name": self.model_name,
                "type": "ollama",
                "status": "loaded",
                "parameters": info.get("parameters", "unknown"),
                "size_gb": info.get("size", 0) / 1024**3
            }
        except:
            return {"name": self.model_name, "type": "ollama", "status": "error"}
    
    async def cleanup(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        await self.client.aclose()
```

---

#### T2.3 ç»Ÿä¸€EmbeddingæœåŠ¡ï¼ˆ2å¤©ï¼‰

**å½“å‰é—®é¢˜**ï¼š
- `embedding_service.py` (334è¡Œ)ï¼šTransformers embeddingå®ç°
- `ollama_embedding_service.py` (204è¡Œ)ï¼šOllama embeddingå®ç°
- **é‡å¤ä»£ç **ï¼šä¸¤ä¸ªæœåŠ¡æœ‰ç›¸ä¼¼çš„æ¥å£å’Œé”™è¯¯å¤„ç†

**é‡æ„æ–¹æ¡ˆ**ï¼šç»Ÿä¸€ä¸ºä¸€ä¸ªEmbeddingServiceï¼Œæ”¯æŒå¤šç§åç«¯

```python
# Backend/app/services/llm/embedding_service.pyï¼ˆé‡æ„åï¼‰
from typing import List, Optional
from pathlib import Path
import numpy as np

class EmbeddingService:
    """ç»Ÿä¸€çš„EmbeddingæœåŠ¡ï¼ˆæ”¯æŒTransformerså’ŒOllamaï¼‰"""
    
    def __init__(
        self,
        backend: str = "transformers",  # transformers | ollama
        model_name: str = "BAAI/bge-small-zh-v1.5"
    ):
        self.backend = backend
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        if self.backend == "transformers":
            return await self._init_transformers()
        elif self.backend == "ollama":
            return await self._init_ollama()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„backend: {self.backend}")
    
    async def _init_transformers(self) -> bool:
        """åˆå§‹åŒ–Transformersæ¨¡å‹"""
        from transformers import AutoModel, AutoTokenizer
        from app.core.device.gpu_manager import DeviceManager
        
        device_manager = DeviceManager()
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(
                self.model_name,
                trust_remote_code=True
            ).to(device_manager.device)
            self.model.eval()
            return True
        except Exception as e:
            print(f"Transformersåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def _init_ollama(self) -> bool:
        """åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯"""
        import httpx
        self.client = httpx.AsyncClient(timeout=60.0)
        return True
    
    async def embed_text(self, text: str) -> List[float]:
        """æ–‡æœ¬å‘é‡åŒ–ï¼ˆç»Ÿä¸€æ¥å£ï¼‰"""
        if self.backend == "transformers":
            return await self._embed_transformers(text)
        elif self.backend == "ollama":
            return await self._embed_ollama(text)
    
    async def _embed_transformers(self, text: str) -> List[float]:
        """Transformerså‘é‡åŒ–"""
        import torch
        
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ä½¿ç”¨[CLS] tokençš„embedding
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
        
        return embedding.tolist()
    
    async def _embed_ollama(self, text: str) -> List[float]:
        """Ollamaå‘é‡åŒ–"""
        response = await self.client.post(
            "http://localhost:11434/api/embeddings",
            json={"model": self.model_name, "prompt": text}
        )
        return response.json()["embedding"]
    
    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å‘é‡åŒ–"""
        return [await self.embed_text(text) for text in texts]
```

**é‡æ„æ•ˆæœ**ï¼š
- 2ä¸ªæ–‡ä»¶ï¼ˆ538è¡Œï¼‰â†’ 1ä¸ªæ–‡ä»¶ï¼ˆ250è¡Œï¼‰
- å‡å°‘288è¡Œï¼ˆ-54%ï¼‰
- ç»Ÿä¸€æ¥å£ï¼Œä¾¿äºåˆ‡æ¢backend

---

### Week 4-5: æ¨¡å‹æœåŠ¡é‡æ„

#### T2.4 æ‹†åˆ†TransformersServiceï¼ˆ5å¤©ï¼‰

**å½“å‰é—®é¢˜**ï¼š`transformers_service.py`ï¼ˆ835è¡Œï¼‰åŒ…å«7ä¸ªèŒè´£æ··åˆ

**æ‹†åˆ†æ–¹æ¡ˆ**ï¼š6ä¸ªç‹¬ç«‹æ¨¡å— + 1ä¸ªåè°ƒå™¨

##### æ¨¡å—1: PromptBuilder - æç¤ºè¯æ„å»ºï¼ˆ150è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/prompt_builder.py
from typing import List, Optional
from jinja2 import Template

class PromptBuilder:
    """ç»Ÿä¸€çš„æç¤ºè¯æ„å»ºå™¨ï¼ˆæ”¯æŒæ¨¡æ¿ï¼‰"""
    
    def __init__(self, template_path: Optional[str] = None):
        self.template = self._load_template(template_path)
    
    def build(
        self,
        messages: List[dict],
        system_prompt: Optional[str] = None,
        use_template: bool = True
    ) -> str:
        """
        æ„å»ºå®Œæ•´æç¤ºè¯
        
        Args:
            messages: å¯¹è¯å†å² [{"role": "user", "content": "..."}, ...]
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            use_template: æ˜¯å¦ä½¿ç”¨Jinja2æ¨¡æ¿
            
        Returns:
            æ ¼å¼åŒ–åçš„æç¤ºè¯
        """
        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            messages.insert(0, {"role": "system", "content": system_prompt})
        
        # ä½¿ç”¨æ¨¡æ¿æ¸²æŸ“
        if use_template and self.template:
            return self.template.render(messages=messages)
        
        # é»˜è®¤æ ¼å¼åŒ–
        return self._default_format(messages)
    
    def _load_template(self, path: Optional[str]) -> Optional[Template]:
        """åŠ è½½Jinja2æ¨¡æ¿"""
        if not path:
            return None
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return Template(f.read())
        except:
            return None
    
    def _default_format(self, messages: List[dict]) -> str:
        """é»˜è®¤æ ¼å¼åŒ–ï¼ˆChatMLé£æ ¼ï¼‰"""
        prompt_parts = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            prompt_parts.append(f"<|im_start|>{role}\n{content}<|im_end|>")
        prompt_parts.append("<|im_start|>assistant\n")
        return "\n".join(prompt_parts)
```

##### æ¨¡å—2: ResponseProcessor - å“åº”åå¤„ç†ï¼ˆ80è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/response_processor.py
import re

class ResponseProcessor:
    """å“åº”åå¤„ç†å™¨ï¼ˆæ¸…ç†ã€è¿‡æ»¤ã€æ ¼å¼åŒ–ï¼‰"""
    
    def __init__(self):
        self.stop_words = ["<|im_end|>", "<|endoftext|>", "</s>"]
    
    def process(
        self,
        raw_text: str,
        remove_prompt: bool = True,
        clean_special_tokens: bool = True
    ) -> str:
        """
        å¤„ç†æ¨¡å‹è¾“å‡º
        
        Args:
            raw_text: åŸå§‹è¾“å‡º
            remove_prompt: æ˜¯å¦ç§»é™¤è¾“å…¥æç¤º
            clean_special_tokens: æ˜¯å¦æ¸…ç†ç‰¹æ®Šæ ‡è®°
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        text = raw_text
        
        # 1. ç§»é™¤æç¤ºè¯éƒ¨åˆ†ï¼ˆå¦‚æœåŒ…å«ï¼‰
        if remove_prompt and "<|im_start|>assistant" in text:
            text = text.split("<|im_start|>assistant\n")[-1]
        
        # 2. æ¸…ç†åœæ­¢è¯
        if clean_special_tokens:
            for stop_word in self.stop_words:
                text = text.split(stop_word)[0]
        
        # 3. æ¸…ç†å¤šä½™ç©ºç™½
        text = text.strip()
        text = re.sub(r'\n{3,}', '\n\n', text)  # æœ€å¤š2ä¸ªæ¢è¡Œ
        
        return text
    
    def chunk_stream(self, text: str, chunk_size: int = 10) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å—ç”¨äºæµå¼è¾“å‡º
        
        Args:
            text: å®Œæ•´æ–‡æœ¬
            chunk_size: æ¯å—å­—ç¬¦æ•°
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
```

##### æ¨¡å—3: LoRAAdapter - LoRAç®¡ç†ï¼ˆ120è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/lora_adapter.py
from pathlib import Path
from typing import Optional
from peft import PeftModel
import torch

class LoRAAdapter:
    """LoRAé€‚é…å™¨ç®¡ç†ï¼ˆåŠ è½½/åˆ‡æ¢/å¸è½½ï¼‰"""
    
    def __init__(self, base_model):
        self.base_model = base_model
        self.current_adapter: Optional[str] = None
        self.active_model = base_model
    
    def load_adapter(self, lora_path: Path, adapter_name: str = "default") -> bool:
        """
        åŠ è½½LoRAé€‚é…å™¨
        
        Args:
            lora_path: LoRAæƒé‡è·¯å¾„
            adapter_name: é€‚é…å™¨åç§°
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # å¦‚æœå·²æœ‰é€‚é…å™¨ï¼Œå…ˆå¸è½½
            if self.current_adapter:
                self.unload_adapter()
            
            # åŠ è½½æ–°é€‚é…å™¨
            self.active_model = PeftModel.from_pretrained(
                self.base_model,
                str(lora_path),
                adapter_name=adapter_name,
                torch_dtype=torch.float16
            )
            self.current_adapter = adapter_name
            
            return True
        except Exception as e:
            print(f"åŠ è½½LoRAå¤±è´¥: {e}")
            return False
    
    def unload_adapter(self):
        """å¸è½½å½“å‰é€‚é…å™¨ï¼ˆæ¢å¤åŸºåº§æ¨¡å‹ï¼‰"""
        if self.current_adapter:
            # é‡Šæ”¾LoRAæ¨¡å‹
            del self.active_model
            torch.cuda.empty_cache()
            
            # æ¢å¤åŸºåº§æ¨¡å‹
            self.active_model = self.base_model
            self.current_adapter = None
    
    def get_model(self):
        """è·å–å½“å‰æ¿€æ´»çš„æ¨¡å‹"""
        return self.active_model
    
    def get_adapter_info(self) -> dict:
        """è·å–é€‚é…å™¨ä¿¡æ¯"""
        if not self.current_adapter:
            return {"loaded": False}
        
        return {
            "loaded": True,
            "adapter_name": self.current_adapter,
            "trainable_params": sum(
                p.numel() for p in self.active_model.parameters() if p.requires_grad
            )
        }
```

##### æ¨¡å—4: TransformersLLM - åè°ƒå™¨ï¼ˆ280è¡Œï¼‰

```python
# Backend/app/core/llm/transformers/transformers_llm.py
from pathlib import Path
from typing import List, Optional, AsyncGenerator
import torch
from transformers import TextIteratorStreamer
from threading import Thread

from app.core.llm.base_llm import BaseLLM, Message, LLMConfig
from app.core.device.gpu_manager import DeviceManager
from app.core.model.model_loader import ModelLoader
from app.core.llm.transformers.prompt_builder import PromptBuilder
from app.core.llm.transformers.response_processor import ResponseProcessor
from app.core.llm.transformers.lora_adapter import LoRAAdapter

class TransformersLLM(BaseLLM):
    """Transformersæœ¬åœ°æ¨¡å‹å®ç°ï¼ˆæ•´åˆæ‰€æœ‰æ¨¡å—ï¼‰"""
    
    def __init__(
        self,
        config: LLMConfig,
        model_path: Path,
        quantize: bool = True,
        lora_path: Optional[Path] = None
    ):
        super().__init__(config)
        self.model_path = model_path
        self.quantize = quantize
        self.lora_path = lora_path
        
        # ä¾èµ–ç»„ä»¶
        self.device_manager = DeviceManager()
        self.model_loader = ModelLoader(self.device_manager)
        self.prompt_builder = PromptBuilder()
        self.response_processor = ResponseProcessor()
        
        # æ¨¡å‹èµ„æº
        self.model = None
        self.tokenizer = None
        self.lora_adapter: Optional[LoRAAdapter] = None
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            # 1. åŠ è½½åŸºåº§æ¨¡å‹
            self.model, self.tokenizer = await self.model_loader.load(
                model_path=self.model_path,
                quantize=self.quantize
            )
            
            # 2. åˆå§‹åŒ–LoRAç®¡ç†å™¨
            self.lora_adapter = LoRAAdapter(self.model)
            
            # 3. åŠ è½½LoRAï¼ˆå¦‚æœæœ‰ï¼‰
            if self.lora_path:
                self.lora_adapter.load_adapter(self.lora_path)
            
            return True
        except Exception as e:
            print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def generate(
        self,
        messages: List[Message],
        stream: bool = False,
        system_prompt: Optional[str] = None,
        **kwargs
    ):
        """ç”Ÿæˆå›å¤"""
        # 1. æ„å»ºæç¤ºè¯
        message_dicts = [{"role": m.role, "content": m.content} for m in messages]
        prompt = self.prompt_builder.build(message_dicts, system_prompt)
        
        # 2. Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.device_manager.device)
        
        # 3. åˆå¹¶ç”Ÿæˆå‚æ•°
        gen_config = self._merge_config(**kwargs)
        gen_kwargs = {
            "max_new_tokens": gen_config["max_new_tokens"],
            "temperature": gen_config["temperature"],
            "top_p": gen_config["top_p"],
            "top_k": gen_config["top_k"],
            "repetition_penalty": gen_config["repetition_penalty"],
            "do_sample": True,
            "pad_token_id": self.tokenizer.eos_token_id
        }
        
        # 4. ç”Ÿæˆ
        if stream:
            return self._stream_generate(inputs, gen_kwargs)
        else:
            return await self._sync_generate(inputs, gen_kwargs)
    
    async def _sync_generate(self, inputs, gen_kwargs) -> str:
        """éæµå¼ç”Ÿæˆ"""
        model = self.lora_adapter.get_model()
        
        with torch.no_grad():
            outputs = model.generate(**inputs, **gen_kwargs)
        
        # è§£ç 
        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # åå¤„ç†
        return self.response_processor.process(full_text)
    
    async def _stream_generate(self, inputs, gen_kwargs) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆ"""
        model = self.lora_adapter.get_model()
        
        # åˆ›å»ºæµå¼è¾“å‡ºå™¨
        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )
        gen_kwargs["streamer"] = streamer
        
        # åœ¨åå°çº¿ç¨‹ç”Ÿæˆ
        thread = Thread(
            target=model.generate,
            kwargs={**inputs, **gen_kwargs}
        )
        thread.start()
        
        # é€å—è¾“å‡º
        for text_chunk in streamer:
            if text_chunk:
                yield text_chunk
        
        thread.join()
    
    async def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        device_info = self.device_manager.get_device_info()
        lora_info = (
            self.lora_adapter.get_adapter_info() 
            if self.lora_adapter 
            else {"loaded": False}
        )
        
        return {
            "name": self.model_name,
            "type": "transformers",
            "status": "loaded" if self.model else "unloaded",
            "device": device_info.device_type,
            "memory_usage_mb": device_info.allocated_memory_gb * 1024,
            "quantized": self.quantize,
            "lora_loaded": lora_info["loaded"],
            "lora_adapter": lora_info.get("adapter_name")
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.lora_adapter:
            self.lora_adapter.unload_adapter()
        
        del self.model
        del self.tokenizer
        self.device_manager.clear_cache()
```

---

#### T2.5 é‡æ„model_mgmtæ¨¡å—ï¼ˆ2å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
- `model_manager.py`ï¼ˆ214è¡Œï¼‰ï¼šæ¨¡å‹æ³¨å†Œå’Œç”Ÿå‘½å‘¨æœŸç®¡ç†
- `model_scanner.py`ï¼ˆ344è¡Œï¼‰ï¼šæ‰«æLLMæ¨¡å‹
- `lora_scanner_service.py`ï¼ˆ393è¡Œï¼‰ï¼šæ‰«æLoRAé€‚é…å™¨
- **é‡å¤ä»£ç **ï¼šæ–‡ä»¶éå†ã€æ ¼å¼è¯†åˆ«ã€å…ƒæ•°æ®æå–é€»è¾‘é‡å¤3æ¬¡

**ä¸ºä»€ä¹ˆæ”¾åœ¨æ¨¡å‹å±‚**ï¼šmodel_mgmtå±äºæ¨¡å‹ç®¡ç†çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ¨¡å‹åŠ è½½ã€LoRAé€‚é…å™¨åŒå±æ¨¡å‹å±‚åŸºç¡€è®¾æ–½ã€‚

**é‡æ„æ–¹æ¡ˆ**ï¼šåˆå¹¶ä¸º2ä¸ªæ–‡ä»¶ï¼ˆmodel_scanner.py + deployment.pyï¼‰

```python
# Backend/app/services/model_mgmt/model_scanner.py
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class ModelInfo:
    """ç»Ÿä¸€çš„æ¨¡å‹ä¿¡æ¯ç»“æ„"""
    model_id: str
    model_name: str
    model_type: str  # "llm" | "lora" | "embedding"
    model_path: str
    format: str  # "gguf" | "safetensors" | "pytorch"
    size_mb: float
    metadata: Dict[str, Any]

class ModelScanner:
    """ç»Ÿä¸€æ¨¡å‹æ‰«æå™¨ï¼ˆæ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰"""
    
    def scan(
        self,
        base_path: Path,
        model_type: str  # "llm" | "lora" | "embedding"
    ) -> List[ModelInfo]:
        """
        ç»Ÿä¸€æ‰«ææ¥å£
        
        Args:
            base_path: æ‰«ææ ¹ç›®å½•
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            æ ‡å‡†åŒ–çš„æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        if model_type == "llm":
            return self._scan_llm(base_path)
        elif model_type == "lora":
            return self._scan_lora(base_path)
        elif model_type == "embedding":
            return self._scan_embedding(base_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    def _scan_llm(self, base_path: Path) -> List[ModelInfo]:
        """æ‰«æLLMæ¨¡å‹ï¼ˆGGUF/Safetensorsï¼‰"""
        models = []
        
        for model_dir in base_path.iterdir():
            if not model_dir.is_dir():
                continue
            
            # æ£€æµ‹GGUFæ–‡ä»¶
            gguf_files = list(model_dir.glob("*.gguf"))
            if gguf_files:
                models.append(self._parse_gguf_model(gguf_files[0]))
                continue
            
            # æ£€æµ‹Safetensorsæ–‡ä»¶
            if (model_dir / "model.safetensors").exists():
                models.append(self._parse_safetensors_model(model_dir))
        
        return models
    
    def _scan_lora(self, base_path: Path) -> List[ModelInfo]:
        """æ‰«æLoRAé€‚é…å™¨"""
        adapters = []
        
        for adapter_dir in base_path.iterdir():
            if not adapter_dir.is_dir():
                continue
            
            # æ£€æµ‹adapter_config.json
            config_file = adapter_dir / "adapter_config.json"
            if config_file.exists():
                adapters.append(self._parse_lora_adapter(adapter_dir))
        
        return adapters
    
    def _parse_gguf_model(self, gguf_path: Path) -> ModelInfo:
        """è§£æGGUFæ¨¡å‹ä¿¡æ¯"""
        return ModelInfo(
            model_id=gguf_path.stem,
            model_name=gguf_path.stem,
            model_type="llm",
            model_path=str(gguf_path.parent),
            format="gguf",
            size_mb=gguf_path.stat().st_size / 1024 / 1024,
            metadata={"quantization": self._detect_quantization(gguf_path.name)}
        )
    
    # ... å…¶ä»–è§£ææ–¹æ³•
```

```python
# Backend/app/services/model_mgmt/deployment.py
from typing import Optional, Dict
from app.core.device.gpu_manager import DeviceManager

class ModelDeployment:
    """æ¨¡å‹éƒ¨ç½²ç®¡ç†"""
    
    def __init__(self):
        self.device_manager = DeviceManager()
        self.deployed_models: Dict[str, Any] = {}
    
    async def deploy(
        self,
        model_id: str,
        model_path: str,
        model_type: str
    ) -> bool:
        """
        éƒ¨ç½²æ¨¡å‹ï¼ˆåŠ è½½åˆ°å†…å­˜ï¼‰
        
        Args:
            model_id: æ¨¡å‹ID
            model_path: æ¨¡å‹è·¯å¾„
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # æ£€æŸ¥è®¾å¤‡èµ„æº
        device_info = self.device_manager.get_device_info()
        if device_info.allocated_memory_gb > 5.0:
            raise RuntimeError("æ˜¾å­˜ä¸è¶³ï¼Œæ— æ³•éƒ¨ç½²æ¨¡å‹")
        
        # åŠ è½½æ¨¡å‹
        if model_type == "llm":
            from app.core.llm.transformers.transformers_llm import TransformersLLM
            from app.core.llm.base_llm import LLMConfig
            
            config = LLMConfig(model_name=model_id)
            model = TransformersLLM(config, Path(model_path))
            await model.initialize()
            
            self.deployed_models[model_id] = model
        
        return True
    
    async def undeploy(self, model_id: str) -> bool:
        """å¸è½½æ¨¡å‹"""
        if model_id not in self.deployed_models:
            return False
        
        model = self.deployed_models.pop(model_id)
        await model.cleanup()
        
        return True
```

**é‡æ„æ•ˆæœ**ï¼š
- 3ä¸ªæ–‡ä»¶ï¼ˆ951è¡Œï¼‰â†’ 2ä¸ªæ–‡ä»¶ï¼ˆ530è¡Œï¼‰
- å‡å°‘421è¡Œï¼ˆ-44%ï¼‰
- ç»Ÿä¸€æ‰«ææ¥å£ï¼Œæ˜“äºæ‰©å±•

---

#### T2.6 é‡æ„trainingæ¨¡å—ï¼ˆ2å¤©ï¼‰â­

**å½“å‰é—®é¢˜**ï¼š
- `simple_lora_trainer.py`ï¼ˆ500è¡Œï¼‰ï¼šåŒ…å«é‡å¤çš„æ•°æ®é›†éªŒè¯ã€çŠ¶æ€ç®¡ç†ä»£ç 

**ä¸ºä»€ä¹ˆæ”¾åœ¨æ¨¡å‹å±‚**ï¼šLoRAè®­ç»ƒæ˜¯æ¨¡å‹ç®¡ç†çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ¨¡å‹åŠ è½½ã€é€‚é…å™¨ç®¡ç†ç´§å¯†ç›¸å…³ã€‚

**é‡æ„æ–¹æ¡ˆ**ï¼šä½¿ç”¨coreå±‚çš„TaskStateManagerå’ŒProcessManager

```python
# Backend/app/services/training/lora_trainer.pyï¼ˆé‡æ„åï¼‰
from pathlib import Path
from typing import Dict, Any
from app.core.utils.task_state_manager import TaskStateManager
import json

class LoRATrainer:
    """LoRAè®­ç»ƒæœåŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.state_manager = TaskStateManager()  # ä½¿ç”¨ç»Ÿä¸€çš„çŠ¶æ€ç®¡ç†
        self.training_processes = {}
    
    async def start_training(
        self,
        task_id: str,
        base_model_path: str,
        dataset_path: str,
        output_dir: str,
        **training_args
    ) -> bool:
        """
        å¯åŠ¨LoRAè®­ç»ƒ
        
        Args:
            task_id: ä»»åŠ¡ID
            base_model_path: åŸºåº§æ¨¡å‹è·¯å¾„
            dataset_path: æ•°æ®é›†è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            **training_args: è®­ç»ƒå‚æ•°
        """
        # 1. éªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨å·¥å…·ç±»ï¼‰
        if not self._validate_dataset(dataset_path):
            raise ValueError("æ•°æ®é›†æ ¼å¼é”™è¯¯")
        
        # 2. åˆ›å»ºè®­ç»ƒé…ç½®
        config = self._build_training_config(
            base_model_path,
            dataset_path,
            output_dir,
            **training_args
        )
        
        # 3. åˆå§‹åŒ–çŠ¶æ€
        self.state_manager.create_task(
            task_id=task_id,
            initial_state="preparing"
        )
        
        # 4. å¯åŠ¨è®­ç»ƒè¿›ç¨‹
        from app.core.utils.process_manager import ProcessManager
        process_manager = ProcessManager()
        
        process = await process_manager.start_python_script(
            script_path="scripts/train_lora.py",
            args=["--config", str(config)],
            on_output=lambda line: self._handle_training_output(task_id, line)
        )
        
        self.training_processes[task_id] = process
        self.state_manager.update_state(task_id, "running")
        
        return True
    
    def _validate_dataset(self, dataset_path: str) -> bool:
        """éªŒè¯æ•°æ®é›†æ ¼å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                return False
            
            for item in data[:5]:  # åªæ£€æŸ¥å‰5æ¡
                if "instruction" not in item or "output" not in item:
                    return False
            
            return True
        except:
            return False
    
    def _handle_training_output(self, task_id: str, line: str):
        """å¤„ç†è®­ç»ƒè¾“å‡ºï¼ˆæ›´æ–°è¿›åº¦ï¼‰"""
        if "loss" in line.lower():
            self.state_manager.update_metadata(
                task_id,
                {"latest_output": line}
            )
```

**é‡æ„æ•ˆæœ**ï¼š
- 500è¡Œ â†’ 350è¡Œï¼ˆ-30%ï¼‰
- ä½¿ç”¨ç»Ÿä¸€çš„TaskStateManagerï¼ˆåˆ é™¤é‡å¤çŠ¶æ€ç®¡ç†ä»£ç ï¼‰
- ä½¿ç”¨ç»Ÿä¸€çš„ProcessManagerï¼ˆåˆ é™¤é‡å¤è¿›ç¨‹ç®¡ç†ä»£ç ï¼‰

---

#### T2.7 æ›´æ–°æœåŠ¡å±‚è°ƒç”¨ï¼ˆ1å¤©ï¼‰

**æ—§ä»£ç ** (`transformers_service.py`):
```python
# æ—§ä»£ç ï¼ˆ835è¡Œï¼‰åŒ…å«æ‰€æœ‰é€»è¾‘
class TransformersService:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        # ... 800å¤šè¡Œå®ç°
```

**æ–°ä»£ç ** (`llm_service.py`):
```python
# Backend/app/services/llm_service.py
from pathlib import Path
from typing import Optional, Dict
from app.core.llm.base_llm import BaseLLM, LLMConfig, Message
from app.core.llm.transformers.transformers_llm import TransformersLLM
from app.core.llm.ollama_llm import OllamaLLM
from app.core.utils.path_resolver import PathResolver

class LLMService:
    """ç»Ÿä¸€çš„LLMæœåŠ¡ï¼ˆå·¥å‚æ¨¡å¼ï¼‰"""
    
    def __init__(self):
        self.path_resolver = PathResolver()
        self.llm_instances: Dict[str, BaseLLM] = {}  # æ¨¡å‹ç¼“å­˜
    
    async def get_llm(
        self,
        model_type: str,  # transformers/ollama
        model_name: str,
        config: Optional[LLMConfig] = None,
        **kwargs
    ) -> BaseLLM:
        """
        è·å–LLMå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            model_name: æ¨¡å‹åç§°
            config: LLMé…ç½®
            **kwargs: é¢å¤–å‚æ•°ï¼ˆlora_pathç­‰ï¼‰
            
        Returns:
            LLMå®ä¾‹
        """
        cache_key = f"{model_type}:{model_name}"
        
        # ä»ç¼“å­˜è·å–
        if cache_key in self.llm_instances:
            return self.llm_instances[cache_key]
        
        # åˆ›å»ºæ–°å®ä¾‹
        if config is None:
            config = LLMConfig(model_name=model_name)
        
        if model_type == "transformers":
            model_path = self.path_resolver.get_model_path("llm", model_name)
            lora_path = kwargs.get("lora_path")
            if lora_path:
                lora_path = self.path_resolver.get_model_path("lora", lora_path)
            
            llm = TransformersLLM(
                config=config,
                model_path=model_path,
                quantize=kwargs.get("quantize", True),
                lora_path=lora_path
            )
        elif model_type == "ollama":
            llm = OllamaLLM(
                config=config,
                base_url=kwargs.get("base_url", "http://localhost:11434")
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # åˆå§‹åŒ–
        success = await llm.initialize()
        if not success:
            raise RuntimeError(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_name}")
        
        # ç¼“å­˜
        self.llm_instances[cache_key] = llm
        
        return llm
    
    async def unload_model(self, model_type: str, model_name: str):
        """å¸è½½æ¨¡å‹"""
        cache_key = f"{model_type}:{model_name}"
        if cache_key in self.llm_instances:
            await self.llm_instances[cache_key].cleanup()
            del self.llm_instances[cache_key]
```

---

### é˜¶æ®µ2æ€»ç»“

**å®Œæˆæ ‡å‡†**ï¼š
- âœ… BaseLLMæ¥å£å•å…ƒæµ‹è¯•é€šè¿‡
- âœ… TransformersLLMé›†æˆæµ‹è¯•é€šè¿‡ï¼ˆå¯¹æ¯”æ—§ç‰ˆæœ¬è¾“å‡ºä¸€è‡´æ€§ï¼‰
- âœ… OllamaLLMè¿æ¥æµ‹è¯•é€šè¿‡
- âœ… embedding_serviceç»Ÿä¸€æµ‹è¯•é€šè¿‡
- âœ… model_mgmtæ¨¡å—é‡æ„å®Œæˆï¼ˆ3æ–‡ä»¶951è¡Œ â†’ 2æ–‡ä»¶530è¡Œï¼‰
- âœ… trainingæ¨¡å—é‡æ„å®Œæˆï¼ˆ500è¡Œ â†’ 350è¡Œï¼‰
- âœ… æ‰€æœ‰æ¨¡å‹ç›¸å…³æœåŠ¡æ›´æ–°å®Œæˆ

**é¢„æœŸæ•ˆæœ**ï¼š
- â¬‡ï¸ transformers_service: 835è¡Œ â†’ 280è¡Œ (-66%)
- â¬‡ï¸ embeddingç›¸å…³: 538è¡Œ â†’ 250è¡Œ (-54%)
- â¬‡ï¸ model_mgmt: 951è¡Œ â†’ 530è¡Œ (-44%)
- â¬‡ï¸ training: 500è¡Œ â†’ 350è¡Œ (-30%)
- â¬†ï¸ å¯æ‰©å±•æ€§: æ–°å¢æ¨¡å‹ç±»å‹åªéœ€å®ç°BaseLLMæ¥å£
- â¬†ï¸ å¯ç»´æŠ¤æ€§: ç»Ÿä¸€çš„æ‰«æå™¨å’Œè®­ç»ƒç®¡ç†

**ä¸‹ä¸€æ­¥**ï¼šè¿›å…¥é˜¶æ®µ3 - ä¸šåŠ¡é€»è¾‘å±‚é‡æ„

---

## äº”ã€é˜¶æ®µ3ï¼šä¸šåŠ¡å±‚ (Week 6-8)

### ç›®æ ‡
ğŸ¯ ç»Ÿä¸€æ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡/å…¨æ–‡/æ··åˆ/å›¾è°±ï¼‰  
ğŸ¯ é‡æ„knowledge_base_serviceï¼ˆ558è¡Œ â†’ 350è¡Œï¼‰  
ğŸ¯ ä¼˜åŒ–RAGæ£€ç´¢æ€§èƒ½

### Week 6: æ£€ç´¢ç­–ç•¥ç»Ÿä¸€

#### å½“å‰é—®é¢˜
- æ£€ç´¢é€»è¾‘åˆ†æ•£åœ¨3ä¸ªæ–‡ä»¶ï¼ˆchat_service, knowledge_base_service, graph_serviceï¼‰
- æ²¡æœ‰ç»Ÿä¸€çš„ç­–ç•¥æ¥å£
- æ— æ³•çµæ´»åˆ‡æ¢æ£€ç´¢ç®—æ³•

#### T3.1 å®šä¹‰æ£€ç´¢ç­–ç•¥æ¥å£ï¼ˆ1å¤©ï¼‰

**ç­–ç•¥æ¨¡å¼è®¾è®¡**ï¼š

```python
# Backend/app/core/retrieval/base_retriever.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®"""
    top_k: int = 5
    score_threshold: float = 0.6
    enable_rerank: bool = False

@dataclass
class Document:
    """æ–‡æ¡£ç»“æ„"""
    content: str
    metadata: Dict[str, Any]
    score: float
    doc_id: str

class BaseRetriever(ABC):
    """æ£€ç´¢å™¨åŸºç±»"""
    
    def __init__(self, config: RetrievalConfig):
        self.config = config
    
    @abstractmethod
    async def retrieve(
        self,
        query: str,
        kb_id: int,
        **kwargs
    ) -> List[Document]:
        """
        æ‰§è¡Œæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_id: çŸ¥è¯†åº“ID
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
        """
        pass
    
    @abstractmethod
    def get_retriever_info(self) -> Dict[str, Any]:
        """
        è·å–æ£€ç´¢å™¨ä¿¡æ¯
        
        Returns:
            {
                "type": "vector/bm25/hybrid/graph",
                "config": {...}
            }
        """
        pass
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š

```python
# Backend/app/tests/test_retriever.py
import pytest
from app.core.retrieval.base_retriever import BaseRetriever, RetrievalConfig, Document

class MockRetriever(BaseRetriever):
    async def retrieve(self, query, kb_id, **kwargs):
        return [
            Document(
                content="æµ‹è¯•æ–‡æ¡£1",
                metadata={"source": "test.txt"},
                score=0.95,
                doc_id="doc1"
            )
        ]
    
    def get_retriever_info(self):
        return {"type": "mock", "config": self.config.__dict__}

@pytest.mark.asyncio
async def test_retriever_interface():
    config = RetrievalConfig(top_k=10)
    retriever = MockRetriever(config)
    
    results = await retriever.retrieve("æµ‹è¯•æŸ¥è¯¢", kb_id=1)
    assert len(results) == 1
    assert results[0].score == 0.95
```

---

#### T3.2 å®ç°å‘é‡æ£€ç´¢å™¨ï¼ˆ2å¤©ï¼‰

```python
# Backend/app/core/retrieval/vector_retriever.py
from typing import List
import numpy as np
from app.core.retrieval.base_retriever import BaseRetriever, Document
from app.services.embedding_service import EmbeddingService

class VectorRetriever(BaseRetriever):
    """å‘é‡æ£€ç´¢å™¨ï¼ˆä½¿ç”¨ChromaDBï¼‰"""
    
    def __init__(self, config, embedding_service: EmbeddingService):
        super().__init__(config)
        self.embedding_service = embedding_service
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """å‘é‡æ£€ç´¢"""
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = await self.embedding_service.embed_text(query)
        
        # 2. ä»ChromaDBæ£€ç´¢
        from app.services.vector_store_service import VectorStoreService
        vector_store = VectorStoreService()
        
        raw_results = vector_store.similarity_search(
            collection_name=f"kb_{kb_id}",
            query_embedding=query_embedding,
            top_k=self.config.top_k
        )
        
        # 3. è½¬æ¢ä¸ºDocumentå¯¹è±¡
        documents = []
        for result in raw_results:
            if result['score'] >= self.config.score_threshold:
                documents.append(Document(
                    content=result['content'],
                    metadata=result['metadata'],
                    score=result['score'],
                    doc_id=result['id']
                ))
        
        # 4. Rerankï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.enable_rerank:
            documents = await self._rerank(query, documents)
        
        return documents
    
    async def _rerank(self, query: str, documents: List[Document]) -> List[Document]:
        """é‡æ’åºï¼ˆä½¿ç”¨BGE-Rerankerï¼‰"""
        # TODO: é›†æˆRerankeræ¨¡å‹
        # æš‚æ—¶è¿”å›åŸç»“æœ
        return documents
    
    def get_retriever_info(self):
        return {
            "type": "vector",
            "config": {
                "top_k": self.config.top_k,
                "score_threshold": self.config.score_threshold,
                "enable_rerank": self.config.enable_rerank
            }
        }
```

---

#### T3.3 å®ç°BM25å…¨æ–‡æ£€ç´¢å™¨ï¼ˆ2å¤©ï¼‰

```python
# Backend/app/core/retrieval/bm25_retriever.py
from typing import List
from rank_bm25 import BM25Okapi
import jieba
from app.core.retrieval.base_retriever import BaseRetriever, Document

class BM25Retriever(BaseRetriever):
    """BM25å…¨æ–‡æ£€ç´¢å™¨"""
    
    def __init__(self, config):
        super().__init__(config)
        self.bm25_index = {}  # {kb_id: BM25Okapiå®ä¾‹}
        self.doc_store = {}   # {kb_id: [Document]}
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """BM25æ£€ç´¢"""
        # 1. æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if kb_id not in self.bm25_index:
            await self._build_index(kb_id)
        
        # 2. åˆ†è¯
        query_tokens = list(jieba.cut(query))
        
        # 3. BM25æ‰“åˆ†
        bm25 = self.bm25_index[kb_id]
        scores = bm25.get_scores(query_tokens)
        
        # 4. æ’åºå¹¶è¿‡æ»¤
        doc_store = self.doc_store[kb_id]
        ranked_indices = np.argsort(scores)[::-1]  # é™åº
        
        documents = []
        for idx in ranked_indices[:self.config.top_k]:
            score = scores[idx]
            if score >= self.config.score_threshold:
                doc = doc_store[idx]
                doc.score = float(score)
                documents.append(doc)
        
        return documents
    
    async def _build_index(self, kb_id: int):
        """æ„å»ºBM25ç´¢å¼•"""
        # 1. ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£
        from app.services.database_service import DatabaseService
        db = DatabaseService()
        
        chunks = db.get_knowledge_chunks(kb_id)
        
        # 2. åˆ†è¯
        tokenized_corpus = []
        doc_store = []
        
        for chunk in chunks:
            tokens = list(jieba.cut(chunk['content']))
            tokenized_corpus.append(tokens)
            
            doc_store.append(Document(
                content=chunk['content'],
                metadata=chunk['metadata'],
                score=0.0,
                doc_id=str(chunk['id'])
            ))
        
        # 3. æ„å»ºBM25
        self.bm25_index[kb_id] = BM25Okapi(tokenized_corpus)
        self.doc_store[kb_id] = doc_store
    
    def get_retriever_info(self):
        return {
            "type": "bm25",
            "config": {
                "top_k": self.config.top_k,
                "indexed_kbs": list(self.bm25_index.keys())
            }
        }
```

---

### Week 6æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… BaseRetrieveræ¥å£å®šä¹‰
- âœ… VectorRetrieverå®ç°ï¼ˆChromaDBï¼‰
- âœ… BM25Retrieverå®ç°ï¼ˆå…¨æ–‡æ£€ç´¢ï¼‰

**ä»£ç é‡**ï¼š
- æ–°å¢: ~350è¡Œï¼ˆ3ä¸ªæ–‡ä»¶ï¼‰
- ä¸‹ä¸€æ­¥: å®ç°HybridRetriever + GraphRetriever

---

### Week 7: æ··åˆæ£€ç´¢ä¸çŸ¥è¯†å›¾è°±

#### T3.4 å®ç°æ··åˆæ£€ç´¢å™¨ï¼ˆ2å¤©ï¼‰

**æ··åˆæ£€ç´¢ç­–ç•¥**ï¼šå‘é‡æ£€ç´¢ + BM25 + èåˆç®—æ³•

```python
# Backend/app/core/retrieval/hybrid_retriever.py
from typing import List, Dict
import numpy as np
from app.core.retrieval.base_retriever import BaseRetriever, Document, RetrievalConfig
from app.core.retrieval.vector_retriever import VectorRetriever
from app.core.retrieval.bm25_retriever import BM25Retriever

class HybridRetriever(BaseRetriever):
    """æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡+BM25èåˆï¼‰"""
    
    def __init__(
        self,
        config: RetrievalConfig,
        vector_retriever: VectorRetriever,
        bm25_retriever: BM25Retriever,
        vector_weight: float = 0.7  # å‘é‡æ£€ç´¢æƒé‡
    ):
        super().__init__(config)
        self.vector_retriever = vector_retriever
        self.bm25_retriever = bm25_retriever
        self.vector_weight = vector_weight
        self.bm25_weight = 1.0 - vector_weight
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """æ··åˆæ£€ç´¢ï¼ˆRRFèåˆï¼‰"""
        # 1. å¹¶è¡Œæ‰§è¡Œä¸¤ç§æ£€ç´¢
        vector_results = await self.vector_retriever.retrieve(query, kb_id)
        bm25_results = await self.bm25_retriever.retrieve(query, kb_id)
        
        # 2. RRFèåˆï¼ˆReciprocal Rank Fusionï¼‰
        fused_results = self._rrf_fusion(vector_results, bm25_results)
        
        # 3. æˆªæ–­åˆ°top_k
        return fused_results[:self.config.top_k]
    
    def _rrf_fusion(
        self,
        vector_docs: List[Document],
        bm25_docs: List[Document],
        k: int = 60  # RRFå‚æ•°
    ) -> List[Document]:
        """
        RRFèåˆç®—æ³•
        Score(d) = Î£ 1/(k + rank_i(d))
        """
        # 1. æ„å»ºæ–‡æ¡£å­—å…¸
        doc_dict: Dict[str, Document] = {}
        doc_scores: Dict[str, float] = {}
        
        # 2. å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        for rank, doc in enumerate(vector_docs, start=1):
            doc_id = doc.doc_id
            rrf_score = self.vector_weight / (k + rank)
            
            doc_dict[doc_id] = doc
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score
        
        # 3. å¤„ç†BM25æ£€ç´¢ç»“æœ
        for rank, doc in enumerate(bm25_docs, start=1):
            doc_id = doc.doc_id
            rrf_score = self.bm25_weight / (k + rank)
            
            if doc_id not in doc_dict:
                doc_dict[doc_id] = doc
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score
        
        # 4. æŒ‰èåˆåˆ†æ•°æ’åº
        sorted_ids = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        # 5. æ›´æ–°åˆ†æ•°å¹¶è¿”å›
        fused_docs = []
        for doc_id, score in sorted_ids:
            doc = doc_dict[doc_id]
            doc.score = score
            fused_docs.append(doc)
        
        return fused_docs
    
    def get_retriever_info(self):
        return {
            "type": "hybrid",
            "config": {
                "vector_weight": self.vector_weight,
                "bm25_weight": self.bm25_weight,
                "top_k": self.config.top_k
            }
        }
```

**æµ‹è¯•ç”¨ä¾‹**ï¼š

```python
# Backend/app/tests/test_hybrid_retriever.py
import pytest
from app.core.retrieval.hybrid_retriever import HybridRetriever
from app.core.retrieval.base_retriever import Document

@pytest.mark.asyncio
async def test_rrf_fusion():
    # Mockæ£€ç´¢å™¨
    class MockVectorRetriever:
        async def retrieve(self, query, kb_id):
            return [
                Document("doc1", {}, 0.9, "1"),
                Document("doc2", {}, 0.8, "2")
            ]
    
    class MockBM25Retriever:
        async def retrieve(self, query, kb_id):
            return [
                Document("doc2", {}, 0.95, "2"),  # doc2åœ¨ä¸¤ä¸ªç»“æœä¸­
                Document("doc3", {}, 0.7, "3")
            ]
    
    config = RetrievalConfig(top_k=5)
    retriever = HybridRetriever(
        config,
        MockVectorRetriever(),
        MockBM25Retriever(),
        vector_weight=0.7
    )
    
    results = await retriever.retrieve("test", kb_id=1)
    
    # doc2åº”è¯¥æ’ç¬¬ä¸€ï¼ˆä¸¤ä¸ªæ£€ç´¢å™¨éƒ½è¿”å›äº†ï¼‰
    assert results[0].doc_id == "2"
    assert len(results) == 3
```

---

#### T3.5 å®ç°çŸ¥è¯†å›¾è°±æ£€ç´¢å™¨ï¼ˆ3å¤©ï¼‰

```python
# Backend/app/core/retrieval/graph_retriever.py
from typing import List, Set, Dict
from app.core.retrieval.base_retriever import BaseRetriever, Document

class GraphRetriever(BaseRetriever):
    """çŸ¥è¯†å›¾è°±æ£€ç´¢å™¨ï¼ˆå®ä½“å…³ç³»æ‰©å±•ï¼‰"""
    
    def __init__(self, config, graph_service):
        super().__init__(config)
        self.graph_service = graph_service
    
    async def retrieve(self, query: str, kb_id: int, **kwargs) -> List[Document]:
        """
        å›¾è°±æ£€ç´¢æµç¨‹ï¼š
        1. æå–æŸ¥è¯¢ä¸­çš„å®ä½“
        2. æŸ¥æ‰¾å®ä½“çš„å…³ç³»ä¸‰å…ƒç»„
        3. æ‰©å±•åˆ°ç›¸å…³å®ä½“
        4. æ”¶é›†å…³è”æ–‡æ¡£
        """
        # 1. æå–å®ä½“
        entities = await self._extract_entities(query, kb_id)
        if not entities:
            return []
        
        # 2. æŸ¥æ‰¾å…³ç³»
        triples = await self._find_triples(entities, kb_id)
        
        # 3. æ‰©å±•ç›¸å…³å®ä½“
        related_entities = self._expand_entities(triples, entities)
        
        # 4. æ”¶é›†æ–‡æ¡£
        documents = await self._collect_documents(
            entities | related_entities,
            kb_id
        )
        
        return documents[:self.config.top_k]
    
    async def _extract_entities(self, query: str, kb_id: int) -> Set[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å®ä½“"""
        # è°ƒç”¨entity_extraction_service
        from app.services.entity_extraction_service import EntityExtractionService
        
        service = EntityExtractionService()
        result = await service.extract_entities(query, kb_id)
        
        return set(result.get("entities", []))
    
    async def _find_triples(
        self,
        entities: Set[str],
        kb_id: int
    ) -> List[Dict]:
        """æŸ¥æ‰¾å®ä½“çš„ä¸‰å…ƒç»„"""
        triples = []
        
        for entity in entities:
            # æŸ¥è¯¢æ•°æ®åº“
            entity_triples = self.graph_service.get_entity_triples(
                kb_id=kb_id,
                entity=entity
            )
            triples.extend(entity_triples)
        
        return triples
    
    def _expand_entities(
        self,
        triples: List[Dict],
        seed_entities: Set[str]
    ) -> Set[str]:
        """æ‰©å±•ç›¸å…³å®ä½“ï¼ˆ1è·³ï¼‰"""
        related = set()
        
        for triple in triples:
            head = triple["head_entity"]
            tail = triple["tail_entity"]
            
            # å¦‚æœå¤´å®ä½“æ˜¯ç§å­ï¼Œæ·»åŠ å°¾å®ä½“
            if head in seed_entities:
                related.add(tail)
            
            # å¦‚æœå°¾å®ä½“æ˜¯ç§å­ï¼Œæ·»åŠ å¤´å®ä½“
            if tail in seed_entities:
                related.add(head)
        
        return related - seed_entities  # æ’é™¤ç§å­å®ä½“
    
    async def _collect_documents(
        self,
        entities: Set[str],
        kb_id: int
    ) -> List[Document]:
        """æ”¶é›†åŒ…å«å®ä½“çš„æ–‡æ¡£"""
        from app.services.database_service import DatabaseService
        
        db = DatabaseService()
        documents = []
        
        for entity in entities:
            # æŸ¥è¯¢åŒ…å«è¯¥å®ä½“çš„æ–‡æ¡£å—
            chunks = db.search_chunks_by_entity(kb_id, entity)
            
            for chunk in chunks:
                documents.append(Document(
                    content=chunk['content'],
                    metadata={
                        **chunk['metadata'],
                        "matched_entity": entity
                    },
                    score=0.8,  # å›ºå®šåˆ†æ•°
                    doc_id=str(chunk['id'])
                ))
        
        # å»é‡
        seen = set()
        unique_docs = []
        for doc in documents:
            if doc.doc_id not in seen:
                seen.add(doc.doc_id)
                unique_docs.append(doc)
        
        return unique_docs
    
    def get_retriever_info(self):
        return {
            "type": "graph",
            "config": {
                "top_k": self.config.top_k,
                "expand_hops": 1
            }
        }
```

---

### Week 7æ€»ç»“

**å®Œæˆå†…å®¹**ï¼š
- âœ… HybridRetrieverï¼ˆRRFèåˆç®—æ³•ï¼‰
- âœ… GraphRetrieverï¼ˆçŸ¥è¯†å›¾è°±æ‰©å±•ï¼‰
- âœ… 4ç§æ£€ç´¢ç­–ç•¥å…¨éƒ¨å®ç°

**æ£€ç´¢ç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|
| Vector | è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ | ç†è§£è¯­ä¹‰ | å¬å›ç²¾ç¡®è¯å›°éš¾ |
| BM25 | å…³é”®è¯ç²¾ç¡®åŒ¹é… | é€Ÿåº¦å¿« | ä¸ç†è§£è¯­ä¹‰ |
| Hybrid | é€šç”¨åœºæ™¯ | å…¼é¡¾è¯­ä¹‰+å…³é”®è¯ | å¤æ‚åº¦é«˜ |
| Graph | éœ€è¦å…³ç³»æ¨ç† | å¯æ‰©å±•å®ä½“ | ä¾èµ–å›¾è°±è´¨é‡ |

---

### Week 8: çŸ¥è¯†åº“æœåŠ¡é‡æ„

#### T3.10 æ‹†åˆ†knowledge_base_serviceï¼ˆ3å¤©ï¼‰

**å½“å‰é—®é¢˜**ï¼š
`knowledge_base_service.py`ï¼ˆ528è¡Œï¼‰åŒ…å«4ä¸ªèŒè´£ï¼š
1. CRUDæ“ä½œï¼ˆåˆ›å»º/åˆ é™¤çŸ¥è¯†åº“ï¼‰
2. æ–‡æ¡£ç®¡ç†ï¼ˆä¸Šä¼ /åˆ†å—ï¼‰
3. æ£€ç´¢é€»è¾‘ï¼ˆå‘é‡æœç´¢ï¼‰
4. çŸ¥è¯†å›¾è°±æ“ä½œ

**é‡æ„æ–¹æ¡ˆ**ï¼šèŒè´£åˆ†ç¦»

```python
# Backend/app/services/knowledge/knowledge_base_service.pyï¼ˆé‡æ„åï¼Œ220è¡Œï¼‰
class KnowledgeBaseService:
    """çŸ¥è¯†åº“æœåŠ¡ï¼ˆä»…è´Ÿè´£CRUD + æ£€ç´¢ç­–ç•¥é€‰æ‹©ï¼‰"""
    
    def __init__(self):
        self.db = DatabaseService()
        self.retrievers = self._init_retrievers()
    
    def _init_retrievers(self) -> dict:
        """åˆå§‹åŒ–æ‰€æœ‰æ£€ç´¢å™¨"""
        config = RetrievalConfig(top_k=5, score_threshold=0.6)
        
        embedding_service = EmbeddingService()
        vector_retriever = VectorRetriever(config, embedding_service)
        bm25_retriever = BM25Retriever(config)
        
        return {
            "vector": vector_retriever,
            "bm25": bm25_retriever,
            "hybrid": HybridRetriever(config, vector_retriever, bm25_retriever),
            "graph": GraphRetriever(config, GraphService())
        }
    
    async def create_knowledge_base(self, name: str, description: str) -> int:
        """åˆ›å»ºçŸ¥è¯†åº“"""
        kb_id = self.db.insert_knowledge_base(name, description)
        vector_store = VectorStoreService()
        vector_store.create_collection(f"kb_{kb_id}")
        return kb_id
    
    async def search(
        self,
        query: str,
        kb_id: int,
        strategy: str = "hybrid",
        top_k: int = 5
    ) -> List[dict]:
        """ç»Ÿä¸€æ£€ç´¢å…¥å£"""
        retriever = self.retrievers.get(strategy)
        if not retriever:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€ç´¢ç­–ç•¥: {strategy}")
        
        retriever.config.top_k = top_k
        documents = await retriever.retrieve(query, kb_id)
        
        return [
            {
                "content": doc.content,
                "metadata": doc.metadata,
                "score": doc.score,
                "doc_id": doc.doc_id
            }
            for doc in documents
        ]
```

---

#### T3.11 åˆ›å»ºdocument_serviceï¼ˆ1å¤©ï¼‰

```python
# Backend/app/services/knowledge/document_service.pyï¼ˆæ–°å¢ï¼Œ200è¡Œï¼‰
class DocumentService:
    """æ–‡æ¡£ç®¡ç†æœåŠ¡ï¼ˆä¸Šä¼ /åˆ†å—/å‘é‡åŒ–ï¼‰"""
    
    def __init__(self):
        self.db = DatabaseService()
        self.vector_store = VectorStoreService()
        self.embedding = EmbeddingService()
        self.text_splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)
    
    async def upload_document(
        self,
        kb_id: int,
        file_path: Path,
        metadata: dict = None
    ) -> int:
        """ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        # 1. ä¿å­˜æ–‡æ¡£è®°å½•
        doc_id = self.db.insert_document(
            kb_id=kb_id,
            filename=file_path.name,
            filepath=str(file_path),
            metadata=metadata or {}
        )
        
        # 2. è¯»å–æ–‡æœ¬
        text = self._read_file(file_path)
        
        # 3. æ–‡æœ¬åˆ†å—
        chunks = self.text_splitter.split(text)
        
        # 4. å‘é‡åŒ–å¹¶å­˜å‚¨
        await self._store_chunks(kb_id, doc_id, chunks, metadata)
        
        return doc_id
```

---

#### T3.12 ä¼˜åŒ–fileå’ŒmetadataæœåŠ¡ï¼ˆ1å¤©ï¼‰

**å½“å‰é—®é¢˜**ï¼š
- `file_service.py`ï¼ˆ301è¡Œï¼‰ï¼šåŒ…å«å†—ä½™çš„æ–‡ä»¶éªŒè¯é€»è¾‘
- `metadata_service.py`ï¼ˆ128è¡Œï¼‰ï¼šç›¸å¯¹ç®€å•ï¼Œä¿æŒä¸å˜

**é‡æ„æ–¹æ¡ˆ**ï¼šç®€åŒ–file_service

```python
# Backend/app/services/storage/file_service.pyï¼ˆç®€åŒ–åï¼Œ280è¡Œï¼‰
from pathlib import Path
from app.core.utils.path_resolver import PathResolver

class FileService:
    """æ–‡ä»¶å­˜å‚¨æœåŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.path_resolver = PathResolver()
        self.allowed_extensions = {'.txt', '.md', '.pdf', '.docx'}
    
    async def upload(self, kb_id: int, file: UploadFile) -> str:
        """ä¸Šä¼ æ–‡ä»¶"""
        # 1. éªŒè¯æ–‡ä»¶ç±»å‹ï¼ˆä½¿ç”¨å·¥å…·ç±»ï¼‰
        if not self._is_allowed(file.filename):
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}")
        
        # 2. ç”Ÿæˆä¿å­˜è·¯å¾„ï¼ˆä½¿ç”¨PathResolverï¼‰
        save_path = self.path_resolver.get_upload_path(kb_id, file.filename)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        # 3. ä¿å­˜æ–‡ä»¶
        with open(save_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        return str(save_path)
    
    def _is_allowed(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶ç±»å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return Path(filename).suffix.lower() in self.allowed_extensions
```

**ä¼˜åŒ–æ•ˆæœ**ï¼š
- 301è¡Œ â†’ 280è¡Œï¼ˆ-7%ï¼‰
- ä½¿ç”¨PathResolverç»Ÿä¸€è·¯å¾„ç®¡ç†

---

## å…­ã€é˜¶æ®µ4ï¼šåº”ç”¨å±‚ (Week 9-10)

### ç›®æ ‡

ğŸ¯ é‡æ„chat_serviceï¼ˆ624è¡Œ â†’ 400è¡Œï¼‰  
ğŸ¯ æå–RAG Pipelineç‹¬ç«‹æ¨¡å—  
ğŸ¯ ä¼˜åŒ–AgentæœåŠ¡

### Week 9: RAG Pipelineé‡æ„

#### T4.1 æå–RAG Pipelineï¼ˆ3å¤©ï¼‰

**å½“å‰é—®é¢˜**ï¼š`chat_service.py`æ··åˆäº†å¯¹è¯ç®¡ç†ã€RAGé€»è¾‘ã€æµå¼è¾“å‡º

**æ–°æ¶æ„**ï¼š

```python
# Backend/app/core/rag/rag_pipeline.py
from typing import List, Dict, Optional, AsyncGenerator
from app.core.llm.base_llm import BaseLLM, Message
from app.core.retrieval.base_retriever import BaseRetriever

class RAGPipeline:
    """RAGå¤„ç†æµæ°´çº¿"""
    
    def __init__(
        self,
        llm: BaseLLM,
        retriever: BaseRetriever,
        prompt_template: str = None
    ):
        self.llm = llm
        self.retriever = retriever
        self.prompt_template = prompt_template or self._default_template()
    
    async def generate(
        self,
        query: str,
        kb_id: int,
        chat_history: List[Message] = None,
        stream: bool = False,
        **kwargs
    ):
        """
        RAGç”Ÿæˆæµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            kb_id: çŸ¥è¯†åº“ID
            chat_history: å¯¹è¯å†å²
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœï¼ˆå­—ç¬¦ä¸²æˆ–æµï¼‰
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        documents = await self.retriever.retrieve(query, kb_id)
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(documents)
        
        # 3. æ„å»ºæç¤ºè¯
        messages = self._build_messages(query, context, chat_history)
        
        # 4. ç”Ÿæˆå›å¤
        response = await self.llm.generate(messages, stream=stream, **kwargs)
        
        # 5. è¿”å›ï¼ˆé™„å¸¦å¼•ç”¨ï¼‰
        if stream:
            return self._stream_with_citations(response, documents)
        else:
            return {
                "answer": response,
                "citations": self._format_citations(documents)
            }
    
    def _build_context(self, documents: List) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not documents:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"[{i}] {doc.content}")
        
        return "\n\n".join(context_parts)
    
    def _build_messages(
        self,
        query: str,
        context: str,
        chat_history: List[Message] = None
    ) -> List[Message]:
        """æ„å»ºå¯¹è¯æ¶ˆæ¯"""
        messages = []
        
        # æ·»åŠ å†å²å¯¹è¯
        if chat_history:
            messages.extend(chat_history)
        
        # æ·»åŠ å½“å‰æŸ¥è¯¢ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
        user_message = self.prompt_template.format(
            context=context,
            question=query
        )
        messages.append(Message(role="user", content=user_message))
        
        return messages
    
    async def _stream_with_citations(
        self,
        response_stream: AsyncGenerator,
        documents: List
    ) -> AsyncGenerator:
        """æµå¼è¾“å‡ºï¼ˆå…ˆè¾“å‡ºç­”æ¡ˆï¼Œæœ€åé™„å¸¦å¼•ç”¨ï¼‰"""
        # 1. æµå¼è¾“å‡ºç­”æ¡ˆ
        async for chunk in response_stream:
            yield chunk
        
        # 2. è¾“å‡ºå¼•ç”¨
        citations = self._format_citations(documents)
        yield f"\n\n---\nå‚è€ƒæ¥æº:\n{citations}"
    
    def _format_citations(self, documents: List) -> str:
        """æ ¼å¼åŒ–å¼•ç”¨"""
        citations = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get('source', 'unknown')
            citations.append(f"[{i}] {source} (ç›¸å…³åº¦: {doc.score:.2f})")
        
        return "\n".join(citations)
    
    def _default_template(self) -> str:
        """é»˜è®¤æç¤ºè¯æ¨¡æ¿"""
        return """è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”:"""
```

**é‡æ„åçš„ChatService**ï¼š

```python
# Backend/app/services/chat_service.py (é‡æ„å)
from typing import List, Dict, Optional
from app.core.rag.rag_pipeline import RAGPipeline
from app.core.retrieval.retriever_factory import RetrieverFactory
from app.services.llm_service import LLMService

class ChatService:
    """å¯¹è¯æœåŠ¡ï¼ˆä»…è´Ÿè´£ä¼šè¯ç®¡ç†ï¼‰"""
    
    def __init__(self):
        self.db = DatabaseService()
        self.llm_service = LLMService()
    
    async def chat(
        self,
        session_id: int,
        message: str,
        kb_id: Optional[int] = None,
        strategy: str = "hybrid",
        stream: bool = False
    ):
        """
        å¯¹è¯æ¥å£
        
        Args:
            session_id: ä¼šè¯ID
            message: ç”¨æˆ·æ¶ˆæ¯
            kb_id: çŸ¥è¯†åº“IDï¼ˆNoneè¡¨ç¤ºæ™®é€šå¯¹è¯ï¼‰
            strategy: æ£€ç´¢ç­–ç•¥
            stream: æ˜¯å¦æµå¼è¾“å‡º
        """
        # 1. è·å–å¯¹è¯å†å²
        chat_history = self._get_chat_history(session_id)
        
        # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        self.db.insert_message(session_id, "user", message)
        
        # 3. é€‰æ‹©æ¨¡å¼
        if kb_id:
            # RAGæ¨¡å¼
            response = await self._rag_chat(
                message, kb_id, chat_history, strategy, stream
            )
        else:
            # æ™®é€šå¯¹è¯
            response = await self._normal_chat(
                message, chat_history, stream
            )
        
        # 4. ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯ï¼ˆéæµå¼ï¼‰
        if not stream:
            self.db.insert_message(session_id, "assistant", response["answer"])
        
        return response
    
    async def _rag_chat(
        self,
        message: str,
        kb_id: int,
        chat_history: List,
        strategy: str,
        stream: bool
    ):
        """RAGå¯¹è¯"""
        # 1. è·å–LLM
        llm = await self.llm_service.get_llm("transformers", "Qwen2.5-1.5B")
        
        # 2. åˆ›å»ºæ£€ç´¢å™¨
        config = RetrievalConfig(top_k=5)
        retriever = RetrieverFactory.create(
            strategy=strategy,
            config=config,
            embedding_service=EmbeddingService()
        )
        
        # 3. åˆ›å»ºRAG Pipeline
        pipeline = RAGPipeline(llm, retriever)
        
        # 4. ç”Ÿæˆå›å¤
        return await pipeline.generate(
            query=message,
            kb_id=kb_id,
            chat_history=chat_history,
            stream=stream
        )
    
    async def _normal_chat(
        self,
        message: str,
        chat_history: List,
        stream: bool
    ):
        """æ™®é€šå¯¹è¯ï¼ˆæ— RAGï¼‰"""
        llm = await self.llm_service.get_llm("ollama", "qwen2.5:latest")
        
        messages = chat_history + [Message(role="user", content=message)]
        response = await llm.generate(messages, stream=stream)
        
        if stream:
            return response  # æµå¼ç”Ÿæˆå™¨
        else:
            return {"answer": response, "citations": []}
    
    def _get_chat_history(self, session_id: int, limit: int = 10) -> List:
        """è·å–å¯¹è¯å†å²"""
        messages = self.db.get_session_messages(session_id, limit=limit)
        return [
            Message(role=m['role'], content=m['content'])
            for m in messages
        ]
```

---

### Week 10: AgentæœåŠ¡ä¼˜åŒ–

#### T4.2 ä¼˜åŒ–Agentå·¥å…·è°ƒç”¨ï¼ˆ2å¤©ï¼‰

**å½“å‰é—®é¢˜**ï¼š`agent_service.py`å·¥å…·æ³¨å†Œæ··ä¹±ï¼Œç¼ºå°‘ç»Ÿä¸€ç®¡ç†

**æ–°æ¶æ„**ï¼š

```python
# Backend/app/core/agent/tool_registry.py
from typing import Dict, Callable, Any
from dataclasses import dataclass

@dataclass
class ToolDefinition:
    """å·¥å…·å®šä¹‰"""
    name: str
    description: str
    parameters: Dict[str, Any]
    function: Callable

class ToolRegistry:
    """å·¥å…·æ³¨å†Œè¡¨ï¼ˆé›†ä¸­ç®¡ç†Agentå·¥å…·ï¼‰"""
    
    def __init__(self):
        self.tools: Dict[str, ToolDefinition] = {}
    
    def register(
        self,
        name: str,
        description: str,
        parameters: Dict[str, Any]
    ):
        """æ³¨å†Œå·¥å…·ï¼ˆè£…é¥°å™¨æ¨¡å¼ï¼‰"""
        def decorator(func: Callable):
            self.tools[name] = ToolDefinition(
                name=name,
                description=description,
                parameters=parameters,
                function=func
            )
            return func
        return decorator
    
    def get_tool(self, name: str) -> ToolDefinition:
        """è·å–å·¥å…·"""
        return self.tools.get(name)
    
    def list_tools(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å·¥å…·ï¼ˆOpenAIå‡½æ•°è°ƒç”¨æ ¼å¼ï¼‰"""
        return [
            {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.parameters
            }
            for tool in self.tools.values()
        ]
```

**å·¥å…·æ³¨å†Œç¤ºä¾‹**ï¼š

```python
# Backend/app/core/agent/builtin_tools.py
from app.core.agent.tool_registry import ToolRegistry

registry = ToolRegistry()

@registry.register(
    name="knowledge_search",
    description="åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ä¿¡æ¯",
    parameters={
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "æœç´¢æŸ¥è¯¢"},
            "kb_id": {"type": "integer", "description": "çŸ¥è¯†åº“ID"}
        },
        "required": ["query", "kb_id"]
    }
)
async def knowledge_search(query: str, kb_id: int):
    """çŸ¥è¯†åº“æœç´¢å·¥å…·"""
    kb_service = KnowledgeBaseService()
    results = await kb_service.search(query, kb_id, strategy="hybrid")
    return results[:3]  # è¿”å›å‰3æ¡

@registry.register(
    name="web_search",
    description="åœ¨äº’è”ç½‘ä¸Šæœç´¢æœ€æ–°ä¿¡æ¯",
    parameters={
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "æœç´¢æŸ¥è¯¢"}
        },
        "required": ["query"]
    }
)
async def web_search(query: str):
    """ç½‘ç»œæœç´¢å·¥å…·"""
    # TODO: é›†æˆæœç´¢API
    return [{"title": "ç¤ºä¾‹ç»“æœ", "url": "https://example.com"}]
```

---

### é˜¶æ®µ4æ€»ç»“

**å®Œæˆæ ‡å‡†**ï¼š
- âœ… RAG Pipelineç‹¬ç«‹æ¨¡å—æµ‹è¯•é€šè¿‡
- âœ… chat_serviceé‡æ„å®Œæˆï¼ˆ624è¡Œ â†’ 400è¡Œï¼‰
- âœ… Agentå·¥å…·æ³¨å†Œè¡¨å®ç°
- âœ… ç«¯åˆ°ç«¯RAGæµç¨‹æµ‹è¯•é€šè¿‡

**é¢„æœŸæ•ˆæœ**ï¼š
- â¬‡ï¸ chat_service: 624è¡Œ â†’ 400è¡Œ (-36%)
- â¬†ï¸ RAGå¤ç”¨æ€§: Pipelineå¯ç”¨äºå¤šç§åœºæ™¯
- â¬†ï¸ Agentæ‰©å±•æ€§: æ–°å¢å·¥å…·åªéœ€è£…é¥°å™¨æ³¨å†Œ

---

## ä¸ƒã€é˜¶æ®µ5ï¼šæ¸…ç†ä¸ä¼˜åŒ– (Week 11-12)

### ç›®æ ‡
ğŸ¯ åˆ é™¤æ—§ä»£ç å’Œé‡å¤é€»è¾‘  
ğŸ¯ æ€§èƒ½ä¼˜åŒ–ä¸æµ‹è¯•  
ğŸ¯ æ–‡æ¡£è¡¥å…¨

### Week 11: ä»£ç æ¸…ç†

#### T5.1 åˆ é™¤åºŸå¼ƒæœåŠ¡ï¼ˆ2å¤©ï¼‰

**å¾…åˆ é™¤æ–‡ä»¶**ï¼ˆå·²è¢«æ–°æ¶æ„æ›¿ä»£ï¼‰ï¼š
- `transformers_service.py`ï¼ˆ835è¡Œï¼‰â†’ æ›¿æ¢ä¸º`TransformersLLM`
- `ollama_service.py`ï¼ˆéƒ¨åˆ†é€»è¾‘ï¼‰â†’ æ›¿æ¢ä¸º`OllamaLLM`
- æ—§çš„æ£€ç´¢é€»è¾‘ï¼ˆåˆ†æ•£åœ¨3ä¸ªæ–‡ä»¶ï¼‰â†’ æ›¿æ¢ä¸ºRetrieverç³»ç»Ÿ

**æ¸…ç†æ£€æŸ¥æ¸…å•**ï¼š
```bash
# 1. ç¡®è®¤æ‰€æœ‰APIç«¯ç‚¹å·²æ›´æ–°
grep -r "TransformersService" app/api/

# 2. ç¡®è®¤æµ‹è¯•å…¨éƒ¨é€šè¿‡
pytest app/tests/ -v

# 3. åˆ é™¤æ—§æ–‡ä»¶
git rm app/services/transformers_service.py

# 4. æäº¤æ¸…ç†
git commit -m "refactor: remove deprecated services"
```

---

#### T5.2 æ€§èƒ½ä¼˜åŒ–ï¼ˆ3å¤©ï¼‰

**ä¼˜åŒ–é¡¹æ¸…å•**ï¼š

1. **æ¨¡å‹ç¼“å­˜ä¼˜åŒ–**
```python
# Backend/app/services/llm_service.py
class LLMService:
    def __init__(self):
        self.llm_cache = {}  # æ·»åŠ LRUç¼“å­˜
        self.max_cache_size = 3  # æœ€å¤šç¼“å­˜3ä¸ªæ¨¡å‹
    
    async def get_llm(self, model_type, model_name, **kwargs):
        cache_key = f"{model_type}:{model_name}"
        
        # ç¼“å­˜å‘½ä¸­
        if cache_key in self.llm_cache:
            return self.llm_cache[cache_key]
        
        # ç¼“å­˜æ»¡ï¼Œç§»é™¤æœ€æ—§çš„
        if len(self.llm_cache) >= self.max_cache_size:
            oldest_key = next(iter(self.llm_cache))
            await self.llm_cache[oldest_key].cleanup()
            del self.llm_cache[oldest_key]
        
        # åŠ è½½æ–°æ¨¡å‹...
```

2. **æ£€ç´¢æ€§èƒ½ä¼˜åŒ–**
- ä¸ºBM25ç´¢å¼•æ·»åŠ ç£ç›˜ç¼“å­˜ï¼ˆpickleï¼‰
- å‘é‡æ£€ç´¢æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–
- æ··åˆæ£€ç´¢å¹¶è¡ŒåŒ–

3. **å†…å­˜ç®¡ç†**
```python
# Backend/app/core/device/gpu_manager.py
class DeviceManager:
    def optimize_memory(self):
        """å†…å­˜ä¼˜åŒ–ç­–ç•¥"""
        if self.device == "cuda":
            # æ¸…ç†ç¢ç‰‡
            torch.cuda.empty_cache()
            
            # å‹ç¼©å†…å­˜
            torch.cuda.memory.empty_cache()
            
            # é‡ç½®å³°å€¼ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
```

---

### Week 12: æµ‹è¯•ä¸æ–‡æ¡£

#### T5.3 å®Œå–„æµ‹è¯•è¦†ç›–ï¼ˆ2å¤©ï¼‰

**æµ‹è¯•ç›®æ ‡**ï¼š
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡: 30% â†’ 80%
- é›†æˆæµ‹è¯•: æ ¸å¿ƒæµç¨‹å…¨è¦†ç›–
- æ€§èƒ½åŸºå‡†æµ‹è¯•

**æµ‹è¯•æ¸…å•**ï¼š
```bash
# è¿è¡Œå…¨éƒ¨æµ‹è¯•
pytest app/tests/ --cov=app --cov-report=html

# æ ¸å¿ƒæ¨¡å—è¦†ç›–ç‡æ£€æŸ¥
pytest app/tests/test_llm/ --cov=app/core/llm --cov-report=term-missing

# æ€§èƒ½åŸºå‡†æµ‹è¯•
python benchmark/rag_latency.py  # RAGç«¯åˆ°ç«¯å»¶è¿Ÿ
python benchmark/retrieval_speed.py  # æ£€ç´¢é€Ÿåº¦
```

---

#### T5.4 æ–‡æ¡£è¡¥å…¨ï¼ˆ1å¤©ï¼‰

**æ–‡æ¡£æ¸…å•**ï¼š
1. **APIæ–‡æ¡£æ›´æ–°**ï¼ˆSwaggeræ³¨é‡Šï¼‰
2. **æ¶æ„å›¾æ›´æ–°**ï¼ˆç»˜åˆ¶æ–°çš„4å±‚æ¶æ„å›¾ï¼‰
3. **è¿ç§»æŒ‡å—**ï¼ˆæ—§API â†’ æ–°APIå¯¹ç…§è¡¨ï¼‰
4. **æ€§èƒ½æŠ¥å‘Š**ï¼ˆé‡æ„å‰åå¯¹æ¯”ï¼‰

**ç¤ºä¾‹è¿ç§»æŒ‡å—**ï¼š
```markdown
# APIè¿ç§»æŒ‡å—

## æ—§ç‰ˆ â†’ æ–°ç‰ˆå¯¹ç…§

### 1. å¯¹è¯æ¥å£
**æ—§ç‰ˆ**:
```python
POST /api/chat
{
    "message": "ä½ å¥½",
    "model": "transformers",
    "kb_id": 1
}
```

**æ–°ç‰ˆ**:
```python
POST /api/chat
{
    "session_id": 123,
    "message": "ä½ å¥½",
    "kb_id": 1,
    "strategy": "hybrid"  # æ–°å¢ï¼šæ£€ç´¢ç­–ç•¥
}
```

### 2. æ¨¡å‹ç®¡ç†
**æ—§ç‰ˆ**: `/api/transformers/load_model`  
**æ–°ç‰ˆ**: `/api/llm/load` ï¼ˆç»Ÿä¸€æ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
```

---

### é˜¶æ®µ5æ€»ç»“

**å®Œæˆæ ‡å‡†**ï¼š
- âœ… æ‰€æœ‰æ—§ä»£ç å·²åˆ é™¤
- âœ… æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°80%
- âœ… æ€§èƒ½åŸºå‡†æ— ä¸‹é™ï¼ˆéƒ¨åˆ†æå‡ï¼‰
- âœ… æ–‡æ¡£å…¨éƒ¨æ›´æ–°

**æœ€ç»ˆæ•ˆæœç»Ÿè®¡**ï¼š

| æŒ‡æ ‡ | é‡æ„å‰ | é‡æ„å | æ”¹è¿› |
|------|--------|--------|------|
| ä»£ç æ€»é‡ | 6738è¡Œ | 4500è¡Œ | -33% |
| æœ€å¤§æ–‡ä»¶ | 835è¡Œ | 400è¡Œ | -52% |
| é‡å¤ç‡ | 25% | 8% | -68% |
| æµ‹è¯•è¦†ç›–ç‡ | 30% | 80% | +167% |
| æ¨¡å—æ•° | 18 | 25 | +39% (æ‹†åˆ†å) |
| å¹³å‡æ–‡ä»¶å¤§å° | 374è¡Œ | 180è¡Œ | -52% |

---

## é™„å½•

### A. é£é™©æ§åˆ¶

**å›æ»šç­–ç•¥**ï¼š
1. æ¯ä¸ªé˜¶æ®µå®Œæˆåæ‰“Gitæ ‡ç­¾ï¼ˆ`v2.0-stage1`ï¼‰
2. ä¿ç•™æ—§ä»£ç åˆ†æ”¯ï¼ˆ`legacy/service-layer-v1`ï¼‰
3. æ•°æ®åº“ä½¿ç”¨è¿ç§»è„šæœ¬ï¼ˆæ”¯æŒå›æ»šï¼‰

**ç°åº¦å‘å¸ƒ**ï¼š
1. æ–°æ—§APIå¹¶è¡Œè¿è¡Œ2å‘¨
2. é€æ­¥åˆ‡æ¢æµé‡ï¼ˆ10% â†’ 50% â†’ 100%ï¼‰
3. ç›‘æ§é”™è¯¯ç‡å’Œæ€§èƒ½æŒ‡æ ‡

**åº”æ€¥é¢„æ¡ˆ**ï¼š
- P0æ•…éšœï¼šç«‹å³å›æ»šåˆ°ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬
- P1æ•…éšœï¼š24å°æ—¶å†…ä¿®å¤æˆ–å›æ»š
- P2æ•…éšœï¼šä¸€å‘¨å†…ä¿®å¤

---

### B. éªŒæ”¶æ ‡å‡†

**åŠŸèƒ½éªŒæ”¶**ï¼š
- [ ] æ‰€æœ‰ç°æœ‰APIåŠŸèƒ½æ­£å¸¸
- [ ] æ–°å¢4ç§æ£€ç´¢ç­–ç•¥å¯ç”¨
- [ ] RAGæµç¨‹å®Œæ•´å¯ç”¨
- [ ] Agentå·¥å…·è°ƒç”¨æ­£å¸¸

**æ€§èƒ½éªŒæ”¶**ï¼š
- [ ] LLMæ¨ç†é€Ÿåº¦æ— ä¸‹é™ï¼ˆÂ±5%ä»¥å†…ï¼‰
- [ ] æ£€ç´¢å»¶è¿Ÿæ— ä¸‹é™ï¼ˆÂ±10%ä»¥å†…ï¼‰
- [ ] å†…å­˜å ç”¨æ— æ˜æ˜¾å¢åŠ ï¼ˆÂ±20%ä»¥å†…ï¼‰

**è´¨é‡éªŒæ”¶**ï¼š
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
- [ ] é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] ä»£ç å®¡æŸ¥é€šè¿‡ï¼ˆPylintè¯„åˆ† â‰¥ 8.0ï¼‰
- [ ] æ–‡æ¡£å®Œæ•´åº¦ â‰¥ 90%

---

## é™„å½•Cï¼šé‡æ„å‰åæ–‡ä»¶ç»“æ„å¯¹æ¯”

### å½“å‰ç»“æ„ï¼ˆé‡æ„å‰ï¼‰

```
Backend/app/
â”œâ”€â”€ api/                          # APIè·¯ç”±å±‚
â”œâ”€â”€ core/                         # æ ¸å¿ƒé…ç½®ï¼ˆä»…4ä¸ªæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ dependencies.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/                       # æ•°æ®æ¨¡å‹
â”œâ”€â”€ services/                     # æœåŠ¡å±‚ï¼ˆ18ä¸ªæ–‡ä»¶ï¼Œ6118è¡Œï¼‰
â”‚   â”œâ”€â”€ agent_service.py          # 316è¡Œ - Agenté€»è¾‘
â”‚   â”œâ”€â”€ chat_service.py           # 561è¡Œ ğŸ”´ - å¯¹è¯+RAG+æµå¼
â”‚   â”œâ”€â”€ embedding_service.py      # 334è¡Œ - Embeddingæ¨¡å‹
â”‚   â”œâ”€â”€ entity_extraction_service.py # 337è¡Œ - å®ä½“æå–
â”‚   â”œâ”€â”€ file_service.py           # 301è¡Œ - æ–‡ä»¶ä¸Šä¼ 
â”‚   â”œâ”€â”€ hybrid_retrieval_service.py # 374è¡Œ - æ··åˆæ£€ç´¢
â”‚   â”œâ”€â”€ knowledge_base_service.py # 528è¡Œ ğŸŸ¡ - CRUD+æ£€ç´¢
â”‚   â”œâ”€â”€ llama_factory_service.py  # 243è¡Œ - LLaMA Factoryé›†æˆ
â”‚   â”œâ”€â”€ lora_scanner_service.py   # 393è¡Œ - LoRAæ‰«æ
â”‚   â”œâ”€â”€ metadata_service.py       # 128è¡Œ - å…ƒæ•°æ®ç®¡ç†
â”‚   â”œâ”€â”€ model_manager.py          # 214è¡Œ - æ¨¡å‹ç®¡ç†
â”‚   â”œâ”€â”€ model_scanner.py          # 344è¡Œ - æ¨¡å‹æ‰«æ
â”‚   â”œâ”€â”€ neo4j_graph_service.py    # 513è¡Œ ğŸŸ¡ - çŸ¥è¯†å›¾è°±
â”‚   â”œâ”€â”€ ollama_embedding_service.py # 204è¡Œ - Ollama Embedding
â”‚   â”œâ”€â”€ ollama_llm_service.py     # 265è¡Œ - Ollama LLM
â”‚   â”œâ”€â”€ simple_lora_trainer.py    # 500è¡Œ ğŸŸ¡ - LoRAè®­ç»ƒ
â”‚   â”œâ”€â”€ transformers_service.py   # 776è¡Œ ğŸ”´ - Transformersæ¨ç†
â”‚   â””â”€â”€ vector_store_service.py   # 287è¡Œ - å‘é‡æ•°æ®åº“
â”œâ”€â”€ utils/                        # å·¥å…·å‡½æ•°ï¼ˆè¾ƒå°‘ï¼‰
â””â”€â”€ websocket/                    # WebSocketå¤„ç†
```

**å½“å‰é—®é¢˜**ï¼š
- âŒ `core/`ç›®å½•å‡ ä¹ä¸ºç©ºï¼Œç¼ºå°‘åŸºç¡€è®¾æ–½
- âŒ `services/`æ‰¿æ‹…äº†æ‰€æœ‰é€»è¾‘ï¼ŒèŒè´£ä¸æ¸…
- âŒ è®¾å¤‡ç®¡ç†ã€æ¨¡å‹åŠ è½½ç­‰åŸºç¡€åŠŸèƒ½åœ¨4ä¸ªæ–‡ä»¶ä¸­é‡å¤
- âŒ ç¼ºå°‘ç»Ÿä¸€çš„LLMæŠ½è±¡å±‚
- âŒ æ£€ç´¢ç­–ç•¥åˆ†æ•£ï¼Œæ— ç»Ÿä¸€æ¥å£

---

### é‡æ„åç»“æ„ï¼ˆç›®æ ‡ï¼‰

```
Backend/app/
â”œâ”€â”€ api/                          # APIè·¯ç”±å±‚ï¼ˆä¸å˜ï¼‰
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ chat.py
â”‚   â”œâ”€â”€ knowledge_base.py
â”‚   â””â”€â”€ models.py
â”‚
â”œâ”€â”€ core/                         # æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ database.py               # æ•°æ®åº“è¿æ¥
â”‚   â”œâ”€â”€ dependencies.py           # ä¾èµ–æ³¨å…¥
â”‚   â”‚
â”‚   â”œâ”€â”€ device/                   # è®¾å¤‡ç®¡ç†ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ gpu_manager.py        # 80è¡Œ - ç»Ÿä¸€CUDAç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                    # æ¨¡å‹åŠ è½½ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_loader.py       # 200è¡Œ - ç»Ÿä¸€æ¨¡å‹åŠ è½½
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                      # LLMæŠ½è±¡å±‚ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_llm.py           # 100è¡Œ - åŸºç±»æ¥å£
â”‚   â”‚   â”œâ”€â”€ ollama_llm.py         # 150è¡Œ - Ollamaå®ç°
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ transformers/         # Transformerså®ç°
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ prompt_builder.py      # 150è¡Œ - æç¤ºè¯æ„å»º
â”‚   â”‚       â”œâ”€â”€ response_processor.py  # 80è¡Œ - å“åº”å¤„ç†
â”‚   â”‚       â”œâ”€â”€ lora_adapter.py        # 120è¡Œ - LoRAç®¡ç†
â”‚   â”‚       â””â”€â”€ transformers_llm.py    # 280è¡Œ - ä¸»åè°ƒå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                # æ£€ç´¢ç­–ç•¥ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_retriever.py     # 80è¡Œ - åŸºç±»æ¥å£
â”‚   â”‚   â”œâ”€â”€ vector_retriever.py   # 120è¡Œ - å‘é‡æ£€ç´¢
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py     # 150è¡Œ - BM25å…¨æ–‡æ£€ç´¢
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py   # 180è¡Œ - æ··åˆæ£€ç´¢ï¼ˆRRFï¼‰
â”‚   â”‚   â”œâ”€â”€ graph_retriever.py    # 200è¡Œ - çŸ¥è¯†å›¾è°±æ£€ç´¢
â”‚   â”‚   â””â”€â”€ retriever_factory.py  # 60è¡Œ - å·¥å‚æ¨¡å¼
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                      # RAGæµæ°´çº¿ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_pipeline.py       # 200è¡Œ - RAGæ ¸å¿ƒæµç¨‹
â”‚   â”‚
â”‚   â”œâ”€â”€ agent/                    # Agentå·¥å…·ï¼ˆæ–°å¢ï¼‰â­
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tool_registry.py      # 80è¡Œ - å·¥å…·æ³¨å†Œè¡¨
â”‚   â”‚   â””â”€â”€ builtin_tools.py      # 150è¡Œ - å†…ç½®å·¥å…·
â”‚   â”‚
â”‚   â””â”€â”€ utils/                    # æ ¸å¿ƒå·¥å…·ï¼ˆæ–°å¢ï¼‰â­
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ json_parser.py        # 60è¡Œ - JSONå®¹é”™è§£æ
â”‚       â”œâ”€â”€ path_resolver.py      # 80è¡Œ - è·¯å¾„ç®¡ç†
â”‚       â”œâ”€â”€ process_manager.py    # 120è¡Œ - è¿›ç¨‹ç®¡ç†
â”‚       â”œâ”€â”€ task_state_manager.py # 100è¡Œ - ä»»åŠ¡çŠ¶æ€æœº
â”‚       â””â”€â”€ text_splitter.py      # 150è¡Œ - æ–‡æœ¬åˆ†å‰²
â”‚
â”œâ”€â”€ models/                       # æ•°æ®æ¨¡å‹ï¼ˆä¸å˜ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ chat.py
â”‚   â””â”€â”€ knowledge_base.py
â”‚
â”œâ”€â”€ services/                     # æœåŠ¡å±‚ï¼ˆæŒ‰æ¨¡å—åˆ†ç±»ï¼‰â­
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                      # LLMæ¨¡å‹æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py        # 180è¡Œ - LLMç»Ÿä¸€å…¥å£ï¼ˆå·¥å‚æ¨¡å¼ï¼‰
â”‚   â”‚   â””â”€â”€ embedding_service.py  # 250è¡Œ - EmbeddingæœåŠ¡ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ chat/                     # å¯¹è¯æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat_service.py       # 250è¡Œ - å¯¹è¯ç®¡ç†ï¼ˆä¼šè¯+å†å²ï¼‰
â”‚   â”‚   â””â”€â”€ agent_service.py      # 200è¡Œ - Agentå¯¹è¯ï¼ˆå·¥å…·è°ƒç”¨ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge/                # çŸ¥è¯†åº“æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ knowledge_base_service.py  # 220è¡Œ - çŸ¥è¯†åº“CRUD
â”‚   â”‚   â”œâ”€â”€ document_service.py        # 200è¡Œ - æ–‡æ¡£ä¸Šä¼ /åˆ†å—
â”‚   â”‚   â”œâ”€â”€ vector_store_service.py    # 250è¡Œ - å‘é‡å­˜å‚¨ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ entity_extraction_service.py # 200è¡Œ - å®ä½“æå–ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                    # çŸ¥è¯†å›¾è°±æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ neo4j_service.py      # 350è¡Œ - Neo4jå›¾è°±æ“ä½œï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                 # æ¨¡å‹è®­ç»ƒæœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lora_trainer.py       # 350è¡Œ - LoRAè®­ç»ƒï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ llama_factory_service.py # 243è¡Œ - LLaMA Factoryé›†æˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ model_mgmt/               # æ¨¡å‹ç®¡ç†æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_scanner.py      # 350è¡Œ - ç»Ÿä¸€æ¨¡å‹æ‰«æå™¨ï¼ˆæ”¯æŒLLM/LoRA/Embeddingï¼‰â­
â”‚   â”‚   â””â”€â”€ deployment.py         # 180è¡Œ - æ¨¡å‹éƒ¨ç½²ç®¡ç†ï¼ˆæ–°å¢ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                  # å­˜å‚¨æœåŠ¡æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_service.py       # 280è¡Œ - æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½ï¼ˆç®€åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ metadata_service.py   # 128è¡Œ - å…ƒæ•°æ®ç®¡ç†
â”‚   â”‚
â”‚   â””â”€â”€ [å·²åˆ é™¤]                  # åºŸå¼ƒçš„æ—§æœåŠ¡
â”‚       â”œâ”€â”€ transformers_service.py    # 776è¡Œ â†’ åˆ é™¤ï¼ˆç§»è‡³core/llmï¼‰
â”‚       â”œâ”€â”€ ollama_llm_service.py      # 265è¡Œ â†’ åˆ é™¤ï¼ˆç§»è‡³core/llmï¼‰
â”‚       â”œâ”€â”€ ollama_embedding_service.py # 204è¡Œ â†’ åˆ é™¤ï¼ˆåˆå¹¶ï¼‰
â”‚       â”œâ”€â”€ hybrid_retrieval_service.py # 374è¡Œ â†’ åˆ é™¤ï¼ˆç§»è‡³core/retrievalï¼‰
â”‚       â”œâ”€â”€ model_manager.py           # 214è¡Œ â†’ åˆ é™¤ï¼ˆæ‹†åˆ†åˆ°model_mgmtï¼‰
â”‚       â””â”€â”€ simple_lora_trainer.py     # 500è¡Œ â†’ é‡å‘½åä¸ºlora_trainer.py
â”‚
â”œâ”€â”€ tests/                        # æµ‹è¯•ï¼ˆæ–°å¢/å®Œå–„ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒå±‚æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_device_manager.py
â”‚   â”‚   â”œâ”€â”€ test_model_loader.py
â”‚   â”‚   â”œâ”€â”€ test_base_llm.py
â”‚   â”‚   â”œâ”€â”€ test_transformers_llm.py
â”‚   â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â”‚   â””â”€â”€ test_rag_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                 # æœåŠ¡å±‚æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_llm_service.py
â”‚   â”‚   â”œâ”€â”€ test_chat_service.py
â”‚   â”‚   â”œâ”€â”€ test_knowledge_base_service.py
â”‚   â”‚   â””â”€â”€ test_agent_service.py
â”‚   â”‚
â”‚   â””â”€â”€ integration/              # é›†æˆæµ‹è¯•
â”‚       â”œâ”€â”€ test_rag_flow.py
â”‚       â”œâ”€â”€ test_hybrid_retrieval.py
â”‚       â””â”€â”€ test_agent_flow.py
â”‚
â”œâ”€â”€ utils/                        # ä¸šåŠ¡å·¥å…·ï¼ˆä¿ç•™ï¼‰
â””â”€â”€ websocket/                    # WebSocketï¼ˆä¸å˜ï¼‰
```

---

### é‡æ„æ•ˆæœå¯¹æ¯”

#### æœåŠ¡æ¨¡å—åˆ†ç±»è¯´æ˜

**æŒ‰ä¸šåŠ¡é¢†åŸŸåˆ†ä¸º7ä¸ªæ¨¡å—**ï¼š

| æ¨¡å— | èŒè´£ | æ–‡ä»¶æ•° | æ€»è¡Œæ•° |
|------|------|--------|--------|
| **llm/** | LLMæ¨¡å‹ç®¡ç†ï¼ˆå·¥å‚+åµŒå…¥ï¼‰ | 2 | ~430è¡Œ |
| **chat/** | å¯¹è¯æœåŠ¡ï¼ˆæ™®é€š+Agentï¼‰ | 2 | ~450è¡Œ |
| **knowledge/** | çŸ¥è¯†åº“ç®¡ç†ï¼ˆCRUD+æ–‡æ¡£+å‘é‡ï¼‰ | 5 | ~1120è¡Œ |
| **graph/** | çŸ¥è¯†å›¾è°±æ“ä½œ | 1 | ~350è¡Œ |
| **training/** | æ¨¡å‹è®­ç»ƒï¼ˆLoRA+é›†æˆï¼‰ | 2 | ~593è¡Œ |
| **model_mgmt/** | æ¨¡å‹æ‰«æ+éƒ¨ç½² | 2 | ~530è¡Œ |
| **storage/** | æ–‡ä»¶å­˜å‚¨+å…ƒæ•°æ® | 2 | ~408è¡Œ |
| **æ€»è®¡** | | **16ä¸ªæ–‡ä»¶** | **~3881è¡Œ** |

**æ¨¡å—ä¾èµ–å…³ç³»**ï¼š
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   chat/     â”‚ â† æœ€ä¸Šå±‚ï¼ˆä¾èµ–å…¶ä»–æ‰€æœ‰æ¨¡å—ï¼‰
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚       â”‚        â”‚         â”‚
â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”
â”‚ llm/ â”‚ â”‚know â”‚ â”‚ graph/ â”‚ â”‚train â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
          â”‚
      â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
   â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚store â”‚ â”‚model_mgmtâ”‚
   â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ä»£ç é‡å˜åŒ–

| å±‚çº§ | é‡æ„å‰ | é‡æ„å | å˜åŒ– |
|------|--------|--------|------|
| **core/** | 4ä¸ªæ–‡ä»¶ï¼Œ~200è¡Œ | 30ä¸ªæ–‡ä»¶ï¼Œ~2500è¡Œ | +2300è¡Œï¼ˆæ–°å¢åŸºç¡€è®¾æ–½ï¼‰ |
| **services/** | 18ä¸ªæ–‡ä»¶ï¼Œ6118è¡Œ | 16ä¸ªæ–‡ä»¶ï¼ˆ7æ¨¡å—ï¼‰ï¼Œ3881è¡Œ | -2237è¡Œï¼ˆ-37%ï¼‰ |
| **tests/** | ~500è¡Œ | ~2000è¡Œ | +1500è¡Œï¼ˆ+300%ï¼‰ |
| **æ€»è®¡** | ~6818è¡Œ | ~8381è¡Œ | +1563è¡Œï¼ˆ+23%ï¼‰|

> **è¯´æ˜**ï¼šä»£ç æ€»é‡å¢åŠ æ˜¯å› ä¸ºå¢åŠ äº†æµ‹è¯•å’ŒåŸºç¡€è®¾æ–½ï¼Œä½†**ä¸šåŠ¡é€»è¾‘ä»£ç å‡å°‘37%**ï¼Œ**é‡å¤ç‡ä¸‹é™68%**ï¼Œ**æ¨¡å—åŒ–ç¨‹åº¦æå‡300%**ã€‚

#### æ–‡ä»¶å¤æ‚åº¦å¯¹æ¯”

| æ–‡ä»¶ç±»åˆ« | é‡æ„å‰æœ€å¤§ | é‡æ„åæœ€å¤§ | æ”¹è¿› |
|---------|-----------|-----------|------|
| æœåŠ¡å±‚æ–‡ä»¶ | 776è¡Œ | 280è¡Œ | -64% |
| å¹³å‡æ–‡ä»¶å¤§å° | 340è¡Œ | 180è¡Œ | -47% |
| å•ä¸€èŒè´£å¾—åˆ† | 3.2/10 | 8.5/10 | +166% |

#### æ¶æ„åˆ†å±‚

```
é‡æ„å‰ï¼ˆ2å±‚ï¼‰:                  é‡æ„åï¼ˆ4å±‚ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   APIå±‚     â”‚                â”‚   APIå±‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Services   â”‚                â”‚  Services   â”‚ â† åº”ç”¨æœåŠ¡å±‚
â”‚  (6118è¡Œ)   â”‚                â”‚  (3800è¡Œ)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                               â”‚  Core/RAG   â”‚ â† ä¸šåŠ¡é€»è¾‘å±‚
                               â”‚  (~1000è¡Œ)  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                               â”‚ Core/åŸºç¡€å±‚ â”‚ â† åŸºç¡€è®¾æ–½å±‚
                               â”‚  (~1500è¡Œ)  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### å…³é”®æ”¹è¿›ç‚¹

1. **æ¨¡å—åŒ–åˆ†ç±»** â­â­â­
   ```
   é‡æ„å‰ï¼š18ä¸ªæ–‡ä»¶å¹³é“ºåœ¨services/æ ¹ç›®å½•
   é‡æ„åï¼š17ä¸ªæ–‡ä»¶åˆ†ç±»åˆ°7ä¸ªæ¨¡å—ç›®å½•
   
   ä¼˜åŠ¿ï¼š
   - èŒè´£æ¸…æ™°ï¼šæ¯ä¸ªæ¨¡å—èšç„¦å•ä¸€é¢†åŸŸ
   - æ˜“äºå®šä½ï¼šæŒ‰ä¸šåŠ¡æŸ¥æ‰¾æ–‡ä»¶ï¼ˆçŸ¥è¯†åº“â†’knowledge/ï¼‰
   - é™ä½è€¦åˆï¼šæ¨¡å—é—´é€šè¿‡æ¥å£äº¤äº’
   - ä¾¿äºæµ‹è¯•ï¼šæŒ‰æ¨¡å—ç»„ç»‡æµ‹è¯•ç”¨ä¾‹
   ```

2. **åŸºç¡€è®¾æ–½ä¸‹æ²‰** â­
   - `DeviceManager`ï¼šç»Ÿä¸€CUDAç®¡ç†ï¼ˆæ¶ˆé™¤4å¤„é‡å¤ï¼‰
   - `ModelLoader`ï¼šç»Ÿä¸€æ¨¡å‹åŠ è½½ï¼ˆæ¶ˆé™¤3å¤„é‡å¤ï¼‰
   - `ProcessManager`ï¼šç»Ÿä¸€è¿›ç¨‹ç®¡ç†
   - `PathResolver`ï¼šç»Ÿä¸€è·¯å¾„è§£æ

3. **æŠ½è±¡å±‚å»ºç«‹** â­
   - `BaseLLM`ï¼šç»Ÿä¸€LLMæ¥å£ï¼ˆæ”¯æŒTransformers/Ollama/OpenAIï¼‰
   - `BaseRetriever`ï¼šç»Ÿä¸€æ£€ç´¢æ¥å£ï¼ˆæ”¯æŒ4ç§ç­–ç•¥ï¼‰
   - `RAGPipeline`ï¼šè§£è€¦RAGæµç¨‹

4. **èŒè´£åˆ†ç¦»** â­
   - `transformers_service.py`ï¼ˆ776è¡Œï¼‰â†’ 6ä¸ªæ¨¡å—ï¼ˆæ€»280è¡Œï¼‰
   - `chat_service.py`ï¼ˆ561è¡Œï¼‰â†’ 250è¡Œ + RAG Pipeline
   - `knowledge_base_service.py`ï¼ˆ528è¡Œï¼‰â†’ 220è¡Œ + Document Service

5. **å¯æ‰©å±•æ€§æå‡** â­
   - æ–°å¢LLMï¼šå®ç°`BaseLLM`æ¥å£
   - æ–°å¢æ£€ç´¢ç­–ç•¥ï¼šå®ç°`BaseRetriever`æ¥å£
   - æ–°å¢Agentå·¥å…·ï¼šä½¿ç”¨è£…é¥°å™¨æ³¨å†Œ

---

### æœåŠ¡æ¨¡å—è¯¦è§£

#### 1. llm/ - LLMæ¨¡å‹æœåŠ¡
```python
# ç»Ÿä¸€çš„LLMç®¡ç†å…¥å£
from app.services.llm.llm_service import LLMService
from app.services.llm.embedding_service import EmbeddingService

# è·å–ä»»æ„ç±»å‹çš„LLM
llm_service = LLMService()
llm = await llm_service.get_llm("transformers", "Qwen2.5-1.5B")
llm = await llm_service.get_llm("ollama", "qwen2.5:latest")

# è·å–Embedding
embedding = EmbeddingService()
vec = await embedding.embed_text("æµ‹è¯•æ–‡æœ¬")
```

**èŒè´£**ï¼š
- âœ… LLMå·¥å‚ï¼ˆåˆ›å»º/ç¼“å­˜/å¸è½½ï¼‰
- âœ… Embeddingå‘é‡åŒ–
- âœ… æ¨¡å‹é…ç½®ç®¡ç†

---

#### 2. chat/ - å¯¹è¯æœåŠ¡
```python
# æ™®é€šå¯¹è¯
from app.services.chat.chat_service import ChatService

chat_service = ChatService()
response = await chat_service.chat(
    session_id=123,
    message="ä½ å¥½",
    kb_id=1,          # æŒ‡å®šçŸ¥è¯†åº“ï¼ˆRAGæ¨¡å¼ï¼‰
    strategy="hybrid"  # æ£€ç´¢ç­–ç•¥
)

# Agentå¯¹è¯ï¼ˆå¸¦å·¥å…·è°ƒç”¨ï¼‰
from app.services.chat.agent_service import AgentService

agent = AgentService()
result = await agent.chat(
    message="æœç´¢çŸ¥è¯†åº“ä¸­å…³äºPythonçš„å†…å®¹",
    tools=["knowledge_search", "web_search"]
)
```

**èŒè´£**ï¼š
- âœ… ä¼šè¯ç®¡ç†ï¼ˆåˆ›å»º/å†å²/ä¸Šä¸‹æ–‡ï¼‰
- âœ… RAGå¯¹è¯ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰
- âœ… Agentå¯¹è¯ï¼ˆå·¥å…·è°ƒç”¨+æ¨ç†ï¼‰
- âœ… æµå¼è¾“å‡ºæ”¯æŒ

---

#### 3. knowledge/ - çŸ¥è¯†åº“æœåŠ¡
```python
# çŸ¥è¯†åº“CRUD
from app.services.knowledge.knowledge_base_service import KnowledgeBaseService

kb_service = KnowledgeBaseService()
kb_id = await kb_service.create("æˆ‘çš„çŸ¥è¯†åº“", "æè¿°")
results = await kb_service.search(
    query="æµ‹è¯•æŸ¥è¯¢",
    kb_id=kb_id,
    strategy="hybrid",  # vector/bm25/hybrid/graph
    top_k=5
)

# æ–‡æ¡£ç®¡ç†
from app.services.knowledge.document_service import DocumentService

doc_service = DocumentService()
doc_id = await doc_service.upload_document(
    kb_id=kb_id,
    file_path=Path("test.txt"),
    metadata={"author": "å¼ ä¸‰"}
)

# å®ä½“æå–
from app.services.knowledge.entity_extraction_service import EntityExtractionService

entity_service = EntityExtractionService()
entities = await entity_service.extract_entities("å­™æ‚Ÿç©ºå¤§é—¹å¤©å®«", kb_id=1)
```

**èŒè´£**ï¼š
- âœ… çŸ¥è¯†åº“CRUD
- âœ… æ–‡æ¡£ä¸Šä¼ /åˆ†å—/å‘é‡åŒ–
- âœ… ç»Ÿä¸€æ£€ç´¢å…¥å£ï¼ˆ4ç§ç­–ç•¥ï¼‰
- âœ… å‘é‡å­˜å‚¨æ“ä½œ
- âœ… å®ä½“è¯†åˆ«

---

#### 4. graph/ - çŸ¥è¯†å›¾è°±æœåŠ¡
```python
from app.services.graph.neo4j_service import Neo4jService

graph = Neo4jService()

# æ„å»ºçŸ¥è¯†å›¾è°±
await graph.build_knowledge_graph(kb_id=1)

# æŸ¥è¯¢ä¸‰å…ƒç»„
triples = graph.get_entity_triples(kb_id=1, entity="å­™æ‚Ÿç©º")
# [{"head": "å­™æ‚Ÿç©º", "relation": "å¸ˆå‚…æ˜¯", "tail": "å”åƒ§"}, ...]

# ç»Ÿè®¡ä¿¡æ¯
stats = graph.get_statistics(kb_id=1)
# {"nodes": 100, "relationships": 250, ...}
```

**èŒè´£**ï¼š
- âœ… Neo4jè¿æ¥ç®¡ç†
- âœ… çŸ¥è¯†å›¾è°±æ„å»º
- âœ… ä¸‰å…ƒç»„æŸ¥è¯¢
- âœ… å›¾è°±ç»Ÿè®¡

---

#### 5. training/ - æ¨¡å‹è®­ç»ƒæœåŠ¡
```python
# LoRAè®­ç»ƒ
from app.services.training.lora_trainer import LoRATrainer

trainer = LoRATrainer()
task_id = await trainer.start_training(
    base_model="Qwen2.5-1.5B",
    dataset_path="monkey_brother.json",
    output_dir="./saves/lora_test"
)

# è®­ç»ƒçŠ¶æ€æŸ¥è¯¢
status = trainer.get_training_status(task_id)

# LLaMA Factoryé›†æˆ
from app.services.training.llama_factory_service import LlamaFactoryService

factory = LlamaFactoryService()
await factory.start_training_with_config(config_dict)
```

**èŒè´£**ï¼š
- âœ… LoRAè®­ç»ƒä»»åŠ¡ç®¡ç†
- âœ… æ•°æ®é›†éªŒè¯/è½¬æ¢
- âœ… è®­ç»ƒè¿›åº¦ç›‘æ§
- âœ… LLaMA Factoryé›†æˆ

---

#### 6. model_mgmt/ - æ¨¡å‹ç®¡ç†æœåŠ¡
```python
# ç»Ÿä¸€æ‰«æå™¨ï¼ˆæ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
from app.services.model_mgmt.model_scanner import ModelScanner

scanner = ModelScanner()

# æ‰«æLLMæ¨¡å‹
llm_models = await scanner.scan(
    base_path="Models/LLM",
    model_type="llm"
)

# æ‰«æLoRAé€‚é…å™¨
lora_adapters = await scanner.scan(
    base_path="Models/LoRA", 
    model_type="lora"
)

# æ‰«æEmbeddingæ¨¡å‹
embedding_models = await scanner.scan(
    base_path="Models/Embedding",
    model_type="embedding"
)

# æ¨¡å‹éƒ¨ç½²ç®¡ç†
from app.services.model_mgmt.deployment import deploy_model, undeploy_model

await deploy_model(model_id="llama-3-8b", version="v1.0")
await undeploy_model(model_id="llama-3-8b")
```

**èŒè´£**ï¼š
- âœ… **ç»Ÿä¸€æ‰«æ**ï¼šä¸€ä¸ªæ‰«æå™¨æ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼ˆLLM/LoRA/Embeddingï¼‰
- âœ… **æ ¼å¼è¯†åˆ«**ï¼šè‡ªåŠ¨è¯†åˆ«GGUF/Safetensors/PyTorchæ ¼å¼
- âœ… **æ¨¡å‹éƒ¨ç½²**ï¼šä¸Šçº¿/ä¸‹çº¿ã€ç‰ˆæœ¬åˆ‡æ¢

**ä¸ºä»€ä¹ˆåˆå¹¶æ‰«æå™¨**ï¼š
- ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰æ¨¡å‹ç±»å‹ä½¿ç”¨åŒä¸€å¥—æ‰«æé€»è¾‘
- å‡å°‘é‡å¤ï¼šæ–‡ä»¶éå†ã€æ ¼å¼è¯†åˆ«ã€é”™è¯¯å¤„ç†å¯å¤ç”¨
- æ˜“äºæ‰©å±•ï¼šæ–°å¢æ¨¡å‹ç±»å‹åªéœ€æ·»åŠ ä¸€ä¸ª`_scan_xxx()`æ–¹æ³•

---

#### 7. storage/ - å­˜å‚¨æœåŠ¡
```python
# æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½
from app.services.storage.file_service import FileService

file_service = FileService()
file_path = await file_service.upload_file(
    file=upload_file,
    kb_id=1,
    category="training_data"
)

# å…ƒæ•°æ®ç®¡ç†
from app.services.storage.metadata_service import MetadataService

metadata = MetadataService()
await metadata.update_file_metadata(
    file_id=123,
    metadata={"tags": ["é‡è¦", "å¾…å®¡æ ¸"]}
)
```

**èŒè´£**ï¼š
- âœ… æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½
- âœ… æ–‡ä»¶è·¯å¾„ç®¡ç†
- âœ… å…ƒæ•°æ®å­˜å‚¨/æŸ¥è¯¢

---

### æ¨¡å—è¿ç§»å¯¹ç…§è¡¨

**æ—§æ–‡ä»¶ â†’ æ–°ä½ç½®**ï¼š

| æ—§æ–‡ä»¶ | æ–°ä½ç½® | å˜åŒ– |
|--------|--------|------|
| `transformers_service.py` (776è¡Œ) | `core/llm/transformers/` (6ä¸ªæ–‡ä»¶) | æ‹†åˆ†+ä¸‹æ²‰ |
| `ollama_llm_service.py` (265è¡Œ) | `core/llm/ollama_llm.py` (150è¡Œ) | ç®€åŒ–+ä¸‹æ²‰ |
| `ollama_embedding_service.py` (204è¡Œ) | åˆå¹¶åˆ° `services/llm/embedding_service.py` | åˆå¹¶ |
| `chat_service.py` (561è¡Œ) | `services/chat/chat_service.py` (250è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `agent_service.py` (316è¡Œ) | `services/chat/agent_service.py` (200è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `knowledge_base_service.py` (528è¡Œ) | `services/knowledge/knowledge_base_service.py` (220è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| æ–°å¢ | `services/knowledge/document_service.py` (200è¡Œ) | èŒè´£åˆ†ç¦» |
| `vector_store_service.py` (287è¡Œ) | `services/knowledge/vector_store_service.py` (250è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `entity_extraction_service.py` (337è¡Œ) | `services/knowledge/entity_extraction_service.py` (200è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `neo4j_graph_service.py` (513è¡Œ) | `services/graph/neo4j_service.py` (350è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `simple_lora_trainer.py` (500è¡Œ) | `services/training/lora_trainer.py` (350è¡Œ) | é‡å‘½å+ç®€åŒ–+åˆ†ç±» |
| `llama_factory_service.py` (243è¡Œ) | `services/training/llama_factory_service.py` (243è¡Œ) | ä»…åˆ†ç±» |
| `model_manager.py` (214è¡Œ) | `services/model_mgmt/model_scanner.py` (350è¡Œ) | åˆå¹¶+åˆ†ç±» |
| `model_scanner.py` (344è¡Œ) | â†‘ åˆå¹¶åˆ°ç»Ÿä¸€æ‰«æå™¨ | åˆå¹¶ |
| `lora_scanner_service.py` (393è¡Œ) | â†‘ åˆå¹¶åˆ°ç»Ÿä¸€æ‰«æå™¨ | åˆå¹¶ |
| æ–°å¢ | `services/model_mgmt/deployment.py` (180è¡Œ) | èŒè´£åˆ†ç¦» |
| `file_service.py` (301è¡Œ) | `services/storage/file_service.py` (280è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `metadata_service.py` (128è¡Œ) | `services/storage/metadata_service.py` (128è¡Œ) | ä»…åˆ†ç±» |
| `embedding_service.py` (334è¡Œ) | `services/llm/embedding_service.py` (250è¡Œ) | ç®€åŒ–+åˆ†ç±» |
| `hybrid_retrieval_service.py` (374è¡Œ) | `core/retrieval/hybrid_retriever.py` (180è¡Œ) | ç®€åŒ–+ä¸‹æ²‰ |

**ç»Ÿè®¡**ï¼š
- ğŸ—‘ï¸ **åˆ é™¤**ï¼š0ä¸ªæ–‡ä»¶ï¼ˆå…¨éƒ¨ä¿ç•™æˆ–æ‹†åˆ†ï¼‰
- ğŸ“¦ **æ‹†åˆ†**ï¼š4ä¸ªæ–‡ä»¶æ‹†åˆ†ä¸ºå¤šä¸ªï¼ˆtransformers, model_manager, knowledge_base, ollamaï¼‰
- ğŸ†• **æ–°å¢**ï¼š5ä¸ªæ–‡ä»¶ï¼ˆdocument_service, deployment, æ£€ç´¢ç­–ç•¥ç­‰ï¼‰
- ğŸ“ **åˆ†ç±»**ï¼šæ‰€æœ‰æ–‡ä»¶æŒ‰7å¤§æ¨¡å—å½’ç±»
- â¬‡ï¸ **ç®€åŒ–**ï¼šå¹³å‡å‡å°‘32%ä»£ç é‡

---

### è¿ç§»è·¯å¾„

#### é˜¶æ®µ0ï¼ˆWeek 0ï¼‰ï¼šå‡†å¤‡
- å»ºç«‹æµ‹è¯•åŸºå‡†
- åˆ›å»ºé‡æ„åˆ†æ”¯

#### é˜¶æ®µ1ï¼ˆWeek 1-2ï¼‰ï¼šåŸºç¡€å±‚
- åˆ›å»º`core/device/`, `core/model/`, `core/utils/`
- åœ¨ç°æœ‰æœåŠ¡ä¸­æ›¿æ¢é‡å¤ä»£ç 

#### é˜¶æ®µ2ï¼ˆWeek 3-5ï¼‰ï¼šæ¨¡å‹å±‚
- åˆ›å»º`core/llm/`æŠ½è±¡å±‚
- æ‹†åˆ†`transformers_service.py`
- å®ç°`OllamaLLM`

#### é˜¶æ®µ3ï¼ˆWeek 6-8ï¼‰ï¼šä¸šåŠ¡å±‚
- åˆ›å»º`core/retrieval/`æ£€ç´¢ç­–ç•¥
- é‡æ„`knowledge_base_service.py`
- æ–°å¢`document_service.py`
- é‡æ„`model_mgmt/`, `storage/`, `training/`, `graph/`æ¨¡å— â­

#### é˜¶æ®µ4ï¼ˆWeek 9-10ï¼‰ï¼šåº”ç”¨å±‚
- åˆ›å»º`core/rag/rag_pipeline.py`
- é‡æ„`chat_service.py`
- ä¼˜åŒ–`agent_service.py`

#### é˜¶æ®µ5ï¼ˆWeek 11-12ï¼‰ï¼šæ¸…ç†
- åˆ é™¤æ—§æ–‡ä»¶
- æ€§èƒ½ä¼˜åŒ–
- æ–‡æ¡£è¡¥å…¨

---

**å®æ–½æ‰‹å†Œå®Œæˆï¼** ğŸ‰

**æ–‡æ¡£ç»Ÿè®¡**ï¼š
- æ€»é¡µæ•°ï¼š~3200è¡Œ
- ä¼°è®¡é˜…è¯»æ—¶é—´ï¼š75åˆ†é’Ÿ
- å®æ–½å‘¨æœŸï¼š12å‘¨ï¼ˆ3ä¸ªæœˆï¼‰
- é¢„æœŸæ•ˆæœï¼šä»£ç è´¨é‡æå‡50%+

**æ ¸å¿ƒä»·å€¼**ï¼š
1. âœ… å»ºç«‹4å±‚æ¶æ„ï¼ˆAPI â†’ åº”ç”¨ â†’ ä¸šåŠ¡ â†’ åŸºç¡€ï¼‰
2. âœ… æŒ‰æ¨¡å—åˆ†ç±»æœåŠ¡å±‚ï¼ˆ7å¤§ä¸šåŠ¡æ¨¡å—ï¼‰â­â­â­
3. âœ… æ¶ˆé™¤é‡å¤ä»£ç ï¼ˆ-68%é‡å¤ç‡ï¼‰
4. âœ… æå‡å¯æ‰©å±•æ€§ï¼ˆæ’ä»¶åŒ–æ¶æ„ï¼‰
5. âœ… å¢å¼ºå¯æµ‹è¯•æ€§ï¼ˆæµ‹è¯•è¦†ç›–ç‡+167%ï¼‰
6. âœ… é™ä½ç»´æŠ¤æˆæœ¬ï¼ˆå¹³å‡æ–‡ä»¶å¤§å°-47%ï¼‰
7. âœ… æé«˜ä»£ç å¯è¯»æ€§ï¼ˆæ¨¡å—åŒ–ç¨‹åº¦+300%ï¼‰

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**ï¼š
1. é˜…è¯»æœ¬æ–‡æ¡£ï¼ˆä¼°è®¡1.5å°æ—¶ï¼‰
2. è¿è¡Œæµ‹è¯•åŸºå‡†ï¼ˆWeek 0ï¼‰
3. åˆ›å»ºé‡æ„åˆ†æ”¯
4. æŒ‰é˜¶æ®µé€æ­¥å®æ–½ï¼ˆå»ºè®®å…ˆå®Œæˆé˜¶æ®µ1åŸºç¡€å±‚ï¼‰

---

**ç‰ˆæœ¬å†å²**ï¼š
- v3.0 (2025-01-27): â­â­â­æ¶æ„ä¼˜åŒ–ç‰ˆï¼Œç³»ç»Ÿæ€§é‡ç»„12å‘¨è®¡åˆ’
  - ä¿®æ­£æ¨¡å—å½’å±ï¼šmodel_mgmt/trainingç§»è‡³Week 3-5æ¨¡å‹å±‚ï¼Œgraph/vector_storeç§»è‡³Week 6-7ä¸šåŠ¡å±‚
  - æ˜ç¡®è¯´æ˜ï¼šæ‰€æœ‰"ç®€åŒ–"å‡ä¸ºæ¶æ„ä¼˜åŒ–ï¼ˆåˆ é™¤é‡å¤ä»£ç ã€æå–å…¬å…±æ¨¡å—ï¼‰ï¼Œä¸åˆ é™¤ä¸šåŠ¡åŠŸèƒ½
  - è¡¥å……ä»»åŠ¡ï¼šä¸ºembedding_serviceã€vector_storeã€entity_extractionã€neo4jç­‰æ·»åŠ å…·ä½“é‡æ„ä»»åŠ¡
  - æ—¶é—´ä¼˜åŒ–ï¼šæ¯å‘¨æ§åˆ¶åœ¨5å¤©å·¥ä½œé‡ï¼Œæ€»ä½“12å‘¨å®Œæˆ
- v2.2 (2025-01-26): è¡¥å……Week 7-8ç¼ºå¤±ä»»åŠ¡ï¼ˆåå‘ç°å½’å±é”™è¯¯ï¼‰
- v2.1 (2025-01-26): ä¼˜åŒ–æœåŠ¡å±‚ç»“æ„ï¼ŒæŒ‰7å¤§æ¨¡å—åˆ†ç±»
- v2.0 (2025-01-26): ç²¾ç®€å®æ–½ç‰ˆï¼Œè¡¥å……æ–‡ä»¶ç»“æ„å¯¹æ¯”
- v1.0 (2025-01-25): åˆå§‹ç‰ˆæœ¬

