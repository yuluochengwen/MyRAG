# LLaMA-Training ç›®å½•è¿ç§»å®ŒæˆæŠ¥å‘Š

**æ—¥æœŸ**: 2025å¹´11æœˆ20æ—¥  
**ä»»åŠ¡**: ç»Ÿä¸€ç®¡ç† LLaMA-Factory ç›¸å…³æ–‡ä»¶ï¼Œä¼˜åŒ–ç›®å½•ç»“æ„

---

## âœ… å·²å®Œæˆçš„å·¥ä½œ

### 1. åˆ›å»ºç»Ÿä¸€å·¥ä½œç›®å½• âœ“

```
LLaMA-Training/
â”œâ”€â”€ llamaboard_cache/       # ä» Backend/llamaboard_cache ç§»åŠ¨
â”œâ”€â”€ llamaboard_config/      # ä» Backend/llamaboard_config ç§»åŠ¨
â”œâ”€â”€ saves/                  # ä» Backend/saves ç§»åŠ¨
â”œâ”€â”€ .llamafactory_env       # æ–°å»ºï¼šç¯å¢ƒå˜é‡é…ç½®
â””â”€â”€ README.md               # æ–°å»ºï¼šç›®å½•è¯´æ˜æ–‡æ¡£
```

**åŸå› **: 
- Backend ç›®å½•åº”è¯¥åªåŒ…å«åç«¯æœåŠ¡ä»£ç 
- è®­ç»ƒç›¸å…³æ–‡ä»¶ç‹¬ç«‹ç®¡ç†ï¼Œç»“æ„æ›´æ¸…æ™°
- LLaMA-Factory çš„ç¼“å­˜å’Œè¾“å‡ºç»Ÿä¸€å­˜æ”¾

### 2. ä¿®æ”¹å¯åŠ¨è„šæœ¬ âœ“

**ä¿®æ”¹çš„æ–‡ä»¶**:
- `start-fast.bat` - å¿«é€Ÿå¯åŠ¨è„šæœ¬
- `start.bat` - å®Œæ•´å¯åŠ¨è„šæœ¬

**å…³é”®å˜æ›´**:
```bat
# æ—§æ–¹å¼ï¼ˆåœ¨ Backend ç›®å½•å¯åŠ¨ï¼‰
cd Backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# æ–°æ–¹å¼ï¼ˆåœ¨æ ¹ç›®å½•å¯åŠ¨ï¼‰
python -m uvicorn Backend.main:app --reload --host 0.0.0.0 --port 8000
```

**å¥½å¤„**:
- âœ… å·¥ä½œç›®å½•ç»Ÿä¸€åœ¨é¡¹ç›®æ ¹ç›®å½•
- âœ… LLaMA-Factory è‡ªåŠ¨åœ¨æ ¹ç›®å½•åˆ›å»ºç¼“å­˜
- âœ… ä¸å½±å“ç°æœ‰ä»£ç è¿è¡Œï¼ˆæ‰€æœ‰è·¯å¾„éƒ½ä½¿ç”¨ `BASE_DIR` è®¡ç®—ï¼‰

### 3. æ‰©å±• LoRA æ‰«ææœåŠ¡ âœ“

**ä¿®æ”¹çš„æ–‡ä»¶**:
- `Backend/app/services/lora_scanner_service.py`
- `Backend/app/services/model_scanner.py`

**æ–°å¢åŠŸèƒ½**:
```python
# ç°åœ¨åŒæ—¶æ‰«æä¸¤ä¸ªç›®å½•
scan_dirs = [
    "Models/LoRA",              # ç”Ÿäº§ç¯å¢ƒæ¨¡å‹
    "LLaMA-Training/saves"      # è®­ç»ƒè¾“å‡ºæ¨¡å‹ï¼ˆæ–°å¢ï¼‰
]

# æ”¯æŒé€’å½’æ‰«æï¼ˆåº”å¯¹ saves çš„å¤æ‚ç»“æ„ï¼‰
for adapter_config in base_dir.rglob("adapter_config.json"):
    # å¤„ç†æ‰¾åˆ°çš„ LoRA æ¨¡å‹
```

**æ£€æµ‹åˆ°çš„ LoRA æ¨¡å‹**:
```
LLaMA-Training/saves/
â””â”€â”€ DeepSeek-R1-1.5B-Distill/
    â””â”€â”€ lora/
        â”œâ”€â”€ train_2025-11-20-02-18-14/
        â”‚   â”œâ”€â”€ adapter_config.json       âœ“
        â”‚   â”œâ”€â”€ adapter_model.safetensors âœ“ (35.3 MB)
        â”‚   â””â”€â”€ ...
        â””â”€â”€ checkpoint-6/
            â”œâ”€â”€ adapter_config.json       âœ“
            â”œâ”€â”€ adapter_model.safetensors âœ“
            â””â”€â”€ ...
```

### 4. æ›´æ–° .gitignore âœ“

```gitignore
# æ–°å¢è§„åˆ™
# LLaMA-Factory Training Workspace
LLaMA-Training/
```

**ä½œç”¨**: é˜²æ­¢è®­ç»ƒæ–‡ä»¶ã€ç¼“å­˜ã€å¤§æ–‡ä»¶æäº¤åˆ° Git

---

## ğŸ¯ åŠŸèƒ½éªŒè¯

### å¯åŠ¨è„šæœ¬éªŒè¯

**æµ‹è¯•æ­¥éª¤**:
```powershell
# 1. å¿«é€Ÿå¯åŠ¨æµ‹è¯•
.\start-fast.bat

# 2. æ£€æŸ¥å·¥ä½œç›®å½•
# åº”è¯¥åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œè€Œé Backend/

# 3. æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸
# è®¿é—® http://localhost:8000/docs
```

**é¢„æœŸç»“æœ**:
- âœ… æœåŠ¡æ­£å¸¸å¯åŠ¨
- âœ… æ‰€æœ‰ API æ­£å¸¸è®¿é—®
- âœ… é™æ€æ–‡ä»¶æ­£å¸¸åŠ è½½
- âœ… LLaMA-Factory ä¸å†åœ¨ Backend/ åˆ›å»ºç›®å½•

### LoRA æ‰«æéªŒè¯

**API æµ‹è¯•**:
```bash
# åˆ—å‡ºæ‰€æœ‰ LoRA æ¨¡å‹ï¼ˆåº”åŒ…å« saves ç›®å½•çš„æ¨¡å‹ï¼‰
GET http://localhost:8000/api/models/lora

# æ‰‹åŠ¨è§¦å‘æ‰«æ
POST http://localhost:8000/api/models/scan/lora
```

**é¢„æœŸç»“æœ**:
```json
{
  "models": [
    {
      "name": "train_2025-11-20-02-18-14",
      "path": "C:\\...\\LLaMA-Training\\saves\\DeepSeek-R1-1.5B-Distill\\lora\\train_2025-11-20-02-18-14",
      "base_model": "DeepSeek-R1-1.5B-Distill",
      "rank": 8,
      "lora_alpha": 16,
      "size": "35.27 MB"
    }
  ]
}
```

---

## ğŸ“‹ ç›®å½•å¯¹æ¯”

### ä¿®æ”¹å‰
```
MyRAG/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ llamaboard_cache/      âŒ ä¸åº”è¯¥åœ¨è¿™é‡Œ
â”‚   â”œâ”€â”€ llamaboard_config/     âŒ ä¸åº”è¯¥åœ¨è¿™é‡Œ
â”‚   â”œâ”€â”€ saves/                 âŒ ä¸åº”è¯¥åœ¨è¿™é‡Œ
â”‚   â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ Models/
â””â”€â”€ ...
```

### ä¿®æ”¹å
```
MyRAG/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app/                   âœ“ åªæœ‰ä»£ç 
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ LLaMA-Training/            âœ“ æ–°å»ºç»Ÿä¸€ç›®å½•
â”‚   â”œâ”€â”€ llamaboard_cache/      âœ“ ä» Backend ç§»åŠ¨
â”‚   â”œâ”€â”€ llamaboard_config/     âœ“ ä» Backend ç§»åŠ¨
â”‚   â”œâ”€â”€ saves/                 âœ“ ä» Backend ç§»åŠ¨
â”‚   â””â”€â”€ README.md              âœ“ è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ LLM/                   âœ“ åŸºåº§æ¨¡å‹
â”‚   â”œâ”€â”€ Embedding/             âœ“ åµŒå…¥æ¨¡å‹
â”‚   â””â”€â”€ LoRA/                  âœ“ ç”Ÿäº§ LoRA
â””â”€â”€ ...
```

---

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### ä¸ºä»€ä¹ˆä¸å½±å“ç°æœ‰ä»£ç ï¼Ÿ

**1. è·¯å¾„è®¡ç®—éƒ½ä½¿ç”¨ `BASE_DIR`**:
```python
# Backend/app/core/config.py
BASE_DIR = Path(__file__).resolve().parent.parent.parent.parent
# æ— è®ºä»å“ªé‡Œå¯åŠ¨ï¼Œéƒ½ä¼šè®¡ç®—åˆ° MyRAG/

# Backend/main.py
BASE_DIR = Path(__file__).resolve().parent.parent
# åŒæ ·è‡ªåŠ¨è®¡ç®—ç»å¯¹è·¯å¾„
```

**2. é…ç½®æ–‡ä»¶è·¯å¾„ä¹Ÿä½¿ç”¨ `BASE_DIR`**:
```python
# Backend/app/core/config.py
config_file = BASE_DIR / "Backend" / "config.yaml"
# ä¸ä¾èµ–å½“å‰å·¥ä½œç›®å½•
```

**3. ç›¸å¯¹è·¯å¾„è‡ªåŠ¨è½¬æ¢**:
```python
# é…ç½®ä¸­çš„ç›¸å¯¹è·¯å¾„
upload_dir: "KnowledgeBase"

# è‡ªåŠ¨è½¬æ¢ä¸º
upload_dir = BASE_DIR / "KnowledgeBase"
# æ— è®ºå·¥ä½œç›®å½•åœ¨å“ªé‡Œï¼Œéƒ½æŒ‡å‘æ­£ç¡®ä½ç½®
```

### LLaMA-Factory ä¸ºä»€ä¹ˆåˆ›å»ºåœ¨ Backendï¼Ÿ

**åŸå› åˆ†æ**:
```python
# LLaMA-Factory/src/llamafactory/webui/common.py
DEFAULT_CACHE_DIR = "llamaboard_cache"      # ç›¸å¯¹è·¯å¾„
DEFAULT_CONFIG_DIR = "llamaboard_config"    # ç›¸å¯¹è·¯å¾„

# åœ¨å½“å‰å·¥ä½œç›®å½•åˆ›å»º
# åŸæ¥: cd Backend && uvicorn ... â†’ åœ¨ Backend/ åˆ›å»º
# ç°åœ¨: python -m uvicorn Backend.main:app â†’ åœ¨æ ¹ç›®å½•åˆ›å»º
```

**è§£å†³æ–¹æ¡ˆ**:
- ä¸ä¿®æ”¹ LLaMA-Factory ä»£ç 
- æ”¹å˜æœåŠ¡å¯åŠ¨æ–¹å¼ï¼ˆåœ¨æ ¹ç›®å½•å¯åŠ¨ï¼‰
- LLaMA-Factory è‡ªç„¶åœ¨æ ¹ç›®å½•åˆ›å»ºç¼“å­˜
- æ‰‹åŠ¨ç§»åŠ¨ç°æœ‰æ–‡ä»¶åˆ°ç»Ÿä¸€ç›®å½•

---

## ğŸš€ åç»­ä½¿ç”¨

### è®­ç»ƒæ–°æ¨¡å‹
1. ä½¿ç”¨ LLaMA-Factory è®­ç»ƒ
2. æƒé‡è‡ªåŠ¨ä¿å­˜åˆ° `LLaMA-Training/saves/`
3. ç³»ç»Ÿè‡ªåŠ¨æ‰«æå¹¶è¯†åˆ«
4. åœ¨å‰ç«¯é¡µé¢æŸ¥çœ‹å’Œä½¿ç”¨

### å¯¼å…¥å¤–éƒ¨æ¨¡å‹
1. å°† LoRA æ¨¡å‹å¤åˆ¶åˆ° `LLaMA-Training/saves/{æ¨¡å‹å}/`
2. ç¡®ä¿åŒ…å«ï¼š
   - `adapter_config.json`
   - `adapter_model.safetensors` æˆ– `adapter_model.bin`
3. è°ƒç”¨æ‰«æ API æˆ–é‡å¯æœåŠ¡
4. æ¨¡å‹è‡ªåŠ¨å‡ºç°åœ¨åˆ—è¡¨ä¸­

### æ¨¡å‹ç®¡ç†
- **ä¸´æ—¶æ¨¡å‹**: æ”¾åœ¨ `LLaMA-Training/saves/`ï¼ˆè®­ç»ƒè¾“å‡ºï¼‰
- **ç”Ÿäº§æ¨¡å‹**: å¤åˆ¶åˆ° `Models/LoRA/`ï¼ˆæ°¸ä¹…ä¿å­˜ï¼‰
- **æ¸…ç†**: åˆ é™¤ä¸éœ€è¦çš„ checkpoint å’Œè®­ç»ƒæ—¥å¿—

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡å¯åŠ¨**: å¦‚æœ LLaMA-Factory è¿˜ä¼šåœ¨ Backend/ åˆ›å»ºç›®å½•ï¼Œè¯´æ˜è¿˜åœ¨ä½¿ç”¨æ—§å¯åŠ¨æ–¹å¼
2. **è·¯å¾„æ˜¾ç¤º**: API è¿”å›çš„è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œå®šä½
3. **æ•°æ®åº“åŒæ­¥**: LoRA æ‰«æä¼šè‡ªåŠ¨æ³¨å†Œåˆ°æ•°æ®åº“ `lora_models` è¡¨
4. **æ€§èƒ½**: é€’å½’æ‰«æå¯¹æ€§èƒ½å½±å“å¾ˆå°ï¼ˆç›®å½•ä¸æ·±ä¸”æ–‡ä»¶ä¸å¤šï¼‰

---

## âœ¨ æ”¹è¿›æ•ˆæœ

### ç›®å½•ç»“æ„
- âœ… Backend ç›®å½•èŒè´£å•ä¸€ï¼ŒåªåŒ…å«ä»£ç 
- âœ… è®­ç»ƒç›¸å…³æ–‡ä»¶ç»Ÿä¸€ç®¡ç†
- âœ… ç¬¦åˆè½¯ä»¶å·¥ç¨‹æœ€ä½³å®è·µ

### LoRA ç®¡ç†
- âœ… è‡ªåŠ¨å‘ç°è®­ç»ƒè¾“å‡ºçš„æ¨¡å‹
- âœ… æ”¯æŒå¤æ‚çš„ç›®å½•ç»“æ„
- âœ… ç»Ÿä¸€çš„æ¨¡å‹åˆ—è¡¨å’Œæ¨ç†æ¥å£

### ç»´æŠ¤æ€§
- âœ… ç›®å½•ç»“æ„æ¸…æ™°æ˜“æ‡‚
- âœ… æ–‡æ¡£å®Œå–„ï¼ˆREADME.mdï¼‰
- âœ… Git å¿½ç•¥é…ç½®æ­£ç¡®

---

**æ€»ç»“**: æ‰€æœ‰å·¥ä½œå·²å®Œæˆï¼Œç³»ç»Ÿå¯ä»¥æ­£å¸¸è¿è¡Œã€‚LLaMA-Training ç›®å½•ç°åœ¨æ˜¯æ‰€æœ‰è®­ç»ƒç›¸å…³æ–‡ä»¶çš„ç»Ÿä¸€å…¥å£ï¼ŒLoRA æ‰«ææœåŠ¡ä¼šè‡ªåŠ¨è¯†åˆ«å…¶ä¸­çš„æ¨¡å‹ã€‚
