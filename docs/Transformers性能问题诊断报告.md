# Transformers æ€§èƒ½é—®é¢˜è¯Šæ–­æŠ¥å‘Š

**è¯Šæ–­æ—¶é—´**: 2025å¹´11æœˆ20æ—¥  
**è¯Šæ–­èŒƒå›´**: æ‰€æœ‰ Transformers ç›¸å…³ä»£ç   
**é—®é¢˜ç±»å‹**: æ€§èƒ½ä½ä¸‹ã€ä¸éµå¾ªæç¤ºè¯

---

## 1. é—®é¢˜æ€»ç»“

### 1.1 æ ¸å¿ƒé—®é¢˜

1. **DeepSeekæ¨¡å‹è¾“å‡ºæ€è€ƒè¿‡ç¨‹** âš ï¸ ä¸¥é‡
   - DeepSeek-R1ç³»åˆ—æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1-1.5B-Distillï¼‰æ˜¯æ¨ç†æ¨¡å‹
   - ä¼šè¾“å‡º `<think>...</think>` æ ‡ç­¾åŒ…è£¹çš„æ€è€ƒè¿‡ç¨‹
   - é¡¹ç›®ä»£ç **å®Œå…¨æ²¡æœ‰å¤„ç†**è¿™ç±»ç‰¹æ®Šè¾“å‡ºæ ¼å¼

2. **æ€§èƒ½æåº¦ä½ä¸‹** âš ï¸ ä¸¥é‡
   - é…ç½®ï¼š`max_tokens: 512` ä½†å®é™…å¯èƒ½ç”Ÿæˆæ›´å¤š
   - ä½¿ç”¨INT4é‡åŒ– + device_map="auto" ä½†æ²¡æœ‰ä¼˜åŒ–ç”Ÿæˆå‚æ•°
   - æ²¡æœ‰é’ˆå¯¹æ¨ç†æ¨¡å‹çš„ç‰¹æ®Šä¼˜åŒ–
   - CPU offloadå¯¼è‡´é€Ÿåº¦ææ…¢

3. **ä¸éµå¾ªæç¤ºè¯** âš ï¸ ä¸­ç­‰
   - Promptæ„å»ºè¿‡äºç®€å•
   - æ²¡æœ‰é’ˆå¯¹ä¸åŒæ¨¡å‹ç±»å‹çš„æ¨¡æ¿é€‚é…
   - DeepSeek-R1éœ€è¦ç‰¹æ®Šçš„æç¤ºè¯æ ¼å¼

---

## 2. è¯¦ç»†é—®é¢˜åˆ†æ

### 2.1 DeepSeek-R1 ç‰¹æ®Šæ€§

**æ¨¡å‹ç‰¹ç‚¹**:
```
DeepSeek-R1 ç³»åˆ—ï¼ˆåŒ…æ‹¬1.5B-Distillï¼‰æ˜¯æ¨ç†å¼ºåŒ–æ¨¡å‹ï¼š
1. ä¼šè¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼ˆ<think>æ ‡ç­¾ï¼‰
2. éœ€è¦ç‰¹å®šçš„promptæ ¼å¼
3. è¾“å‡ºæ ¼å¼ï¼š<think>æ€è€ƒè¿‡ç¨‹</think>\næœ€ç»ˆç­”æ¡ˆ
```

**å½“å‰ä»£ç é—®é¢˜**:
```python
# transformers_service.py ç¬¬367è¡Œ
response = self.current_tokenizer.decode(
    output_ids[0][input_length:],
    skip_special_tokens=True  # âš ï¸ è¿™é‡Œåªæ˜¯è·³è¿‡ç‰¹æ®Štokenï¼Œä¸å¤„ç†<think>æ ‡ç­¾
)
return response.strip()  # âš ï¸ ç›´æ¥è¿”å›ï¼ŒåŒ…å«æ‰€æœ‰æ€è€ƒè¿‡ç¨‹
```

**ç”¨æˆ·çœ‹åˆ°çš„è¾“å‡º**:
```
<think>
é¦–å…ˆåˆ†æç”¨æˆ·çš„é—®é¢˜...
ç„¶åæ£€ç´¢ç›¸å…³æ–‡æ¡£...
æœ€åç»„ç»‡ç­”æ¡ˆ...
</think>

è¿™æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚
```

### 2.2 Prompt æ¨¡æ¿é—®é¢˜

**å½“å‰å®ç°** (`transformers_service.py` ç¬¬479-507è¡Œ):
```python
def _build_prompt(self, messages: List[Dict[str, str]]) -> str:
    # æ£€æµ‹æ˜¯å¦æœ‰apply_chat_templateæ–¹æ³•
    if hasattr(self.current_tokenizer, "apply_chat_template"):
        try:
            return self.current_tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception as e:
            logger.warning(f"apply_chat_templateå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤æ¨¡æ¿")
    
    # é»˜è®¤æ¨¡æ¿(é€‚ç”¨äºå¤§å¤šæ•°æ¨¡å‹) âš ï¸ å¤ªç®€å•ï¼Œä¸é€‚é…DeepSeek
    prompt = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        
        if role == "system":
            prompt += f"System: {content}\n\n"
        elif role == "user":
            prompt += f"User: {content}\n\n"
        elif role == "assistant":
            prompt += f"Assistant: {content}\n\n"
    
    prompt += "Assistant: "
    return prompt
```

**é—®é¢˜**:
1. æ²¡æœ‰æ£€æµ‹æ¨¡å‹ç±»å‹
2. æ²¡æœ‰ä¸ºDeepSeek-R1æ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤ï¼ˆå¦‚"ç›´æ¥å›ç­”ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹"ï¼‰
3. é»˜è®¤æ¨¡æ¿å¯¹äºæ¨ç†æ¨¡å‹æ•ˆæœå·®

### 2.3 ç”Ÿæˆå‚æ•°é—®é¢˜

**å½“å‰é…ç½®** (`transformers_service.py` ç¬¬331-339è¡Œ):
```python
generation_config = {
    "max_new_tokens": max_tokens,  # âš ï¸ é»˜è®¤512ï¼Œä½†æ²¡æœ‰é™åˆ¶æ€è€ƒè¿‡ç¨‹é•¿åº¦
    "temperature": temperature,
    "do_sample": temperature > 0,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "pad_token_id": self.current_tokenizer.eos_token_id,
}
```

**é—®é¢˜**:
1. æ²¡æœ‰é’ˆå¯¹æ¨ç†æ¨¡å‹çš„ä¼˜åŒ–å‚æ•°
2. ç¼ºå°‘ `early_stopping` å‚æ•°
3. ç¼ºå°‘ `num_beams` å‚æ•°ï¼ˆBeam Searchå¯èƒ½æ›´å¥½ï¼‰
4. `max_new_tokens=512` å¯¹äºåŒ…å«æ€è€ƒè¿‡ç¨‹çš„è¾“å‡ºå¤ªé•¿

### 2.4 æ€§èƒ½é—®é¢˜

**CPU Offloadé—®é¢˜** (`transformers_service.py` ç¬¬119-125è¡Œ):
```python
if quantize and self.device == "cuda":
    load_kwargs["quantization_config"] = self.quantization_config
    load_kwargs["device_map"] = "auto"
    load_kwargs["max_memory"] = {0: "5.5GiB", "cpu": "0GiB"}  # âš ï¸ é™åˆ¶CPUä¸º0GB
```

**åˆ†æ**:
- è™½ç„¶é™åˆ¶CPUä¸º0GBï¼Œä½†INT4é‡åŒ–çš„1.5Bæ¨¡å‹å®é™…å¤§çº¦800MB
- 6GBæ˜¾å­˜åº”è¯¥å®Œå…¨å¤Ÿç”¨
- ä½†ä½¿ç”¨ `device_map="auto"` å¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„è®¾å¤‡é—´é€šä¿¡

**æ¨ç†é€Ÿåº¦é—®é¢˜**:
```python
# ç¬¬340-357è¡Œ
try:
    with torch.no_grad():
        timeout = max(60, max_tokens // 10)  # âš ï¸ 512 tokensè¶…æ—¶60ç§’
        output_ids = await asyncio.wait_for(
            loop.run_in_executor(
                None,
                lambda: self.current_model.generate(**inputs, **generation_config)
            ),
            timeout=timeout
        )
except asyncio.TimeoutError:
    logger.error(f"ç”Ÿæˆè¶…æ—¶({timeout}ç§’)ï¼Œè¿”å›éƒ¨åˆ†ç»“æœ")
    return "æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤è¶…æ—¶ã€‚"
```

**é—®é¢˜**:
- 60ç§’è¶…æ—¶è¯´æ˜ç”Ÿæˆé€Ÿåº¦ææ…¢
- æ²¡æœ‰ä½¿ç”¨KV-cacheä¼˜åŒ–
- æ²¡æœ‰ä½¿ç”¨Flash Attention

### 2.5 æ¨¡å‹åŠ è½½é—®é¢˜

**å½“å‰é…ç½®**:
```yaml
# config.yaml
llm:
  default_model: "DeepSeek-OCR-3B"  # âš ï¸ è¿™æ˜¯OCRæ¨¡å‹ï¼Œä¸æ˜¯å¯¹è¯æ¨¡å‹
  transformers_quantization: "int4"
  transformers_max_memory: 5.5
  max_tokens: 512
```

**é—®é¢˜**:
1. `DeepSeek-OCR-3B` æ˜¯ä¸“é—¨çš„OCRæ¨¡å‹ï¼Œä¸é€‚åˆé€šç”¨å¯¹è¯
2. å¦‚æœå®é™…ä½¿ç”¨çš„æ˜¯ `DeepSeek-R1-1.5B-Distill`ï¼Œé…ç½®ä¸åŒ¹é…

---

## 3. è§£å†³æ–¹æ¡ˆ

### 3.1 ç«‹å³ä¿®å¤ï¼šè¿‡æ»¤æ€è€ƒè¿‡ç¨‹

**ä¿®æ”¹ `transformers_service.py`**:

```python
def _post_process_response(self, response: str, model_name: str) -> str:
    """
    åå¤„ç†æ¨¡å‹è¾“å‡º
    
    Args:
        response: åŸå§‹æ¨¡å‹è¾“å‡º
        model_name: æ¨¡å‹åç§°
        
    Returns:
        å¤„ç†åçš„è¾“å‡º
    """
    # æ£€æµ‹æ˜¯å¦æ˜¯æ¨ç†æ¨¡å‹ï¼ˆDeepSeek-R1ç³»åˆ—ï¼‰
    if "deepseek-r1" in model_name.lower() or "r1" in model_name.lower():
        # ç§»é™¤æ€è€ƒè¿‡ç¨‹æ ‡ç­¾
        import re
        
        # æ–¹æ³•1: ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå†…å®¹
        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
        
        # æ–¹æ³•2: å¦‚æœæœ‰å¤šä¸ªæ¢è¡Œï¼Œåªä¿ç•™æœ€åä¸€æ®µï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
        if '\n\n' in response:
            parts = response.split('\n\n')
            # æ‰¾åˆ°æœ€é•¿çš„éç©ºæ®µè½ä½œä¸ºç­”æ¡ˆ
            non_empty_parts = [p.strip() for p in parts if p.strip() and not p.strip().startswith('<')]
            if non_empty_parts:
                response = max(non_empty_parts, key=len)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        response = re.sub(r'\n{3,}', '\n\n', response)
        response = response.strip()
    
    return response

# åœ¨chatæ–¹æ³•ä¸­è°ƒç”¨ï¼ˆç¬¬367è¡Œä¹‹åï¼‰
response = self.current_tokenizer.decode(
    output_ids[0][input_length:],
    skip_special_tokens=True
)

# ğŸ”§ æ·»åŠ åå¤„ç†
response = self._post_process_response(response, model_name)

return response.strip()
```

### 3.2 ä¼˜åŒ– Prompt æ¨¡æ¿

**ä¿®æ”¹ `transformers_service.py` çš„ `_build_prompt` æ–¹æ³•**:

```python
def _build_prompt(self, messages: List[Dict[str, str]]) -> str:
    """
    æ„å»ºæ¨¡å‹è¾“å…¥promptï¼ˆæ”¯æŒä¸åŒæ¨¡å‹ç±»å‹ï¼‰
    """
    # æ£€æµ‹æ˜¯å¦æ˜¯æ¨ç†æ¨¡å‹
    is_reasoning_model = (self.current_model_name and 
                         ("r1" in self.current_model_name.lower() or 
                          "deepseek-r1" in self.current_model_name.lower()))
    
    # å¦‚æœæ˜¯æ¨ç†æ¨¡å‹ï¼Œæ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤
    if is_reasoning_model:
        # åœ¨systemæ¶ˆæ¯ä¸­æ·»åŠ æŒ‡ä»¤
        system_instruction = (
            "è¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€‚"
            "ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚å’Œæç¤ºè¯å†…å®¹å›ç­”ã€‚"
        )
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯
        has_system = any(msg["role"] == "system" for msg in messages)
        if has_system:
            # è¿½åŠ åˆ°ç°æœ‰systemæ¶ˆæ¯
            for msg in messages:
                if msg["role"] == "system":
                    msg["content"] = f"{msg['content']}\n\n{system_instruction}"
                    break
        else:
            # æ·»åŠ æ–°çš„systemæ¶ˆæ¯
            messages = [{"role": "system", "content": system_instruction}] + messages
    
    # å°è¯•ä½¿ç”¨tokenizerçš„chat_template
    if hasattr(self.current_tokenizer, "apply_chat_template"):
        try:
            return self.current_tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception as e:
            logger.warning(f"apply_chat_templateå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤æ¨¡æ¿")
    
    # é»˜è®¤æ¨¡æ¿
    prompt = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        
        if role == "system":
            prompt += f"System: {content}\n\n"
        elif role == "user":
            prompt += f"User: {content}\n\n"
        elif role == "assistant":
            prompt += f"Assistant: {content}\n\n"
    
    prompt += "Assistant: "
    return prompt
```

### 3.3 ä¼˜åŒ–ç”Ÿæˆå‚æ•°

**ä¿®æ”¹ `transformers_service.py` çš„ç”Ÿæˆé…ç½®**:

```python
# æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´å‚æ•°
is_reasoning_model = ("r1" in model.lower() or "deepseek-r1" in model.lower())

generation_config = {
    "max_new_tokens": min(max_tokens, 256) if is_reasoning_model else max_tokens,
    "temperature": temperature,
    "do_sample": temperature > 0,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "pad_token_id": self.current_tokenizer.eos_token_id,
    "eos_token_id": self.current_tokenizer.eos_token_id,
    "early_stopping": True,  # ğŸ”§ æ·»åŠ æ—©åœ
}

# å¦‚æœæ˜¯æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨Greedy Decodingï¼ˆæ›´å¿«ï¼‰
if is_reasoning_model and temperature <= 0.3:
    generation_config["do_sample"] = False
    generation_config["num_beams"] = 1  # Greedy
```

### 3.4 æ€§èƒ½ä¼˜åŒ–

**æ–¹æ¡ˆA: å®Œå…¨GPUåŠ è½½ï¼ˆæ¨èï¼‰**

```python
# ä¿®æ”¹åŠ è½½é…ç½®ï¼Œå¯¹äºå°æ¨¡å‹ä¸ä½¿ç”¨device_map
if quantize and self.device == "cuda":
    load_kwargs["quantization_config"] = self.quantization_config
    
    # ğŸ”§ å¯¹äºå°æ¨¡å‹ï¼ˆ<3Bï¼‰ï¼Œç›´æ¥åŠ è½½åˆ°GPUï¼Œä¸ä½¿ç”¨device_map
    model_size_gb = self._estimate_model_size(model_path)
    if model_size_gb < 2.0:  # INT4é‡åŒ–å<2GBçš„æ¨¡å‹
        load_kwargs["device_map"] = None  # ä¸ä½¿ç”¨device_map
        # æ¨¡å‹ä¼šè‡ªåŠ¨åŠ è½½åˆ°cuda:0
    else:
        load_kwargs["device_map"] = "auto"
        load_kwargs["max_memory"] = {0: "5.5GiB", "cpu": "0GiB"}

def _estimate_model_size(self, model_path: Path) -> float:
    """ä¼°ç®—æ¨¡å‹å¤§å°ï¼ˆGBï¼‰"""
    try:
        # è¯»å–config.jsonè·å–å‚æ•°é‡
        config_file = model_path / "config.json"
        if config_file.exists():
            import json
            with open(config_file) as f:
                config = json.load(f)
            
            # ä¼°ç®—ï¼šå‚æ•°é‡ Ã— 4bit / 8bit/byte / 1024Â³
            # ä¾‹å¦‚ï¼š1.5B Ã— 0.5 bytes/param â‰ˆ 0.75GB
            vocab_size = config.get("vocab_size", 32000)
            hidden_size = config.get("hidden_size", 2048)
            num_layers = config.get("num_hidden_layers", 24)
            
            # ç²—ç•¥ä¼°ç®—å‚æ•°é‡ï¼ˆbillionï¼‰
            params_b = (vocab_size * hidden_size + 
                       num_layers * hidden_size * hidden_size * 4) / 1e9
            
            # INT4é‡åŒ–åå¤§å°
            size_gb = params_b * 0.5  # 4bit = 0.5 byte per parameter
            return size_gb
    except:
        pass
    
    # é™çº§ï¼šè®¡ç®—safetensorsæ–‡ä»¶å¤§å°
    total_size = sum(
        f.stat().st_size 
        for f in model_path.rglob('*.safetensors')
    ) / 1024**3
    
    # INT4é‡åŒ–é€šå¸¸æ˜¯åŸå§‹å¤§å°çš„1/4
    return total_size * 0.25
```

**æ–¹æ¡ˆB: å¯ç”¨Flash Attentionï¼ˆæ¨èï¼‰**

```python
# åœ¨ç”Ÿæˆé…ç½®ä¸­æ·»åŠ 
generation_config["use_cache"] = True  # ä½¿ç”¨KV cache
generation_config["attn_implementation"] = "flash_attention_2"  # å¦‚æœæ”¯æŒ

# æˆ–åœ¨æ¨¡å‹åŠ è½½æ—¶
load_kwargs["attn_implementation"] = "flash_attention_2"
```

**æ–¹æ¡ˆC: å‡å°‘max_tokens**

```yaml
# config.yaml
llm:
  max_tokens: 256  # ğŸ”§ ä»512é™è‡³256ï¼Œå¯¹è¯å·²è¶³å¤Ÿ
```

### 3.5 æ›´æ¢æ¨èæ¨¡å‹

**é—®é¢˜æ¨¡å‹**:
- âŒ `DeepSeek-OCR-3B`: OCRä¸“ç”¨ï¼Œä¸é€‚åˆå¯¹è¯
- âš ï¸ `DeepSeek-R1-1.5B-Distill`: æ¨ç†æ¨¡å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†

**æ¨èæ¨¡å‹**ï¼ˆ6GBæ˜¾å­˜ï¼‰:
```
1. Qwen2.5-3B-Instruct (æ¨è) â­â­â­â­â­
   - ä¸“ä¸ºå¯¹è¯ä¼˜åŒ–
   - INT4é‡åŒ–åçº¦1.5GB
   - é€Ÿåº¦å¿«ï¼Œè´¨é‡é«˜
   - ä¸¥æ ¼éµå¾ªæŒ‡ä»¤

2. Qwen2.5-1.5B-Instruct
   - æ›´å°ï¼Œæ›´å¿«
   - INT4é‡åŒ–åçº¦800MB
   - é€‚åˆå¿«é€Ÿå“åº”

3. Phi-3-mini-4k-instruct
   - 3.8Bå‚æ•°
   - INT4é‡åŒ–åçº¦2GB
   - å¾®è½¯å¼€æºï¼Œè´¨é‡å¥½

4. MiniCPM-2B-dpo (ä¸­æ–‡ä¼˜åŒ–)
   - 2.4Bå‚æ•°
   - INT4é‡åŒ–åçº¦1.2GB
   - ä¸­æ–‡æ•ˆæœä¼˜ç§€
```

**é…ç½®ä¿®æ”¹**:
```yaml
# config.yaml
llm:
  default_model: "Qwen2.5-3B-Instruct"  # ğŸ”§ æ›´æ¢ä¸ºå¯¹è¯æ¨¡å‹
  transformers_quantization: "int4"
  max_tokens: 256  # ğŸ”§ é™ä½tokenæ•°
```

---

## 4. å®æ–½æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šç«‹å³ä¿®å¤ï¼ˆ10åˆ†é’Ÿï¼‰

1. åœ¨ `transformers_service.py` æ·»åŠ  `_post_process_response` æ–¹æ³•
2. åœ¨ `chat` æ–¹æ³•ä¸­è°ƒç”¨åå¤„ç†
3. ä¿®æ”¹ `config.yaml` é™ä½ `max_tokens` åˆ° 256

### ç¬¬äºŒæ­¥ï¼šä¼˜åŒ–Promptï¼ˆ15åˆ†é’Ÿï¼‰

1. ä¿®æ”¹ `_build_prompt` æ–¹æ³•ï¼Œæ£€æµ‹æ¨ç†æ¨¡å‹
2. ä¸ºæ¨ç†æ¨¡å‹æ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤
3. æµ‹è¯•æ•ˆæœ

### ç¬¬ä¸‰æ­¥ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆ20åˆ†é’Ÿï¼‰

1. æ·»åŠ  `_estimate_model_size` æ–¹æ³•
2. ä¿®æ”¹æ¨¡å‹åŠ è½½é€»è¾‘
3. ä¼˜åŒ–ç”Ÿæˆå‚æ•°
4. æµ‹è¯•é€Ÿåº¦æå‡

### ç¬¬å››æ­¥ï¼šæ›´æ¢æ¨¡å‹ï¼ˆå¯é€‰ï¼Œ30åˆ†é’Ÿï¼‰

1. ä¸‹è½½ Qwen2.5-3B-Instruct
2. æ”¾åˆ° `Models/LLM/` ç›®å½•
3. ä¿®æ”¹é…ç½®æ–‡ä»¶
4. æµ‹è¯•å¯¹è¯è´¨é‡

---

## 5. é¢„æœŸæ•ˆæœ

### ä¿®å¤å‰

```
[ç”¨æˆ·] ç®€å•ä»‹ç»ä¸€ä¸‹FastAPI
[æ¨¡å‹] <think>
ç”¨æˆ·æƒ³äº†è§£FastAPI...
æˆ‘éœ€è¦ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢...
å…ˆä»‹ç»å®šä¹‰ï¼Œå†è¯´ç‰¹ç‚¹...
</think>

FastAPIæ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„Python Webæ¡†æ¶...
```

**æ—¶é—´**: ~30-60ç§’  
**è´¨é‡**: âŒ åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼Œå½±å“ä½“éªŒ

### ä¿®å¤å

```
[ç”¨æˆ·] ç®€å•ä»‹ç»ä¸€ä¸‹FastAPI
[æ¨¡å‹] FastAPIæ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„Python Webæ¡†æ¶ï¼ŒåŸºäºæ ‡å‡†Pythonç±»å‹æç¤ºæ„å»ºï¼Œå…·æœ‰é«˜æ€§èƒ½ã€æ˜“ç”¨æ€§å’Œè‡ªåŠ¨APIæ–‡æ¡£ç”Ÿæˆç­‰ç‰¹ç‚¹ã€‚
```

**æ—¶é—´**: ~5-10ç§’  
**è´¨é‡**: âœ… ç®€æ´å‡†ç¡®ï¼Œä¸¥æ ¼éµå¾ªæŒ‡ä»¤

---

## 6. é•¿æœŸä¼˜åŒ–å»ºè®®

1. **å¼•å…¥vLLM**: æ¨ç†é€Ÿåº¦æå‡3-5å€
2. **å¼•å…¥GGUFæ ¼å¼**: ä½¿ç”¨llama.cppï¼Œé€Ÿåº¦æ›´å¿«
3. **æ¨¡å‹ç¼“å­˜**: é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹
4. **æ‰¹å¤„ç†**: æ”¯æŒå¤šç”¨æˆ·å¹¶å‘è¯·æ±‚
5. **é‡åŒ–å®éªŒ**: æµ‹è¯•INT8æ˜¯å¦æ›´å¿«ï¼ˆå¯èƒ½åè€Œå¿«ï¼‰

---

## 7. æµ‹è¯•æ¸…å•

- [ ] æ€è€ƒè¿‡ç¨‹æ˜¯å¦è¢«è¿‡æ»¤
- [ ] ç”Ÿæˆé€Ÿåº¦æ˜¯å¦æå‡åˆ°10ç§’å†…
- [ ] æ˜¯å¦ä¸¥æ ¼éµå¾ªæç¤ºè¯
- [ ] ä¸­æ–‡å›ç­”è´¨é‡
- [ ] è‹±æ–‡å›ç­”è´¨é‡
- [ ] ä¸Šä¸‹æ–‡è®°å¿†æ˜¯å¦æ­£å¸¸
- [ ] RAGæ£€ç´¢ç»“æœæ˜¯å¦æ­£ç¡®ä½¿ç”¨
- [ ] æµå¼è¾“å‡ºæ˜¯å¦æ­£å¸¸

---

**ä¼˜å…ˆçº§**:
1. ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼šè¿‡æ»¤æ€è€ƒè¿‡ç¨‹ã€é™ä½max_tokens
2. ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼šä¼˜åŒ–Promptã€ä¼˜åŒ–åŠ è½½é€»è¾‘
3. ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼šæ›´æ¢æ¨¡å‹ã€å¼•å…¥vLLM

**é¢„è®¡æ€»è€—æ—¶**: 1-2å°æ—¶ï¼ˆä¸å«æ¨¡å‹ä¸‹è½½ï¼‰
