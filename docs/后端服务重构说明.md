# 后端服务重构说明

本文档详细说明了 `Backend/app/services` 目录的重构结构及其各个文件的作用。

## 1. 目录结构概览

重构后的服务层采用了模块化设计，将不同功能领域的服务归类到相应的子目录中：

```
Backend/app/services/
├── chat/               # 对话业务逻辑
│   ├── chat.py         # 核心对话服务 (ChatService)
│   └── __init__.py
├── common/             # 通用工具服务
│   ├── prompt_utils.py # Prompt构建工具
│   └── __init__.py
├── llm_engine/         # LLM 引擎与模型管理
│   ├── manager.py      # 模型管理器 (ModelManager)
│   ├── model_loader.py # 模型加载器 (ModelLoader)
│   ├── ollama.py       # Ollama 服务集成 (OllamaLLMService)
│   ├── scanner.py      # 模型扫描器 (ModelScanner)
│   ├── transformers.py # Transformers 本地推理 (TransformersService)
│   └── __init__.py
├── rag/                # RAG (检索增强生成) 核心组件
│   ├── embedding.py    # 嵌入模型服务 (EmbeddingService)
│   ├── entity_extraction.py # 实体提取服务 (EntityExtractionService)
│   ├── file.py         # 文件管理服务 (FileService)
│   ├── graph_store.py  # 图数据库服务 (Neo4jGraphService)
│   ├── knowledge_base.py # 知识库管理服务 (KnowledgeBaseService)
│   ├── metadata.py     # 元数据管理服务 (MetadataService)
│   ├── ollama_embedding.py # Ollama 嵌入服务 (OllamaEmbeddingService)
│   ├── retrieval.py    # 混合检索服务 (HybridRetrievalService)
│   ├── vector_store.py # 向量数据库服务 (VectorStoreService)
│   └── __init__.py
├── training/           # 模型训练模块
│   ├── llama_factory.py # LLaMA-Factory 集成 (LlamaFactoryService)
│   ├── lora.py         # 简易 LoRA 训练 (SimpleLoRATrainer)
│   ├── lora_scanner.py # LoRA 模型扫描 (LoRAScannerService)
│   └── __init__.py
└── __init__.py         # 根包导出
```

## 2. 模块详细说明

### 2.1 Chat (对话模块)
负责处理用户与系统的交互逻辑，是 RAG 和 LLM 的上层封装。

*   **`chat.py`**: `ChatService` 类。核心业务逻辑，协调检索（RAG）、历史记录管理和 LLM 调用。支持纯对话模式、RAG 模式和混合检索模式。

### 2.2 RAG (检索增强生成模块)
负责知识库的构建、存储、索引和检索。

*   **`knowledge_base.py`**: `KnowledgeBaseService` 类。管理知识库的增删改查，是 RAG 流程的入口。
*   **`file.py`**: `FileService` 类。处理文件上传、解析和存储。
*   **`embedding.py`**: `EmbeddingService` 类。负责文本向量化，支持本地模型（SentenceTransformers）。
*   **`vector_store.py`**: `VectorStoreService` 类。管理向量数据库（ChromaDB），负责向量的存储和相似度搜索。
*   **`retrieval.py`**: `HybridRetrievalService` 类。实现混合检索逻辑，结合向量检索和知识图谱检索。
*   **`graph_store.py`**: `Neo4jGraphService` 类。管理 Neo4j 图数据库，处理知识图谱的节点和关系。
*   **`entity_extraction.py`**: `EntityExtractionService` 类。使用 LLM 从文本中提取实体和关系，用于构建知识图谱。
*   **`ollama_embedding.py`**: `OllamaEmbeddingService` 类。提供基于 Ollama 的嵌入服务接口。
*   **`metadata.py`**: `MetadataService` 类。管理知识库的元数据文件（info.json）。

### 2.3 LLM Engine (大模型引擎模块)
负责底层的模型加载、推理和管理。

*   **`transformers.py`**: `TransformersService` 类。基于 HuggingFace Transformers 的本地推理服务，支持流式输出和量化。
*   **`ollama.py`**: `OllamaLLMService` 类。集成 Ollama API，提供外部模型推理能力。
*   **`model_loader.py`**: `ModelLoader` 类。封装了 `AutoModelForCausalLM` 和 `PeftModel` 的加载逻辑，处理量化配置。
*   **`manager.py`**: `ModelManager` 类。管理模型的生命周期（加载、卸载、删除）。
*   **`scanner.py`**: `ModelScanner` 类。扫描本地文件系统，发现可用的 LLM 和 Embedding 模型。

### 2.4 Training (训练模块)
负责模型的微调（Fine-tuning）任务。

*   **`lora.py`**: `SimpleLoRATrainer` 类。提供简易的 LoRA 训练流程，基于 `peft` 和 `transformers`。
*   **`llama_factory.py`**: `LlamaFactoryService` 类。集成 LLaMA-Factory 框架，提供更高级的训练 Web UI 管理。
*   **`lora_scanner.py`**: `LoRAScannerService` 类。扫描和管理已训练好的 LoRA 适配器。

### 2.5 Common (通用模块)
*   **`prompt_utils.py`**: 提供 Prompt 构建和管理的通用工具函数。

## 3. 导入规范

在代码中引用服务时，建议使用完整的模块路径或从 `app.services` 根包导入（如果已在 `__init__.py` 中导出）。

**示例：**

```python
# 推荐方式 1：从子模块导入（明确来源）
from app.services.rag.knowledge_base import KnowledgeBaseService
from app.services.chat.chat import ChatService

# 推荐方式 2：从根包导入（便捷，前提是 __init__.py 已导出）
from app.services import KnowledgeBaseService, ChatService
```

## 4. 维护指南

*   **新增服务**：请根据功能归属，将新服务文件放入对应的子目录中。
*   **新增模块**：如果现有分类无法覆盖，可创建新的子目录（如 `app/services/agent/`）。
*   **避免循环依赖**：尽量避免在 `__init__.py` 中进行复杂的逻辑操作，仅做导出。如果遇到循环导入，请使用局部导入（在函数内部导入）。
