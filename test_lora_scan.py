"""
æµ‹è¯• LoRA æ‰«æåŠŸèƒ½
éªŒè¯æ˜¯å¦èƒ½æ­£ç¡®æ‰«æ LLaMA-Training/saves ç›®å½•
"""
import sys
from pathlib import Path

# æ·»åŠ  Backend åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "Backend"))

from app.services.model_scanner import ModelScanner
from app.core.config import settings

def test_lora_scan():
    print("=" * 60)
    print("æµ‹è¯• LoRA æ¨¡å‹æ‰«æåŠŸèƒ½")
    print("=" * 60)
    print()
    
    # åˆ›å»ºæ‰«æå™¨
    scanner = ModelScanner()
    
    # æ˜¾ç¤ºæ‰«æç›®å½•
    print(f"ğŸ“ Models/LoRA ç›®å½•: {scanner.lora_dir}")
    print(f"   å­˜åœ¨: {scanner.lora_dir.exists()}")
    print()
    
    training_saves = scanner.base_dir / "LLaMA-Training" / "saves"
    print(f"ğŸ“ LLaMA-Training/saves ç›®å½•: {training_saves}")
    print(f"   å­˜åœ¨: {training_saves.exists()}")
    print()
    
    # æ‰«æ LoRA æ¨¡å‹
    print("ğŸ” å¼€å§‹æ‰«æ LoRA æ¨¡å‹...")
    print()
    
    lora_models = scanner.scan_lora_models()
    
    # æ˜¾ç¤ºç»“æœ
    print(f"âœ… æ‰¾åˆ° {len(lora_models)} ä¸ª LoRA æ¨¡å‹:")
    print()
    
    for i, model in enumerate(lora_models, 1):
        print(f"{i}. {model['name']}")
        print(f"   è·¯å¾„: {model['path']}")
        print(f"   åŸºåº§æ¨¡å‹: {model['base_model']}")
        print(f"   Rank: {model['rank']}")
        print(f"   Alpha: {model['lora_alpha']}")
        print(f"   å¤§å°: {model['size']}")
        print(f"   åˆ›å»ºæ—¶é—´: {model['created_at']}")
        print()
    
    print("=" * 60)
    print("æµ‹è¯•å®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    test_lora_scan()
